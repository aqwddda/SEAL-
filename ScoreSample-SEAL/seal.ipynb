{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40c02be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "from itertools import chain\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from scipy.sparse.csgraph import shortest_path\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from torch.nn import BCEWithLogitsLoss, Conv1d, MaxPool1d, ModuleList\n",
    "from torch_geometric.data import Data, InMemoryDataset\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import MLP, GCNConv, SortAggregation\n",
    "from torch_geometric.transforms import RandomLinkSplit\n",
    "from torch_geometric.utils import k_hop_subgraph, to_scipy_sparse_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb6f2d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SEALDataset(InMemoryDataset):\n",
    "    def __init__(self, dataset, num_hops, split='train'):\n",
    "        self._data = dataset[0]\n",
    "        self.num_hops = num_hops\n",
    "        super().__init__(dataset.root)\n",
    "        index = ['train', 'val', 'test'].index(split)\n",
    "        self.load(self.processed_paths[index])\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['SEAL_train_data.pt', 'SEAL_val_data.pt', 'SEAL_test_data.pt']\n",
    "\n",
    "    def process(self):\n",
    "        transform = RandomLinkSplit(num_val=0.05, num_test=0.1,\n",
    "                                    is_undirected=True, split_labels=True)\n",
    "        train_data, val_data, test_data = transform(self._data)\n",
    "\n",
    "        self._max_z = 0\n",
    "\n",
    "        # Collect a list of subgraphs for training, validation and testing:\n",
    "        train_pos_data_list = self.extract_enclosing_subgraphs(\n",
    "            train_data.edge_index, train_data.pos_edge_label_index, 1)\n",
    "        train_neg_data_list = self.extract_enclosing_subgraphs(\n",
    "            train_data.edge_index, train_data.neg_edge_label_index, 0)\n",
    "\n",
    "        val_pos_data_list = self.extract_enclosing_subgraphs(\n",
    "            val_data.edge_index, val_data.pos_edge_label_index, 1)\n",
    "        val_neg_data_list = self.extract_enclosing_subgraphs(\n",
    "            val_data.edge_index, val_data.neg_edge_label_index, 0)\n",
    "\n",
    "        test_pos_data_list = self.extract_enclosing_subgraphs(\n",
    "            test_data.edge_index, test_data.pos_edge_label_index, 1)\n",
    "        test_neg_data_list = self.extract_enclosing_subgraphs(\n",
    "            test_data.edge_index, test_data.neg_edge_label_index, 0)\n",
    "\n",
    "        # Convert node labeling to one-hot features.\n",
    "        for data in chain(train_pos_data_list, train_neg_data_list,\n",
    "                          val_pos_data_list, val_neg_data_list,\n",
    "                          test_pos_data_list, test_neg_data_list):\n",
    "            # We solely learn links from structure, dropping any node features:\n",
    "            data.x = F.one_hot(data.z, self._max_z + 1).to(torch.float)\n",
    "\n",
    "        train_data_list = train_pos_data_list + train_neg_data_list\n",
    "        self.save(train_data_list, self.processed_paths[0])\n",
    "        val_data_list = val_pos_data_list + val_neg_data_list\n",
    "        self.save(val_data_list, self.processed_paths[1])\n",
    "        test_data_list = test_pos_data_list + test_neg_data_list\n",
    "        self.save(test_data_list, self.processed_paths[2])\n",
    "\n",
    "    def extract_enclosing_subgraphs(self, edge_index, edge_label_index, y):\n",
    "        data_list = []\n",
    "        for src, dst in edge_label_index.t().tolist():\n",
    "            sub_nodes, sub_edge_index, mapping, _ = k_hop_subgraph(\n",
    "                [src, dst], self.num_hops, edge_index, relabel_nodes=True)\n",
    "            src, dst = mapping.tolist()\n",
    "\n",
    "            # Remove target link from the subgraph.\n",
    "            mask1 = (sub_edge_index[0] != src) | (sub_edge_index[1] != dst)\n",
    "            mask2 = (sub_edge_index[0] != dst) | (sub_edge_index[1] != src)\n",
    "            sub_edge_index = sub_edge_index[:, mask1 & mask2]\n",
    "\n",
    "            # Calculate node labeling.\n",
    "            z = self.drnl_node_labeling(sub_edge_index, src, dst,\n",
    "                                        num_nodes=sub_nodes.size(0))\n",
    "\n",
    "            data = Data(x=self._data.x[sub_nodes], z=z,\n",
    "                        edge_index=sub_edge_index, y=y)\n",
    "            data_list.append(data)\n",
    "\n",
    "        return data_list\n",
    "\n",
    "    def drnl_node_labeling(self, edge_index, src, dst, num_nodes=None):\n",
    "        # Double-radius node labeling (DRNL).\n",
    "        src, dst = (dst, src) if src > dst else (src, dst)\n",
    "        adj = to_scipy_sparse_matrix(edge_index, num_nodes=num_nodes).tocsr()\n",
    "\n",
    "        idx = list(range(src)) + list(range(src + 1, adj.shape[0]))\n",
    "        adj_wo_src = adj[idx, :][:, idx]\n",
    "\n",
    "        idx = list(range(dst)) + list(range(dst + 1, adj.shape[0]))\n",
    "        adj_wo_dst = adj[idx, :][:, idx]\n",
    "\n",
    "        dist2src = shortest_path(adj_wo_dst, directed=False, unweighted=True,\n",
    "                                 indices=src)\n",
    "        dist2src = np.insert(dist2src, dst, 0, axis=0)\n",
    "        dist2src = torch.from_numpy(dist2src)\n",
    "\n",
    "        dist2dst = shortest_path(adj_wo_src, directed=False, unweighted=True,\n",
    "                                 indices=dst - 1)\n",
    "        dist2dst = np.insert(dist2dst, src, 0, axis=0)\n",
    "        dist2dst = torch.from_numpy(dist2dst)\n",
    "\n",
    "        dist = dist2src + dist2dst\n",
    "        dist_over_2, dist_mod_2 = dist // 2, dist % 2\n",
    "\n",
    "        z = 1 + torch.min(dist2src, dist2dst)\n",
    "        z += dist_over_2 * (dist_over_2 + dist_mod_2 - 1)\n",
    "        z[src] = 1.\n",
    "        z[dst] = 1.\n",
    "        z[torch.isnan(z)] = 0.\n",
    "\n",
    "        self._max_z = max(int(z.max()), self._max_z)\n",
    "\n",
    "        return z.to(torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8911a3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Planetoid('./data/Planetoid', name='Cora')\n",
    "\n",
    "train_dataset = SEALDataset(dataset, num_hops=2, split='train')\n",
    "val_dataset = SEALDataset(dataset, num_hops=2, split='val')\n",
    "test_dataset = SEALDataset(dataset, num_hops=2, split='test')\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f51a973",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DGCNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_dim, num_layers, GNN=GCNConv, k=0.6):\n",
    "        super().__init__()\n",
    "\n",
    "        if k < 1:  # Transform percentile to number.\n",
    "            num_nodes = sorted([data.num_nodes for data in train_dataset])\n",
    "            k = num_nodes[int(math.ceil(k * len(num_nodes))) - 1]\n",
    "            k = int(max(10, k))\n",
    "\n",
    "        self.convs = ModuleList()\n",
    "        self.convs.append(GNN(train_dataset.num_features, hidden_dim))\n",
    "        for i in range(0, num_layers - 1):\n",
    "            self.convs.append(GNN(hidden_dim, hidden_dim))\n",
    "        self.convs.append(GNN(hidden_dim, 1))\n",
    "\n",
    "        conv1d_channels = [16, 32]\n",
    "        total_latent_dim = hidden_dim * num_layers + 1\n",
    "        conv1d_kws = [total_latent_dim, 5]\n",
    "        self.conv1 = Conv1d(1, conv1d_channels[0], conv1d_kws[0],\n",
    "                            conv1d_kws[0])\n",
    "        self.pool = SortAggregation(k)\n",
    "        self.maxpool1d = MaxPool1d(2, 2)\n",
    "        self.conv2 = Conv1d(conv1d_channels[0], conv1d_channels[1],\n",
    "                            conv1d_kws[1], 1)\n",
    "        dense_dim = int((k - 2) / 2 + 1)\n",
    "        dense_dim = (dense_dim - conv1d_kws[1] + 1) * conv1d_channels[1]\n",
    "        self.mlp = MLP([dense_dim, 128, 1], dropout=0.5, norm=None)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        xs = [x]\n",
    "        for conv in self.convs:\n",
    "            xs += [conv(xs[-1], edge_index).tanh()]\n",
    "        x = torch.cat(xs[1:], dim=-1)\n",
    "\n",
    "        # Global pooling.\n",
    "        x = self.pool(x, batch)\n",
    "        x = x.unsqueeze(1)  # [num_graphs, 1, k * hidden]\n",
    "        x = self.conv1(x).relu()\n",
    "        x = self.maxpool1d(x)\n",
    "        x = self.conv2(x).relu()\n",
    "        x = x.view(x.size(0), -1)  # [num_graphs, dense_dim]\n",
    "\n",
    "        return self.mlp(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "203d44fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64.51414884135473\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = DGCNN(hidden_dim=32, num_layers=3).to(device)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.0001)\n",
    "criterion = BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9276a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index, data.batch)\n",
    "        loss = criterion(out.view(-1), data.y.to(torch.float))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += float(loss) * data.num_graphs\n",
    "\n",
    "    return total_loss / len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a69a5849",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "\n",
    "    y_pred, y_true = [], []\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        logits = model(data.x, data.edge_index, data.batch)\n",
    "        y_pred.append(logits.view(-1).cpu())\n",
    "        y_true.append(data.y.view(-1).cpu().to(torch.float))\n",
    "\n",
    "    return roc_auc_score(torch.cat(y_true), torch.cat(y_pred)), average_precision_score(torch.cat(y_true),torch.cat(y_pred) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "125f31b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Loss: 0.6118, Val_AUC: 0.8278, Val_AP: 0.8452Test_AUC: 0.8507, Test_AP: 0.8452\n",
      "Epoch: 02, Loss: 0.4475, Val_AUC: 0.8816, Val_AP: 0.8997Test_AUC: 0.9149, Test_AP: 0.9227\n",
      "Epoch: 03, Loss: 0.3972, Val_AUC: 0.8931, Val_AP: 0.9107Test_AUC: 0.9279, Test_AP: 0.9393\n",
      "Epoch: 04, Loss: 0.3773, Val_AUC: 0.8951, Val_AP: 0.9139Test_AUC: 0.9315, Test_AP: 0.9424\n",
      "Epoch: 05, Loss: 0.3682, Val_AUC: 0.8966, Val_AP: 0.9154Test_AUC: 0.9348, Test_AP: 0.9460\n",
      "Epoch: 06, Loss: 0.3637, Val_AUC: 0.8966, Val_AP: 0.9155Test_AUC: 0.9336, Test_AP: 0.9460\n",
      "Epoch: 07, Loss: 0.3604, Val_AUC: 0.8952, Val_AP: 0.9172Test_AUC: 0.9336, Test_AP: 0.9460\n",
      "Epoch: 08, Loss: 0.3623, Val_AUC: 0.8973, Val_AP: 0.9174Test_AUC: 0.9346, Test_AP: 0.9471\n",
      "Epoch: 09, Loss: 0.3565, Val_AUC: 0.8972, Val_AP: 0.9175Test_AUC: 0.9346, Test_AP: 0.9471\n",
      "Epoch: 10, Loss: 0.3557, Val_AUC: 0.8977, Val_AP: 0.9192Test_AUC: 0.9347, Test_AP: 0.9472\n",
      "Epoch: 11, Loss: 0.3542, Val_AUC: 0.9001, Val_AP: 0.9211Test_AUC: 0.9358, Test_AP: 0.9482\n",
      "Epoch: 12, Loss: 0.3539, Val_AUC: 0.9012, Val_AP: 0.9216Test_AUC: 0.9366, Test_AP: 0.9489\n",
      "Epoch: 13, Loss: 0.3525, Val_AUC: 0.9012, Val_AP: 0.9220Test_AUC: 0.9366, Test_AP: 0.9489\n",
      "Epoch: 14, Loss: 0.3506, Val_AUC: 0.9029, Val_AP: 0.9233Test_AUC: 0.9380, Test_AP: 0.9500\n",
      "Epoch: 15, Loss: 0.3494, Val_AUC: 0.9042, Val_AP: 0.9242Test_AUC: 0.9379, Test_AP: 0.9494\n",
      "Epoch: 16, Loss: 0.3508, Val_AUC: 0.9026, Val_AP: 0.9226Test_AUC: 0.9379, Test_AP: 0.9494\n",
      "Epoch: 17, Loss: 0.3495, Val_AUC: 0.9071, Val_AP: 0.9261Test_AUC: 0.9392, Test_AP: 0.9507\n",
      "Epoch: 18, Loss: 0.3481, Val_AUC: 0.9046, Val_AP: 0.9258Test_AUC: 0.9392, Test_AP: 0.9507\n",
      "Epoch: 19, Loss: 0.3468, Val_AUC: 0.9056, Val_AP: 0.9261Test_AUC: 0.9392, Test_AP: 0.9507\n",
      "Epoch: 20, Loss: 0.3466, Val_AUC: 0.9067, Val_AP: 0.9265Test_AUC: 0.9392, Test_AP: 0.9507\n",
      "Epoch: 21, Loss: 0.3450, Val_AUC: 0.9086, Val_AP: 0.9277Test_AUC: 0.9397, Test_AP: 0.9502\n",
      "Epoch: 22, Loss: 0.3424, Val_AUC: 0.9109, Val_AP: 0.9296Test_AUC: 0.9407, Test_AP: 0.9514\n",
      "Epoch: 23, Loss: 0.3439, Val_AUC: 0.9082, Val_AP: 0.9277Test_AUC: 0.9407, Test_AP: 0.9514\n",
      "Epoch: 24, Loss: 0.3443, Val_AUC: 0.9105, Val_AP: 0.9294Test_AUC: 0.9407, Test_AP: 0.9514\n",
      "Epoch: 25, Loss: 0.3425, Val_AUC: 0.9089, Val_AP: 0.9282Test_AUC: 0.9407, Test_AP: 0.9514\n",
      "Epoch: 26, Loss: 0.3423, Val_AUC: 0.9097, Val_AP: 0.9289Test_AUC: 0.9407, Test_AP: 0.9514\n",
      "Epoch: 27, Loss: 0.3414, Val_AUC: 0.9102, Val_AP: 0.9293Test_AUC: 0.9407, Test_AP: 0.9514\n",
      "Epoch: 28, Loss: 0.3389, Val_AUC: 0.9094, Val_AP: 0.9288Test_AUC: 0.9407, Test_AP: 0.9514\n",
      "Epoch: 29, Loss: 0.3395, Val_AUC: 0.9106, Val_AP: 0.9299Test_AUC: 0.9407, Test_AP: 0.9514\n",
      "Epoch: 30, Loss: 0.3421, Val_AUC: 0.9080, Val_AP: 0.9284Test_AUC: 0.9407, Test_AP: 0.9514\n",
      "Epoch: 31, Loss: 0.3393, Val_AUC: 0.9090, Val_AP: 0.9290Test_AUC: 0.9407, Test_AP: 0.9514\n",
      "Epoch: 32, Loss: 0.3378, Val_AUC: 0.9126, Val_AP: 0.9314Test_AUC: 0.9411, Test_AP: 0.9521\n",
      "Epoch: 33, Loss: 0.3390, Val_AUC: 0.9108, Val_AP: 0.9303Test_AUC: 0.9411, Test_AP: 0.9521\n",
      "Epoch: 34, Loss: 0.3378, Val_AUC: 0.9097, Val_AP: 0.9303Test_AUC: 0.9411, Test_AP: 0.9521\n",
      "Epoch: 35, Loss: 0.3385, Val_AUC: 0.9069, Val_AP: 0.9282Test_AUC: 0.9411, Test_AP: 0.9521\n",
      "Epoch: 36, Loss: 0.3385, Val_AUC: 0.9090, Val_AP: 0.9295Test_AUC: 0.9411, Test_AP: 0.9521\n",
      "Epoch: 37, Loss: 0.3342, Val_AUC: 0.9093, Val_AP: 0.9300Test_AUC: 0.9411, Test_AP: 0.9521\n",
      "Epoch: 38, Loss: 0.3365, Val_AUC: 0.9091, Val_AP: 0.9296Test_AUC: 0.9411, Test_AP: 0.9521\n",
      "Epoch: 39, Loss: 0.3371, Val_AUC: 0.9074, Val_AP: 0.9288Test_AUC: 0.9411, Test_AP: 0.9521\n",
      "Epoch: 40, Loss: 0.3360, Val_AUC: 0.9093, Val_AP: 0.9300Test_AUC: 0.9411, Test_AP: 0.9521\n",
      "Epoch: 41, Loss: 0.3353, Val_AUC: 0.9087, Val_AP: 0.9298Test_AUC: 0.9411, Test_AP: 0.9521\n",
      "Epoch: 42, Loss: 0.3373, Val_AUC: 0.9099, Val_AP: 0.9309Test_AUC: 0.9411, Test_AP: 0.9521\n",
      "Epoch: 43, Loss: 0.3363, Val_AUC: 0.9063, Val_AP: 0.9294Test_AUC: 0.9411, Test_AP: 0.9521\n",
      "Epoch: 44, Loss: 0.3360, Val_AUC: 0.9086, Val_AP: 0.9307Test_AUC: 0.9411, Test_AP: 0.9521\n",
      "Epoch: 45, Loss: 0.3330, Val_AUC: 0.9085, Val_AP: 0.9306Test_AUC: 0.9411, Test_AP: 0.9521\n",
      "Epoch: 46, Loss: 0.3330, Val_AUC: 0.9065, Val_AP: 0.9295Test_AUC: 0.9411, Test_AP: 0.9521\n",
      "Epoch: 47, Loss: 0.3336, Val_AUC: 0.9096, Val_AP: 0.9309Test_AUC: 0.9411, Test_AP: 0.9521\n",
      "Epoch: 48, Loss: 0.3326, Val_AUC: 0.9076, Val_AP: 0.9304Test_AUC: 0.9411, Test_AP: 0.9521\n",
      "Epoch: 49, Loss: 0.3338, Val_AUC: 0.9084, Val_AP: 0.9306Test_AUC: 0.9411, Test_AP: 0.9521\n",
      "Epoch: 50, Loss: 0.3319, Val_AUC: 0.9077, Val_AP: 0.9302Test_AUC: 0.9411, Test_AP: 0.9521\n",
      "Median time per epoch: 7.6287s\n"
     ]
    }
   ],
   "source": [
    "times = []\n",
    "best_val_auc = test_auc = 0\n",
    "for epoch in range(1, 51):\n",
    "    start = time.time()\n",
    "    loss = train()\n",
    "    val_auc, val_ap = test(val_loader)\n",
    "    if val_auc > best_val_auc:\n",
    "        best_val_auc = val_auc\n",
    "        test_auc, test_ap = test(test_loader)\n",
    "    print(f'Epoch: {epoch:02d}, Loss: {loss:.4f}, Val_AUC: {val_auc:.4f}, Val_AP: {val_ap:.4f}'\n",
    "          f'Test_AUC: {test_auc:.4f}, Test_AP: {test_ap:.4f}')\n",
    "    times.append(time.time() - start)\n",
    "print(f\"Median time per epoch: {torch.tensor(times).median():.4f}s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
