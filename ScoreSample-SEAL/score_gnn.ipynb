{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fd51917",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T21:12:58.656745Z",
     "iopub.status.busy": "2025-05-28T21:12:58.656745Z",
     "iopub.status.idle": "2025-05-28T21:13:05.507051Z",
     "shell.execute_reply": "2025-05-28T21:13:05.507051Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import copy, os\n",
    "import logging\n",
    "import numpy as np\n",
    "import out_manager as om\n",
    "from config import Config\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.utils import negative_sampling\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from model.score_gnn import scoregnn_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90015b11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T21:13:05.508799Z",
     "iopub.status.busy": "2025-05-28T21:13:05.508799Z",
     "iopub.status.idle": "2025-05-28T21:13:05.542155Z",
     "shell.execute_reply": "2025-05-28T21:13:05.542155Z"
    }
   },
   "outputs": [],
   "source": [
    "config = Config()\n",
    "ModelClass = scoregnn_dict[config.scoregnn.gnn_type]\n",
    "out_dir = om.get_out_dir(config)\n",
    "log_path = os.path.join(out_dir, \"scoregnn_log.txt\")\n",
    "om.setup_logging(log_path)\n",
    "seed = config.seed\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "device = config.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44cc6aae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T21:13:05.542155Z",
     "iopub.status.busy": "2025-05-28T21:13:05.542155Z",
     "iopub.status.idle": "2025-05-28T21:13:05.550017Z",
     "shell.execute_reply": "2025-05-28T21:13:05.550017Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, data, optimizer, predictor):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index)\n",
    "\n",
    "    # #每次手动采样负样本，效果更好\n",
    "    # neg_edge_index = negative_sampling(\n",
    "    #     edge_index = data.edge_index,\n",
    "    #     num_nodes = data.num_nodes,\n",
    "    #     num_neg_sample = data.edge_index.size(1)\n",
    "    # )\n",
    "\n",
    "    # edge_label_index = torch.cat([\n",
    "    #     data.edge_label_index, neg_edge_index\n",
    "    #     ], dim=-1)\n",
    "    # edge_label = torch.cat([\n",
    "    #     data.edge_label, data.edge_label.new_zeros(neg_edge_index.size(1))\n",
    "    #     ],dim = 0)\n",
    "\n",
    "    edge_label_index = torch.cat([\n",
    "        data.pos_edge_label_index, data.neg_edge_label_index\n",
    "    ], dim=1)\n",
    "    edge_label = torch.cat([\n",
    "        data.pos_edge_label, data.neg_edge_label\n",
    "    ], dim=0)\n",
    "    \n",
    "    score = predictor(out, edge_label_index)\n",
    "    loss = F.binary_cross_entropy_with_logits(score, edge_label)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1be434a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T21:13:05.550017Z",
     "iopub.status.busy": "2025-05-28T21:13:05.550017Z",
     "iopub.status.idle": "2025-05-28T21:13:05.557282Z",
     "shell.execute_reply": "2025-05-28T21:13:05.557282Z"
    }
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test(model, data, predictor):\n",
    "    model.eval()\n",
    "\n",
    "    edge_label_index = torch.cat([\n",
    "        data.pos_edge_label_index, data.neg_edge_label_index\n",
    "    ], dim=1)\n",
    "    edge_label = torch.cat([\n",
    "        data.pos_edge_label, data.neg_edge_label\n",
    "    ], dim=0)\n",
    "\n",
    "    out = model(data.x, data.edge_index)\n",
    "    score = predictor(out, edge_label_index).cpu().numpy()\n",
    "    auc = roc_auc_score(edge_label.cpu().numpy(), score)\n",
    "    ap = average_precision_score(edge_label.cpu().numpy(), score)\n",
    "    return auc, ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8491527b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T21:13:05.557282Z",
     "iopub.status.busy": "2025-05-28T21:13:05.557282Z",
     "iopub.status.idle": "2025-05-28T21:13:12.926352Z",
     "shell.execute_reply": "2025-05-28T21:13:12.926352Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 0.7114 Train_AUC: 0.9023, Train_AP: 0.9050 Val_AUC: 0.8598, Val_AP: 0.8729 Test_AUC: 0.8844, Test_AP: 0.9005\n",
      "Epoch: 002, Loss: 0.5827 Train_AUC: 0.8768, Train_AP: 0.8933 Val_AUC: 0.8491, Val_AP: 0.8685 Test_AUC: 0.8670, Test_AP: 0.8910\n",
      "Epoch: 003, Loss: 0.5719 Train_AUC: 0.8739, Train_AP: 0.8854 Val_AUC: 0.8347, Val_AP: 0.8563 Test_AUC: 0.8568, Test_AP: 0.8820\n",
      "Epoch: 004, Loss: 0.5608 Train_AUC: 0.8774, Train_AP: 0.8834 Val_AUC: 0.8256, Val_AP: 0.8491 Test_AUC: 0.8528, Test_AP: 0.8774\n",
      "Epoch: 005, Loss: 0.5495 Train_AUC: 0.8586, Train_AP: 0.8710 Val_AUC: 0.8084, Val_AP: 0.8391 Test_AUC: 0.8321, Test_AP: 0.8646\n",
      "Epoch: 006, Loss: 0.5467 Train_AUC: 0.8725, Train_AP: 0.8792 Val_AUC: 0.8207, Val_AP: 0.8457 Test_AUC: 0.8502, Test_AP: 0.8747\n",
      "Epoch: 007, Loss: 0.5451 Train_AUC: 0.8723, Train_AP: 0.8790 Val_AUC: 0.8208, Val_AP: 0.8460 Test_AUC: 0.8502, Test_AP: 0.8748\n",
      "Epoch: 008, Loss: 0.5473 Train_AUC: 0.8729, Train_AP: 0.8796 Val_AUC: 0.8221, Val_AP: 0.8470 Test_AUC: 0.8519, Test_AP: 0.8761\n",
      "Epoch: 009, Loss: 0.5405 Train_AUC: 0.8732, Train_AP: 0.8797 Val_AUC: 0.8228, Val_AP: 0.8469 Test_AUC: 0.8526, Test_AP: 0.8761\n",
      "Epoch: 010, Loss: 0.5427 Train_AUC: 0.8729, Train_AP: 0.8796 Val_AUC: 0.8222, Val_AP: 0.8468 Test_AUC: 0.8522, Test_AP: 0.8759\n",
      "Epoch: 011, Loss: 0.5356 Train_AUC: 0.8710, Train_AP: 0.8786 Val_AUC: 0.8203, Val_AP: 0.8459 Test_AUC: 0.8507, Test_AP: 0.8753\n",
      "Epoch: 012, Loss: 0.5371 Train_AUC: 0.8700, Train_AP: 0.8774 Val_AUC: 0.8193, Val_AP: 0.8450 Test_AUC: 0.8488, Test_AP: 0.8739\n",
      "Epoch: 013, Loss: 0.5366 Train_AUC: 0.8690, Train_AP: 0.8766 Val_AUC: 0.8184, Val_AP: 0.8442 Test_AUC: 0.8480, Test_AP: 0.8731\n",
      "Epoch: 014, Loss: 0.5306 Train_AUC: 0.8684, Train_AP: 0.8761 Val_AUC: 0.8177, Val_AP: 0.8437 Test_AUC: 0.8474, Test_AP: 0.8727\n",
      "Epoch: 015, Loss: 0.5350 Train_AUC: 0.8676, Train_AP: 0.8758 Val_AUC: 0.8170, Val_AP: 0.8429 Test_AUC: 0.8456, Test_AP: 0.8715\n",
      "Epoch: 016, Loss: 0.5282 Train_AUC: 0.8673, Train_AP: 0.8754 Val_AUC: 0.8169, Val_AP: 0.8430 Test_AUC: 0.8463, Test_AP: 0.8719\n",
      "Epoch: 017, Loss: 0.5350 Train_AUC: 0.8667, Train_AP: 0.8749 Val_AUC: 0.8166, Val_AP: 0.8428 Test_AUC: 0.8459, Test_AP: 0.8716\n",
      "Epoch: 018, Loss: 0.5303 Train_AUC: 0.8657, Train_AP: 0.8744 Val_AUC: 0.8157, Val_AP: 0.8424 Test_AUC: 0.8450, Test_AP: 0.8712\n",
      "Epoch: 019, Loss: 0.5328 Train_AUC: 0.8659, Train_AP: 0.8747 Val_AUC: 0.8160, Val_AP: 0.8428 Test_AUC: 0.8468, Test_AP: 0.8724\n",
      "Epoch: 020, Loss: 0.5309 Train_AUC: 0.8669, Train_AP: 0.8750 Val_AUC: 0.8168, Val_AP: 0.8430 Test_AUC: 0.8469, Test_AP: 0.8721\n",
      "Epoch: 021, Loss: 0.5305 Train_AUC: 0.8659, Train_AP: 0.8743 Val_AUC: 0.8159, Val_AP: 0.8424 Test_AUC: 0.8454, Test_AP: 0.8711\n",
      "Epoch: 022, Loss: 0.5280 Train_AUC: 0.8649, Train_AP: 0.8736 Val_AUC: 0.8151, Val_AP: 0.8419 Test_AUC: 0.8444, Test_AP: 0.8705\n",
      "Epoch: 023, Loss: 0.5309 Train_AUC: 0.8639, Train_AP: 0.8730 Val_AUC: 0.8145, Val_AP: 0.8415 Test_AUC: 0.8435, Test_AP: 0.8699\n",
      "Epoch: 024, Loss: 0.5209 Train_AUC: 0.8630, Train_AP: 0.8724 Val_AUC: 0.8137, Val_AP: 0.8409 Test_AUC: 0.8427, Test_AP: 0.8694\n",
      "Epoch: 025, Loss: 0.5260 Train_AUC: 0.8621, Train_AP: 0.8718 Val_AUC: 0.8129, Val_AP: 0.8405 Test_AUC: 0.8418, Test_AP: 0.8689\n",
      "Epoch: 026, Loss: 0.5202 Train_AUC: 0.8610, Train_AP: 0.8712 Val_AUC: 0.8121, Val_AP: 0.8400 Test_AUC: 0.8409, Test_AP: 0.8683\n",
      "Epoch: 027, Loss: 0.5191 Train_AUC: 0.8602, Train_AP: 0.8707 Val_AUC: 0.8115, Val_AP: 0.8397 Test_AUC: 0.8401, Test_AP: 0.8680\n",
      "Epoch: 028, Loss: 0.5235 Train_AUC: 0.8593, Train_AP: 0.8703 Val_AUC: 0.8108, Val_AP: 0.8393 Test_AUC: 0.8394, Test_AP: 0.8677\n",
      "Epoch: 029, Loss: 0.5223 Train_AUC: 0.8585, Train_AP: 0.8702 Val_AUC: 0.8105, Val_AP: 0.8392 Test_AUC: 0.8389, Test_AP: 0.8676\n",
      "Epoch: 030, Loss: 0.5210 Train_AUC: 0.8579, Train_AP: 0.8700 Val_AUC: 0.8094, Val_AP: 0.8390 Test_AUC: 0.8380, Test_AP: 0.8673\n",
      "Epoch: 031, Loss: 0.5199 Train_AUC: 0.8574, Train_AP: 0.8698 Val_AUC: 0.8096, Val_AP: 0.8392 Test_AUC: 0.8382, Test_AP: 0.8676\n",
      "Epoch: 032, Loss: 0.5166 Train_AUC: 0.8567, Train_AP: 0.8696 Val_AUC: 0.8096, Val_AP: 0.8393 Test_AUC: 0.8381, Test_AP: 0.8677\n",
      "Epoch: 033, Loss: 0.5165 Train_AUC: 0.8561, Train_AP: 0.8694 Val_AUC: 0.8092, Val_AP: 0.8391 Test_AUC: 0.8380, Test_AP: 0.8677\n",
      "Epoch: 034, Loss: 0.5195 Train_AUC: 0.8559, Train_AP: 0.8695 Val_AUC: 0.8092, Val_AP: 0.8389 Test_AUC: 0.8375, Test_AP: 0.8674\n",
      "Epoch: 035, Loss: 0.5171 Train_AUC: 0.8557, Train_AP: 0.8696 Val_AUC: 0.8100, Val_AP: 0.8392 Test_AUC: 0.8371, Test_AP: 0.8671\n",
      "Epoch: 036, Loss: 0.5124 Train_AUC: 0.8549, Train_AP: 0.8693 Val_AUC: 0.8097, Val_AP: 0.8389 Test_AUC: 0.8362, Test_AP: 0.8666\n",
      "Epoch: 037, Loss: 0.5135 Train_AUC: 0.8534, Train_AP: 0.8685 Val_AUC: 0.8095, Val_AP: 0.8386 Test_AUC: 0.8351, Test_AP: 0.8661\n",
      "Epoch: 038, Loss: 0.5167 Train_AUC: 0.8518, Train_AP: 0.8678 Val_AUC: 0.8089, Val_AP: 0.8381 Test_AUC: 0.8342, Test_AP: 0.8654\n",
      "Epoch: 039, Loss: 0.5099 Train_AUC: 0.8504, Train_AP: 0.8671 Val_AUC: 0.8088, Val_AP: 0.8380 Test_AUC: 0.8331, Test_AP: 0.8647\n",
      "Epoch: 040, Loss: 0.5157 Train_AUC: 0.8497, Train_AP: 0.8668 Val_AUC: 0.8091, Val_AP: 0.8381 Test_AUC: 0.8325, Test_AP: 0.8644\n",
      "Epoch: 041, Loss: 0.5132 Train_AUC: 0.8490, Train_AP: 0.8666 Val_AUC: 0.8091, Val_AP: 0.8383 Test_AUC: 0.8320, Test_AP: 0.8641\n",
      "Epoch: 042, Loss: 0.5173 Train_AUC: 0.8483, Train_AP: 0.8663 Val_AUC: 0.8093, Val_AP: 0.8385 Test_AUC: 0.8316, Test_AP: 0.8639\n",
      "Epoch: 043, Loss: 0.5109 Train_AUC: 0.8475, Train_AP: 0.8660 Val_AUC: 0.8096, Val_AP: 0.8388 Test_AUC: 0.8311, Test_AP: 0.8637\n",
      "Epoch: 044, Loss: 0.5169 Train_AUC: 0.8467, Train_AP: 0.8657 Val_AUC: 0.8096, Val_AP: 0.8391 Test_AUC: 0.8307, Test_AP: 0.8637\n",
      "Epoch: 045, Loss: 0.5079 Train_AUC: 0.8460, Train_AP: 0.8656 Val_AUC: 0.8097, Val_AP: 0.8394 Test_AUC: 0.8305, Test_AP: 0.8637\n",
      "Epoch: 046, Loss: 0.5080 Train_AUC: 0.8450, Train_AP: 0.8653 Val_AUC: 0.8096, Val_AP: 0.8395 Test_AUC: 0.8298, Test_AP: 0.8635\n",
      "Epoch: 047, Loss: 0.5080 Train_AUC: 0.8440, Train_AP: 0.8650 Val_AUC: 0.8095, Val_AP: 0.8396 Test_AUC: 0.8293, Test_AP: 0.8633\n",
      "Epoch: 048, Loss: 0.5028 Train_AUC: 0.8432, Train_AP: 0.8648 Val_AUC: 0.8094, Val_AP: 0.8399 Test_AUC: 0.8288, Test_AP: 0.8632\n",
      "Epoch: 049, Loss: 0.5009 Train_AUC: 0.8423, Train_AP: 0.8645 Val_AUC: 0.8089, Val_AP: 0.8398 Test_AUC: 0.8281, Test_AP: 0.8629\n",
      "Epoch: 050, Loss: 0.5019 Train_AUC: 0.8414, Train_AP: 0.8643 Val_AUC: 0.8079, Val_AP: 0.8397 Test_AUC: 0.8272, Test_AP: 0.8625\n",
      "Epoch: 051, Loss: 0.5061 Train_AUC: 0.8412, Train_AP: 0.8646 Val_AUC: 0.8073, Val_AP: 0.8395 Test_AUC: 0.8264, Test_AP: 0.8623\n",
      "Epoch: 052, Loss: 0.5033 Train_AUC: 0.8410, Train_AP: 0.8648 Val_AUC: 0.8068, Val_AP: 0.8394 Test_AUC: 0.8257, Test_AP: 0.8620\n",
      "Epoch: 053, Loss: 0.4994 Train_AUC: 0.8408, Train_AP: 0.8649 Val_AUC: 0.8065, Val_AP: 0.8392 Test_AUC: 0.8251, Test_AP: 0.8617\n",
      "Epoch: 054, Loss: 0.4992 Train_AUC: 0.8406, Train_AP: 0.8651 Val_AUC: 0.8063, Val_AP: 0.8389 Test_AUC: 0.8246, Test_AP: 0.8615\n",
      "Epoch: 055, Loss: 0.4967 Train_AUC: 0.8414, Train_AP: 0.8660 Val_AUC: 0.8075, Val_AP: 0.8395 Test_AUC: 0.8248, Test_AP: 0.8618\n",
      "Epoch: 056, Loss: 0.5003 Train_AUC: 0.8427, Train_AP: 0.8674 Val_AUC: 0.8096, Val_AP: 0.8408 Test_AUC: 0.8253, Test_AP: 0.8621\n",
      "Epoch: 057, Loss: 0.4977 Train_AUC: 0.8440, Train_AP: 0.8692 Val_AUC: 0.8122, Val_AP: 0.8423 Test_AUC: 0.8266, Test_AP: 0.8633\n",
      "Epoch: 058, Loss: 0.5014 Train_AUC: 0.8464, Train_AP: 0.8713 Val_AUC: 0.8137, Val_AP: 0.8432 Test_AUC: 0.8292, Test_AP: 0.8647\n",
      "Epoch: 059, Loss: 0.4990 Train_AUC: 0.8490, Train_AP: 0.8732 Val_AUC: 0.8157, Val_AP: 0.8444 Test_AUC: 0.8313, Test_AP: 0.8658\n",
      "Epoch: 060, Loss: 0.4959 Train_AUC: 0.8509, Train_AP: 0.8746 Val_AUC: 0.8174, Val_AP: 0.8453 Test_AUC: 0.8329, Test_AP: 0.8666\n",
      "Epoch: 061, Loss: 0.4900 Train_AUC: 0.8529, Train_AP: 0.8761 Val_AUC: 0.8196, Val_AP: 0.8467 Test_AUC: 0.8350, Test_AP: 0.8678\n",
      "Epoch: 062, Loss: 0.4957 Train_AUC: 0.8539, Train_AP: 0.8769 Val_AUC: 0.8207, Val_AP: 0.8473 Test_AUC: 0.8362, Test_AP: 0.8684\n",
      "Epoch: 063, Loss: 0.4962 Train_AUC: 0.8545, Train_AP: 0.8772 Val_AUC: 0.8220, Val_AP: 0.8481 Test_AUC: 0.8369, Test_AP: 0.8688\n",
      "Epoch: 064, Loss: 0.4914 Train_AUC: 0.8571, Train_AP: 0.8788 Val_AUC: 0.8243, Val_AP: 0.8496 Test_AUC: 0.8390, Test_AP: 0.8701\n",
      "Epoch: 065, Loss: 0.4975 Train_AUC: 0.8605, Train_AP: 0.8809 Val_AUC: 0.8267, Val_AP: 0.8510 Test_AUC: 0.8419, Test_AP: 0.8716\n",
      "Epoch: 066, Loss: 0.4883 Train_AUC: 0.8653, Train_AP: 0.8838 Val_AUC: 0.8296, Val_AP: 0.8527 Test_AUC: 0.8457, Test_AP: 0.8739\n",
      "Epoch: 067, Loss: 0.4893 Train_AUC: 0.8708, Train_AP: 0.8872 Val_AUC: 0.8339, Val_AP: 0.8553 Test_AUC: 0.8508, Test_AP: 0.8769\n",
      "Epoch: 068, Loss: 0.4891 Train_AUC: 0.8742, Train_AP: 0.8895 Val_AUC: 0.8377, Val_AP: 0.8575 Test_AUC: 0.8544, Test_AP: 0.8791\n",
      "Epoch: 069, Loss: 0.4833 Train_AUC: 0.8778, Train_AP: 0.8919 Val_AUC: 0.8409, Val_AP: 0.8595 Test_AUC: 0.8584, Test_AP: 0.8816\n",
      "Epoch: 070, Loss: 0.4824 Train_AUC: 0.8813, Train_AP: 0.8941 Val_AUC: 0.8439, Val_AP: 0.8615 Test_AUC: 0.8624, Test_AP: 0.8841\n",
      "Epoch: 071, Loss: 0.4870 Train_AUC: 0.8822, Train_AP: 0.8945 Val_AUC: 0.8447, Val_AP: 0.8619 Test_AUC: 0.8634, Test_AP: 0.8846\n",
      "Epoch: 072, Loss: 0.4824 Train_AUC: 0.8828, Train_AP: 0.8948 Val_AUC: 0.8457, Val_AP: 0.8625 Test_AUC: 0.8644, Test_AP: 0.8852\n",
      "Epoch: 073, Loss: 0.4851 Train_AUC: 0.8843, Train_AP: 0.8956 Val_AUC: 0.8473, Val_AP: 0.8637 Test_AUC: 0.8663, Test_AP: 0.8865\n",
      "Epoch: 074, Loss: 0.4821 Train_AUC: 0.8868, Train_AP: 0.8973 Val_AUC: 0.8496, Val_AP: 0.8656 Test_AUC: 0.8693, Test_AP: 0.8887\n",
      "Epoch: 075, Loss: 0.4820 Train_AUC: 0.8886, Train_AP: 0.8984 Val_AUC: 0.8516, Val_AP: 0.8674 Test_AUC: 0.8718, Test_AP: 0.8906\n",
      "Epoch: 076, Loss: 0.4790 Train_AUC: 0.8898, Train_AP: 0.8990 Val_AUC: 0.8532, Val_AP: 0.8685 Test_AUC: 0.8734, Test_AP: 0.8918\n",
      "Epoch: 077, Loss: 0.4794 Train_AUC: 0.8907, Train_AP: 0.8995 Val_AUC: 0.8543, Val_AP: 0.8692 Test_AUC: 0.8746, Test_AP: 0.8926\n",
      "Epoch: 078, Loss: 0.4788 Train_AUC: 0.8911, Train_AP: 0.8996 Val_AUC: 0.8554, Val_AP: 0.8700 Test_AUC: 0.8754, Test_AP: 0.8932\n",
      "Epoch: 079, Loss: 0.4787 Train_AUC: 0.8918, Train_AP: 0.9001 Val_AUC: 0.8568, Val_AP: 0.8711 Test_AUC: 0.8767, Test_AP: 0.8943\n",
      "Epoch: 080, Loss: 0.4774 Train_AUC: 0.8927, Train_AP: 0.9006 Val_AUC: 0.8579, Val_AP: 0.8721 Test_AUC: 0.8781, Test_AP: 0.8954\n",
      "Epoch: 081, Loss: 0.4781 Train_AUC: 0.8933, Train_AP: 0.9011 Val_AUC: 0.8590, Val_AP: 0.8730 Test_AUC: 0.8792, Test_AP: 0.8965\n",
      "Epoch: 082, Loss: 0.4719 Train_AUC: 0.8938, Train_AP: 0.9015 Val_AUC: 0.8601, Val_AP: 0.8738 Test_AUC: 0.8802, Test_AP: 0.8973\n",
      "Epoch: 083, Loss: 0.4739 Train_AUC: 0.8938, Train_AP: 0.9016 Val_AUC: 0.8606, Val_AP: 0.8743 Test_AUC: 0.8806, Test_AP: 0.8978\n",
      "Epoch: 084, Loss: 0.4755 Train_AUC: 0.8937, Train_AP: 0.9016 Val_AUC: 0.8608, Val_AP: 0.8744 Test_AUC: 0.8808, Test_AP: 0.8979\n",
      "Epoch: 085, Loss: 0.4744 Train_AUC: 0.8937, Train_AP: 0.9016 Val_AUC: 0.8610, Val_AP: 0.8746 Test_AUC: 0.8809, Test_AP: 0.8981\n",
      "Epoch: 086, Loss: 0.4734 Train_AUC: 0.8939, Train_AP: 0.9017 Val_AUC: 0.8612, Val_AP: 0.8748 Test_AUC: 0.8812, Test_AP: 0.8984\n",
      "Epoch: 087, Loss: 0.4765 Train_AUC: 0.8940, Train_AP: 0.9018 Val_AUC: 0.8611, Val_AP: 0.8748 Test_AUC: 0.8811, Test_AP: 0.8985\n",
      "Epoch: 088, Loss: 0.4720 Train_AUC: 0.8942, Train_AP: 0.9019 Val_AUC: 0.8611, Val_AP: 0.8749 Test_AUC: 0.8813, Test_AP: 0.8987\n",
      "Epoch: 089, Loss: 0.4727 Train_AUC: 0.8944, Train_AP: 0.9020 Val_AUC: 0.8611, Val_AP: 0.8749 Test_AUC: 0.8815, Test_AP: 0.8988\n",
      "Epoch: 090, Loss: 0.4715 Train_AUC: 0.8945, Train_AP: 0.9021 Val_AUC: 0.8611, Val_AP: 0.8749 Test_AUC: 0.8815, Test_AP: 0.8990\n",
      "Epoch: 091, Loss: 0.4718 Train_AUC: 0.8949, Train_AP: 0.9023 Val_AUC: 0.8613, Val_AP: 0.8751 Test_AUC: 0.8819, Test_AP: 0.8993\n",
      "Epoch: 092, Loss: 0.4675 Train_AUC: 0.8952, Train_AP: 0.9025 Val_AUC: 0.8617, Val_AP: 0.8755 Test_AUC: 0.8824, Test_AP: 0.8997\n",
      "Epoch: 093, Loss: 0.4689 Train_AUC: 0.8954, Train_AP: 0.9027 Val_AUC: 0.8621, Val_AP: 0.8758 Test_AUC: 0.8829, Test_AP: 0.9000\n",
      "Epoch: 094, Loss: 0.4704 Train_AUC: 0.8953, Train_AP: 0.9026 Val_AUC: 0.8623, Val_AP: 0.8760 Test_AUC: 0.8832, Test_AP: 0.9002\n",
      "Epoch: 095, Loss: 0.4674 Train_AUC: 0.8952, Train_AP: 0.9026 Val_AUC: 0.8626, Val_AP: 0.8762 Test_AUC: 0.8835, Test_AP: 0.9004\n",
      "Epoch: 096, Loss: 0.4676 Train_AUC: 0.8953, Train_AP: 0.9027 Val_AUC: 0.8630, Val_AP: 0.8764 Test_AUC: 0.8839, Test_AP: 0.9006\n",
      "Epoch: 097, Loss: 0.4647 Train_AUC: 0.8955, Train_AP: 0.9028 Val_AUC: 0.8631, Val_AP: 0.8766 Test_AUC: 0.8839, Test_AP: 0.9006\n",
      "Epoch: 098, Loss: 0.4674 Train_AUC: 0.8960, Train_AP: 0.9032 Val_AUC: 0.8632, Val_AP: 0.8767 Test_AUC: 0.8837, Test_AP: 0.9006\n",
      "Epoch: 099, Loss: 0.4644 Train_AUC: 0.8964, Train_AP: 0.9035 Val_AUC: 0.8641, Val_AP: 0.8773 Test_AUC: 0.8835, Test_AP: 0.9004\n",
      "Epoch: 100, Loss: 0.4631 Train_AUC: 0.8973, Train_AP: 0.9041 Val_AUC: 0.8643, Val_AP: 0.8775 Test_AUC: 0.8839, Test_AP: 0.9008\n",
      "Epoch: 101, Loss: 0.4681 Train_AUC: 0.8972, Train_AP: 0.9043 Val_AUC: 0.8646, Val_AP: 0.8778 Test_AUC: 0.8841, Test_AP: 0.9010\n",
      "Epoch: 102, Loss: 0.4625 Train_AUC: 0.8974, Train_AP: 0.9046 Val_AUC: 0.8650, Val_AP: 0.8781 Test_AUC: 0.8843, Test_AP: 0.9012\n",
      "Epoch: 103, Loss: 0.4596 Train_AUC: 0.8973, Train_AP: 0.9047 Val_AUC: 0.8650, Val_AP: 0.8781 Test_AUC: 0.8843, Test_AP: 0.9012\n",
      "Epoch: 104, Loss: 0.4601 Train_AUC: 0.8985, Train_AP: 0.9055 Val_AUC: 0.8655, Val_AP: 0.8784 Test_AUC: 0.8846, Test_AP: 0.9015\n",
      "Epoch: 105, Loss: 0.4614 Train_AUC: 0.8995, Train_AP: 0.9063 Val_AUC: 0.8657, Val_AP: 0.8785 Test_AUC: 0.8845, Test_AP: 0.9014\n",
      "Epoch: 106, Loss: 0.4576 Train_AUC: 0.9009, Train_AP: 0.9074 Val_AUC: 0.8661, Val_AP: 0.8788 Test_AUC: 0.8847, Test_AP: 0.9017\n",
      "Epoch: 107, Loss: 0.4582 Train_AUC: 0.9019, Train_AP: 0.9081 Val_AUC: 0.8665, Val_AP: 0.8793 Test_AUC: 0.8853, Test_AP: 0.9023\n",
      "Epoch: 108, Loss: 0.4558 Train_AUC: 0.9052, Train_AP: 0.9102 Val_AUC: 0.8675, Val_AP: 0.8800 Test_AUC: 0.8860, Test_AP: 0.9031\n",
      "Epoch: 109, Loss: 0.4545 Train_AUC: 0.9066, Train_AP: 0.9111 Val_AUC: 0.8685, Val_AP: 0.8807 Test_AUC: 0.8868, Test_AP: 0.9037\n",
      "Epoch: 110, Loss: 0.4536 Train_AUC: 0.9018, Train_AP: 0.9091 Val_AUC: 0.8664, Val_AP: 0.8801 Test_AUC: 0.8850, Test_AP: 0.9029\n",
      "Epoch: 111, Loss: 0.4554 Train_AUC: 0.8994, Train_AP: 0.9076 Val_AUC: 0.8655, Val_AP: 0.8798 Test_AUC: 0.8838, Test_AP: 0.9020\n",
      "Epoch: 112, Loss: 0.4537 Train_AUC: 0.8807, Train_AP: 0.8976 Val_AUC: 0.8516, Val_AP: 0.8725 Test_AUC: 0.8720, Test_AP: 0.8941\n",
      "Epoch: 113, Loss: 0.4505 Train_AUC: 0.8844, Train_AP: 0.8998 Val_AUC: 0.8547, Val_AP: 0.8739 Test_AUC: 0.8741, Test_AP: 0.8952\n",
      "Epoch: 114, Loss: 0.4498 Train_AUC: 0.9071, Train_AP: 0.9111 Val_AUC: 0.8674, Val_AP: 0.8802 Test_AUC: 0.8864, Test_AP: 0.9034\n",
      "Epoch: 115, Loss: 0.4481 Train_AUC: 0.9062, Train_AP: 0.9102 Val_AUC: 0.8662, Val_AP: 0.8792 Test_AUC: 0.8872, Test_AP: 0.9038\n",
      "Epoch: 116, Loss: 0.4502 Train_AUC: 0.9077, Train_AP: 0.9113 Val_AUC: 0.8670, Val_AP: 0.8798 Test_AUC: 0.8875, Test_AP: 0.9041\n",
      "Epoch: 117, Loss: 0.4484 Train_AUC: 0.9084, Train_AP: 0.9123 Val_AUC: 0.8688, Val_AP: 0.8811 Test_AUC: 0.8875, Test_AP: 0.9044\n",
      "Epoch: 118, Loss: 0.4468 Train_AUC: 0.8521, Train_AP: 0.8820 Val_AUC: 0.8366, Val_AP: 0.8638 Test_AUC: 0.8575, Test_AP: 0.8850\n",
      "Epoch: 119, Loss: 0.4498 Train_AUC: 0.8839, Train_AP: 0.9019 Val_AUC: 0.8485, Val_AP: 0.8725 Test_AUC: 0.8707, Test_AP: 0.8960\n",
      "Epoch: 120, Loss: 0.4471 Train_AUC: 0.9088, Train_AP: 0.9123 Val_AUC: 0.8670, Val_AP: 0.8798 Test_AUC: 0.8865, Test_AP: 0.9034\n",
      "Epoch: 121, Loss: 0.4483 Train_AUC: 0.9085, Train_AP: 0.9118 Val_AUC: 0.8681, Val_AP: 0.8803 Test_AUC: 0.8865, Test_AP: 0.9032\n",
      "Epoch: 122, Loss: 0.4461 Train_AUC: 0.9069, Train_AP: 0.9101 Val_AUC: 0.8621, Val_AP: 0.8764 Test_AUC: 0.8837, Test_AP: 0.9012\n",
      "Epoch: 123, Loss: 0.4490 Train_AUC: 0.9091, Train_AP: 0.9117 Val_AUC: 0.8648, Val_AP: 0.8779 Test_AUC: 0.8853, Test_AP: 0.9023\n",
      "Epoch: 124, Loss: 0.4454 Train_AUC: 0.9091, Train_AP: 0.9115 Val_AUC: 0.8639, Val_AP: 0.8771 Test_AUC: 0.8843, Test_AP: 0.9013\n",
      "Epoch: 125, Loss: 0.4446 Train_AUC: 0.9091, Train_AP: 0.9106 Val_AUC: 0.8586, Val_AP: 0.8741 Test_AUC: 0.8808, Test_AP: 0.8986\n",
      "Epoch: 126, Loss: 0.4436 Train_AUC: 0.8865, Train_AP: 0.8966 Val_AUC: 0.8300, Val_AP: 0.8586 Test_AUC: 0.8503, Test_AP: 0.8802\n",
      "Epoch: 127, Loss: 0.4408 Train_AUC: 0.9037, Train_AP: 0.9088 Val_AUC: 0.8679, Val_AP: 0.8797 Test_AUC: 0.8866, Test_AP: 0.9028\n",
      "Epoch: 128, Loss: 0.4399 Train_AUC: 0.9072, Train_AP: 0.9104 Val_AUC: 0.8507, Val_AP: 0.8704 Test_AUC: 0.8749, Test_AP: 0.8961\n",
      "Epoch: 129, Loss: 0.4435 Train_AUC: 0.9048, Train_AP: 0.9082 Val_AUC: 0.8636, Val_AP: 0.8770 Test_AUC: 0.8847, Test_AP: 0.9015\n",
      "Epoch: 130, Loss: 0.4424 Train_AUC: 0.9034, Train_AP: 0.9069 Val_AUC: 0.8637, Val_AP: 0.8768 Test_AUC: 0.8849, Test_AP: 0.9013\n",
      "Epoch: 131, Loss: 0.4387 Train_AUC: 0.9011, Train_AP: 0.9052 Val_AUC: 0.8599, Val_AP: 0.8744 Test_AUC: 0.8828, Test_AP: 0.8999\n",
      "Epoch: 132, Loss: 0.4380 Train_AUC: 0.8989, Train_AP: 0.9036 Val_AUC: 0.8588, Val_AP: 0.8737 Test_AUC: 0.8819, Test_AP: 0.8991\n",
      "Epoch: 133, Loss: 0.4425 Train_AUC: 0.8991, Train_AP: 0.9036 Val_AUC: 0.8593, Val_AP: 0.8739 Test_AUC: 0.8821, Test_AP: 0.8992\n",
      "Epoch: 134, Loss: 0.4356 Train_AUC: 0.8976, Train_AP: 0.9025 Val_AUC: 0.8596, Val_AP: 0.8737 Test_AUC: 0.8822, Test_AP: 0.8993\n",
      "Epoch: 135, Loss: 0.4377 Train_AUC: 0.8967, Train_AP: 0.9020 Val_AUC: 0.8577, Val_AP: 0.8730 Test_AUC: 0.8813, Test_AP: 0.8985\n",
      "Epoch: 136, Loss: 0.4361 Train_AUC: 0.8957, Train_AP: 0.9005 Val_AUC: 0.8595, Val_AP: 0.8735 Test_AUC: 0.8824, Test_AP: 0.8989\n",
      "Epoch: 137, Loss: 0.4381 Train_AUC: 0.8987, Train_AP: 0.9028 Val_AUC: 0.8631, Val_AP: 0.8757 Test_AUC: 0.8848, Test_AP: 0.9005\n",
      "Epoch: 138, Loss: 0.4358 Train_AUC: 0.9002, Train_AP: 0.9044 Val_AUC: 0.8643, Val_AP: 0.8765 Test_AUC: 0.8849, Test_AP: 0.9009\n",
      "Epoch: 139, Loss: 0.4397 Train_AUC: 0.8988, Train_AP: 0.9036 Val_AUC: 0.8628, Val_AP: 0.8759 Test_AUC: 0.8847, Test_AP: 0.9008\n",
      "Epoch: 140, Loss: 0.4338 Train_AUC: 0.9013, Train_AP: 0.9054 Val_AUC: 0.8643, Val_AP: 0.8767 Test_AUC: 0.8852, Test_AP: 0.9014\n",
      "Epoch: 141, Loss: 0.4368 Train_AUC: 0.9002, Train_AP: 0.9046 Val_AUC: 0.8627, Val_AP: 0.8759 Test_AUC: 0.8851, Test_AP: 0.9011\n",
      "Epoch: 142, Loss: 0.4318 Train_AUC: 0.8995, Train_AP: 0.9042 Val_AUC: 0.8630, Val_AP: 0.8761 Test_AUC: 0.8850, Test_AP: 0.9010\n",
      "Epoch: 143, Loss: 0.4320 Train_AUC: 0.8998, Train_AP: 0.9044 Val_AUC: 0.8631, Val_AP: 0.8762 Test_AUC: 0.8851, Test_AP: 0.9011\n",
      "Epoch: 144, Loss: 0.4343 Train_AUC: 0.9005, Train_AP: 0.9049 Val_AUC: 0.8627, Val_AP: 0.8760 Test_AUC: 0.8852, Test_AP: 0.9013\n",
      "Epoch: 145, Loss: 0.4317 Train_AUC: 0.9009, Train_AP: 0.9053 Val_AUC: 0.8628, Val_AP: 0.8761 Test_AUC: 0.8850, Test_AP: 0.9013\n",
      "Epoch: 146, Loss: 0.4321 Train_AUC: 0.9008, Train_AP: 0.9054 Val_AUC: 0.8622, Val_AP: 0.8760 Test_AUC: 0.8845, Test_AP: 0.9012\n",
      "Epoch: 147, Loss: 0.4330 Train_AUC: 0.9000, Train_AP: 0.9053 Val_AUC: 0.8646, Val_AP: 0.8773 Test_AUC: 0.8849, Test_AP: 0.9014\n",
      "Epoch: 148, Loss: 0.4352 Train_AUC: 0.8987, Train_AP: 0.9045 Val_AUC: 0.8650, Val_AP: 0.8776 Test_AUC: 0.8851, Test_AP: 0.9013\n",
      "Epoch: 149, Loss: 0.4331 Train_AUC: 0.8988, Train_AP: 0.9045 Val_AUC: 0.8646, Val_AP: 0.8774 Test_AUC: 0.8849, Test_AP: 0.9011\n",
      "Epoch: 150, Loss: 0.4340 Train_AUC: 0.9004, Train_AP: 0.9055 Val_AUC: 0.8649, Val_AP: 0.8776 Test_AUC: 0.8853, Test_AP: 0.9016\n",
      "Epoch: 151, Loss: 0.4312 Train_AUC: 0.8988, Train_AP: 0.9056 Val_AUC: 0.8653, Val_AP: 0.8779 Test_AUC: 0.8848, Test_AP: 0.9015\n",
      "Epoch: 152, Loss: 0.4335 Train_AUC: 0.9088, Train_AP: 0.9111 Val_AUC: 0.8659, Val_AP: 0.8781 Test_AUC: 0.8858, Test_AP: 0.9021\n",
      "Epoch: 153, Loss: 0.4338 Train_AUC: 0.9027, Train_AP: 0.9075 Val_AUC: 0.8668, Val_AP: 0.8791 Test_AUC: 0.8874, Test_AP: 0.9031\n",
      "Epoch: 154, Loss: 0.4304 Train_AUC: 0.9011, Train_AP: 0.9062 Val_AUC: 0.8658, Val_AP: 0.8782 Test_AUC: 0.8859, Test_AP: 0.9020\n",
      "Epoch: 155, Loss: 0.4341 Train_AUC: 0.9073, Train_AP: 0.9101 Val_AUC: 0.8665, Val_AP: 0.8784 Test_AUC: 0.8872, Test_AP: 0.9031\n",
      "Epoch: 156, Loss: 0.4284 Train_AUC: 0.9044, Train_AP: 0.9079 Val_AUC: 0.8642, Val_AP: 0.8777 Test_AUC: 0.8865, Test_AP: 0.9025\n",
      "Epoch: 157, Loss: 0.4252 Train_AUC: 0.9029, Train_AP: 0.9069 Val_AUC: 0.8642, Val_AP: 0.8771 Test_AUC: 0.8857, Test_AP: 0.9020\n",
      "Epoch: 158, Loss: 0.4265 Train_AUC: 0.9009, Train_AP: 0.9056 Val_AUC: 0.8644, Val_AP: 0.8772 Test_AUC: 0.8858, Test_AP: 0.9019\n",
      "Epoch: 159, Loss: 0.4257 Train_AUC: 0.9027, Train_AP: 0.9070 Val_AUC: 0.8657, Val_AP: 0.8785 Test_AUC: 0.8866, Test_AP: 0.9025\n",
      "Epoch: 160, Loss: 0.4245 Train_AUC: 0.9019, Train_AP: 0.9065 Val_AUC: 0.8646, Val_AP: 0.8773 Test_AUC: 0.8854, Test_AP: 0.9020\n",
      "Epoch: 161, Loss: 0.4277 Train_AUC: 0.9053, Train_AP: 0.9090 Val_AUC: 0.8663, Val_AP: 0.8783 Test_AUC: 0.8861, Test_AP: 0.9027\n",
      "Epoch: 162, Loss: 0.4234 Train_AUC: 0.9084, Train_AP: 0.9108 Val_AUC: 0.8665, Val_AP: 0.8782 Test_AUC: 0.8860, Test_AP: 0.9029\n",
      "Epoch: 163, Loss: 0.4227 Train_AUC: 0.9014, Train_AP: 0.9080 Val_AUC: 0.8668, Val_AP: 0.8796 Test_AUC: 0.8855, Test_AP: 0.9030\n",
      "Epoch: 164, Loss: 0.4231 Train_AUC: 0.9015, Train_AP: 0.9066 Val_AUC: 0.8658, Val_AP: 0.8781 Test_AUC: 0.8842, Test_AP: 0.9013\n",
      "Epoch: 165, Loss: 0.4225 Train_AUC: 0.9011, Train_AP: 0.9065 Val_AUC: 0.8658, Val_AP: 0.8783 Test_AUC: 0.8854, Test_AP: 0.9019\n",
      "Epoch: 166, Loss: 0.4240 Train_AUC: 0.9016, Train_AP: 0.9070 Val_AUC: 0.8660, Val_AP: 0.8783 Test_AUC: 0.8858, Test_AP: 0.9023\n",
      "Epoch: 167, Loss: 0.4278 Train_AUC: 0.8862, Train_AP: 0.8991 Val_AUC: 0.8588, Val_AP: 0.8747 Test_AUC: 0.8791, Test_AP: 0.8982\n",
      "Epoch: 168, Loss: 0.4228 Train_AUC: 0.8895, Train_AP: 0.9019 Val_AUC: 0.8613, Val_AP: 0.8768 Test_AUC: 0.8811, Test_AP: 0.8996\n",
      "Epoch: 169, Loss: 0.4215 Train_AUC: 0.8947, Train_AP: 0.9041 Val_AUC: 0.8646, Val_AP: 0.8782 Test_AUC: 0.8842, Test_AP: 0.9013\n",
      "Epoch: 170, Loss: 0.4211 Train_AUC: 0.9067, Train_AP: 0.9099 Val_AUC: 0.8659, Val_AP: 0.8788 Test_AUC: 0.8871, Test_AP: 0.9030\n",
      "Epoch: 171, Loss: 0.4201 Train_AUC: 0.9026, Train_AP: 0.9076 Val_AUC: 0.8669, Val_AP: 0.8791 Test_AUC: 0.8850, Test_AP: 0.9011\n",
      "Epoch: 172, Loss: 0.4192 Train_AUC: 0.8937, Train_AP: 0.9005 Val_AUC: 0.8471, Val_AP: 0.8678 Test_AUC: 0.8598, Test_AP: 0.8857\n",
      "Epoch: 173, Loss: 0.4190 Train_AUC: 0.9081, Train_AP: 0.9111 Val_AUC: 0.8643, Val_AP: 0.8772 Test_AUC: 0.8849, Test_AP: 0.9011\n",
      "Epoch: 174, Loss: 0.4192 Train_AUC: 0.9107, Train_AP: 0.9130 Val_AUC: 0.8652, Val_AP: 0.8783 Test_AUC: 0.8886, Test_AP: 0.9043\n",
      "Epoch: 175, Loss: 0.4161 Train_AUC: 0.9085, Train_AP: 0.9113 Val_AUC: 0.8652, Val_AP: 0.8781 Test_AUC: 0.8888, Test_AP: 0.9043\n",
      "Epoch: 176, Loss: 0.4158 Train_AUC: 0.8804, Train_AP: 0.8942 Val_AUC: 0.8400, Val_AP: 0.8639 Test_AUC: 0.8535, Test_AP: 0.8827\n",
      "Epoch: 177, Loss: 0.4180 Train_AUC: 0.8643, Train_AP: 0.8839 Val_AUC: 0.8395, Val_AP: 0.8626 Test_AUC: 0.8542, Test_AP: 0.8815\n",
      "Epoch: 178, Loss: 0.4155 Train_AUC: 0.8994, Train_AP: 0.9049 Val_AUC: 0.8594, Val_AP: 0.8745 Test_AUC: 0.8833, Test_AP: 0.9001\n",
      "Epoch: 179, Loss: 0.4120 Train_AUC: 0.9034, Train_AP: 0.9092 Val_AUC: 0.8683, Val_AP: 0.8804 Test_AUC: 0.8891, Test_AP: 0.9045\n",
      "Epoch: 180, Loss: 0.4129 Train_AUC: 0.9077, Train_AP: 0.9106 Val_AUC: 0.8628, Val_AP: 0.8769 Test_AUC: 0.8872, Test_AP: 0.9031\n",
      "Epoch: 181, Loss: 0.4106 Train_AUC: 0.9098, Train_AP: 0.9124 Val_AUC: 0.8649, Val_AP: 0.8784 Test_AUC: 0.8883, Test_AP: 0.9043\n",
      "Epoch: 182, Loss: 0.4112 Train_AUC: 0.8886, Train_AP: 0.9027 Val_AUC: 0.8635, Val_AP: 0.8783 Test_AUC: 0.8842, Test_AP: 0.9022\n",
      "Epoch: 183, Loss: 0.4114 Train_AUC: 0.8844, Train_AP: 0.9007 Val_AUC: 0.8579, Val_AP: 0.8760 Test_AUC: 0.8785, Test_AP: 0.8985\n",
      "Epoch: 184, Loss: 0.4068 Train_AUC: 0.9006, Train_AP: 0.9076 Val_AUC: 0.8671, Val_AP: 0.8799 Test_AUC: 0.8850, Test_AP: 0.9015\n",
      "Epoch: 185, Loss: 0.4070 Train_AUC: 0.9062, Train_AP: 0.9100 Val_AUC: 0.8696, Val_AP: 0.8808 Test_AUC: 0.8863, Test_AP: 0.9020\n",
      "Epoch: 186, Loss: 0.4104 Train_AUC: 0.9102, Train_AP: 0.9114 Val_AUC: 0.8654, Val_AP: 0.8778 Test_AUC: 0.8841, Test_AP: 0.9001\n",
      "Epoch: 187, Loss: 0.4117 Train_AUC: 0.9062, Train_AP: 0.9072 Val_AUC: 0.8639, Val_AP: 0.8759 Test_AUC: 0.8811, Test_AP: 0.8972\n",
      "Epoch: 188, Loss: 0.4088 Train_AUC: 0.8842, Train_AP: 0.8894 Val_AUC: 0.8451, Val_AP: 0.8618 Test_AUC: 0.8576, Test_AP: 0.8784\n",
      "Epoch: 189, Loss: 0.4110 Train_AUC: 0.8046, Train_AP: 0.8034 Val_AUC: 0.8045, Val_AP: 0.8221 Test_AUC: 0.8222, Test_AP: 0.8417\n",
      "Epoch: 190, Loss: 0.4065 Train_AUC: 0.8483, Train_AP: 0.8623 Val_AUC: 0.8084, Val_AP: 0.8397 Test_AUC: 0.8216, Test_AP: 0.8551\n",
      "Epoch: 191, Loss: 0.4048 Train_AUC: 0.8154, Train_AP: 0.8435 Val_AUC: 0.7898, Val_AP: 0.8290 Test_AUC: 0.7991, Test_AP: 0.8405\n",
      "Epoch: 192, Loss: 0.4067 Train_AUC: 0.8297, Train_AP: 0.8525 Val_AUC: 0.8025, Val_AP: 0.8357 Test_AUC: 0.8157, Test_AP: 0.8512\n",
      "Epoch: 193, Loss: 0.4070 Train_AUC: 0.8311, Train_AP: 0.8523 Val_AUC: 0.8013, Val_AP: 0.8349 Test_AUC: 0.8179, Test_AP: 0.8523\n",
      "Epoch: 194, Loss: 0.4099 Train_AUC: 0.8338, Train_AP: 0.8543 Val_AUC: 0.8088, Val_AP: 0.8379 Test_AUC: 0.8219, Test_AP: 0.8530\n",
      "Epoch: 195, Loss: 0.4079 Train_AUC: 0.8033, Train_AP: 0.8262 Val_AUC: 0.7858, Val_AP: 0.8222 Test_AUC: 0.7980, Test_AP: 0.8362\n",
      "Epoch: 196, Loss: 0.4065 Train_AUC: 0.8044, Train_AP: 0.8292 Val_AUC: 0.7846, Val_AP: 0.8231 Test_AUC: 0.7957, Test_AP: 0.8363\n",
      "Epoch: 197, Loss: 0.4076 Train_AUC: 0.8370, Train_AP: 0.8534 Val_AUC: 0.8109, Val_AP: 0.8379 Test_AUC: 0.8258, Test_AP: 0.8543\n",
      "Epoch: 198, Loss: 0.4076 Train_AUC: 0.8607, Train_AP: 0.8708 Val_AUC: 0.8306, Val_AP: 0.8502 Test_AUC: 0.8460, Test_AP: 0.8680\n",
      "Epoch: 199, Loss: 0.4057 Train_AUC: 0.8437, Train_AP: 0.8623 Val_AUC: 0.8133, Val_AP: 0.8429 Test_AUC: 0.8307, Test_AP: 0.8604\n",
      "Epoch: 200, Loss: 0.4043 Train_AUC: 0.8603, Train_AP: 0.8736 Val_AUC: 0.8249, Val_AP: 0.8502 Test_AUC: 0.8435, Test_AP: 0.8695\n",
      "Final Test AUC: 0.8863, AP: 0.9020\n"
     ]
    }
   ],
   "source": [
    "train_data = torch.load(f'./data/{config.dataset}/split/train_data.pt').to(device)\n",
    "val_data = torch.load(f'./data/{config.dataset}/split/val_data.pt').to(device)\n",
    "test_data = torch.load(f'./data/{config.dataset}/split/test_data.pt').to(device)\n",
    "\n",
    "model = ModelClass(config.data_init_num_features, hidden_dim = config.scoregnn.hidden_dim, \n",
    "                 output_dim = config.scoregnn.output_dim , num_layers = config.scoregnn.num_layers, \n",
    "                 dropout = config.scoregnn.dropout).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config.scoregnn.lr)\n",
    "\n",
    "final_model = final_predictor = None\n",
    "best_val_auc = final_test_auc = final_test_ap = 0\n",
    "\n",
    "model.reset_parameters()\n",
    "\n",
    "for epoch in range(1, 1 + config.scoregnn.epochs):\n",
    "    loss = train(model, train_data, optimizer, config.scoregnn.predictor)\n",
    "    train_auc, train_ap = test(model, train_data, config.scoregnn.predictor)\n",
    "    val_auc, val_ap = test(model, val_data, config.scoregnn.predictor)\n",
    "    test_auc, test_ap = test(model, test_data, config.scoregnn.predictor)\n",
    "    if val_auc > best_val_auc:\n",
    "        best_val_auc = val_auc\n",
    "        final_test_auc = test_auc\n",
    "        final_test_ap = test_ap\n",
    "        final_model = copy.deepcopy(model)\n",
    "        final_predictor = copy.deepcopy(config.scoregnn.predictor)\n",
    "\n",
    "    logging.info(f'Epoch: {epoch:03d}, Loss: {loss:.4f} '\n",
    "             f'Train_AUC: {train_auc:.4f}, Train_AP: {train_ap:.4f} '\n",
    "             f'Val_AUC: {val_auc:.4f}, Val_AP: {val_ap:.4f} '\n",
    "             f'Test_AUC: {test_auc:.4f}, Test_AP: {test_ap:.4f}')\n",
    "    \n",
    "logging.info(f'Final Test AUC: {final_test_auc:.4f}, AP: {final_test_ap:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85f543ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T21:13:12.926352Z",
     "iopub.status.busy": "2025-05-28T21:13:12.926352Z",
     "iopub.status.idle": "2025-05-28T21:13:12.937868Z",
     "shell.execute_reply": "2025-05-28T21:13:12.937868Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    'model': final_model.state_dict(),\n",
    "    'predictor': final_predictor.state_dict()\n",
    "}, './model/scoregnn.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
