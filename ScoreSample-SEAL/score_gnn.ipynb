{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7fd51917",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import numpy as np\n",
    "import torch_geometric.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.utils import negative_sampling\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from model.score_gnn import ScoreGNN, DotProductPredictor, HadamardMLPPredictor, ConcatMLPPredictor\n",
    "\n",
    "seed = 2025\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "491484ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ScoreGNN(nn.Module):\n",
    "#     def __init__(self, input_dim, hidden_dim, output_dim, num_layers = 3, dropout =0.5):\n",
    "#         super().__init__()\n",
    "#         self.convs = nn.ModuleList()\n",
    "#         self.convs.append(GCNConv(input_dim, hidden_dim))\n",
    "#         for _ in range(num_layers -2):\n",
    "#             self.convs.append(GCNConv(hidden_dim,hidden_dim))\n",
    "#         self.convs.append(GCNConv(hidden_dim, output_dim))\n",
    "#         self.bns = nn.ModuleList([nn.BatchNorm1d(hidden_dim) for _ in range(num_layers - 1)])\n",
    "#         self.drops = nn.ModuleList([nn.Dropout(dropout) for _ in range(num_layers - 1)])\n",
    "\n",
    "#     def reset_parameters(self):\n",
    "#         for conv in self.convs:\n",
    "#             conv.reset_parameters()\n",
    "#         for bn in self.bns:\n",
    "#             bn.reset_parameters()\n",
    "\n",
    "#     def forward(self, x, edge_index):\n",
    "\n",
    "#         h = x\n",
    "\n",
    "#         for i in range(len(self.convs) - 1):\n",
    "#             h = self.convs[i](h, edge_index)\n",
    "#             h = self.bns[i](h)\n",
    "#             h = F.relu(h)\n",
    "#             h = self.drops[i](h)\n",
    "\n",
    "#         #最后一层没有加bn和dropout\n",
    "#         out = self.convs[-1](h, edge_index)\n",
    "\n",
    "#         return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82c67313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DotProductPredictor(nn.Module):\n",
    "#     def forward(self, out, edge_label_index):\n",
    "#         src = edge_label_index[0]\n",
    "#         dst = edge_label_index[1]\n",
    "#         return (out[src] * out[dst]).sum(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "75f62994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class HadamardMLPPredictor(nn.Module):\n",
    "#     def __init__(self, input_dim, hidden_dim=64, output_dim=1):\n",
    "#         super().__init__()\n",
    "#         self.mlp = nn.Sequential(\n",
    "#             nn.Linear(input_dim, hidden_dim),\n",
    "#             nn.BatchNorm1d(hidden_dim),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(hidden_dim, output_dim)\n",
    "#         )\n",
    "        \n",
    "#     def reset_parameters(self):\n",
    "#         for layer in self.mlp:\n",
    "#             if hasattr(layer, 'reset_parameters'):\n",
    "#                 layer.reset_parameters()\n",
    "\n",
    "#     def forward(self, out, edge_label_index):\n",
    "#         src = edge_label_index[0]\n",
    "#         dst = edge_label_index[1]\n",
    "#         # Element-wise product (Hadamard)\n",
    "#         dot = out[src] * out[dst]\n",
    "#         return self.mlp(dot).view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "812074c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ConcatMLPPredictor(nn.Module):\n",
    "#     def __init__(self, input_dim, hidden_dim=64, output_dim=1):\n",
    "#         super().__init__()\n",
    "#         self.mlp = nn.Sequential(\n",
    "#             nn.Linear(2 * input_dim, hidden_dim),\n",
    "#             nn.BatchNorm1d(hidden_dim),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(hidden_dim, output_dim)\n",
    "#         )\n",
    "\n",
    "#     def reset_parameters(self):\n",
    "#         for layer in self.mlp:\n",
    "#             if hasattr(layer, 'reset_parameters'):\n",
    "#                 layer.reset_parameters()\n",
    "\n",
    "#     def forward(self, out, edge_label_index):\n",
    "#         src = edge_label_index[0]\n",
    "#         dst = edge_label_index[1]\n",
    "#         # Concatenate embeddings\n",
    "#         h = torch.cat([out[src], out[dst]], dim=-1)\n",
    "#         return self.mlp(h).view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "44cc6aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, optimizer, predictor):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index)\n",
    "\n",
    "    # #每次手动采样负样本，效果更好\n",
    "    # neg_edge_index = negative_sampling(\n",
    "    #     edge_index = data.edge_index,\n",
    "    #     num_nodes = data.num_nodes,\n",
    "    #     num_neg_sample = data.edge_index.size(1)\n",
    "    # )\n",
    "\n",
    "    # edge_label_index = torch.cat([\n",
    "    #     data.edge_label_index, neg_edge_index\n",
    "    #     ], dim=-1)\n",
    "    # edge_label = torch.cat([\n",
    "    #     data.edge_label, data.edge_label.new_zeros(neg_edge_index.size(1))\n",
    "    #     ],dim = 0)\n",
    "\n",
    "    edge_label_index = torch.cat([\n",
    "        data.pos_edge_label_index, data.neg_edge_label_index\n",
    "    ], dim=1)\n",
    "    edge_label = torch.cat([\n",
    "        data.pos_edge_label, data.neg_edge_label\n",
    "    ], dim=0)\n",
    "    \n",
    "    score = predictor(out, edge_label_index)\n",
    "    loss = F.binary_cross_entropy_with_logits(score, edge_label)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c1be434a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test(model, data, predictor):\n",
    "    model.eval()\n",
    "\n",
    "    edge_label_index = torch.cat([\n",
    "        data.pos_edge_label_index, data.neg_edge_label_index\n",
    "    ], dim=1)\n",
    "    edge_label = torch.cat([\n",
    "        data.pos_edge_label, data.neg_edge_label\n",
    "    ], dim=0)\n",
    "\n",
    "    out = model(data.x, data.edge_index)\n",
    "    score = predictor(out, edge_label_index).cpu().numpy()\n",
    "    auc = roc_auc_score(edge_label.cpu().numpy(), score)\n",
    "    ap = average_precision_score(edge_label.cpu().numpy(), score)\n",
    "    return auc, ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "643851ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'hidden_dim': 256,\n",
    "    'output_dim': 128,\n",
    "    'num_layers': 3,\n",
    "    'dropout': 0.5,\n",
    "    'lr': 0.01,\n",
    "    'epochs': 200,\n",
    "    'predictor': HadamardMLPPredictor(input_dim=128).to(device), \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8491527b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 0.6738 Train_AUC: 0.7977, Train_AP: 0.7752 Val_AUC: 0.7133, Val_AP: 0.7397 Test_AUC: 0.6833, Test_AP: 0.7119\n",
      "Epoch: 002, Loss: 0.5769 Train_AUC: 0.7858, Train_AP: 0.7696 Val_AUC: 0.6971, Val_AP: 0.7290 Test_AUC: 0.6829, Test_AP: 0.7127\n",
      "Epoch: 003, Loss: 0.4981 Train_AUC: 0.7752, Train_AP: 0.7633 Val_AUC: 0.6922, Val_AP: 0.7258 Test_AUC: 0.6812, Test_AP: 0.7106\n",
      "Epoch: 004, Loss: 0.4334 Train_AUC: 0.7850, Train_AP: 0.7697 Val_AUC: 0.7000, Val_AP: 0.7304 Test_AUC: 0.6850, Test_AP: 0.7135\n",
      "Epoch: 005, Loss: 0.4064 Train_AUC: 0.7873, Train_AP: 0.7700 Val_AUC: 0.7002, Val_AP: 0.7306 Test_AUC: 0.6833, Test_AP: 0.7121\n",
      "Epoch: 006, Loss: 0.3869 Train_AUC: 0.7879, Train_AP: 0.7704 Val_AUC: 0.6992, Val_AP: 0.7304 Test_AUC: 0.6837, Test_AP: 0.7127\n",
      "Epoch: 007, Loss: 0.3741 Train_AUC: 0.7901, Train_AP: 0.7708 Val_AUC: 0.7004, Val_AP: 0.7300 Test_AUC: 0.6837, Test_AP: 0.7116\n",
      "Epoch: 008, Loss: 0.3627 Train_AUC: 0.7904, Train_AP: 0.7714 Val_AUC: 0.7001, Val_AP: 0.7298 Test_AUC: 0.6839, Test_AP: 0.7118\n",
      "Epoch: 009, Loss: 0.3547 Train_AUC: 0.7939, Train_AP: 0.7747 Val_AUC: 0.7051, Val_AP: 0.7330 Test_AUC: 0.6879, Test_AP: 0.7150\n",
      "Epoch: 010, Loss: 0.3454 Train_AUC: 0.7932, Train_AP: 0.7746 Val_AUC: 0.7036, Val_AP: 0.7328 Test_AUC: 0.6845, Test_AP: 0.7130\n",
      "Epoch: 011, Loss: 0.3396 Train_AUC: 0.7940, Train_AP: 0.7755 Val_AUC: 0.7035, Val_AP: 0.7325 Test_AUC: 0.6842, Test_AP: 0.7128\n",
      "Epoch: 012, Loss: 0.3336 Train_AUC: 0.7988, Train_AP: 0.7798 Val_AUC: 0.7073, Val_AP: 0.7354 Test_AUC: 0.6870, Test_AP: 0.7154\n",
      "Epoch: 013, Loss: 0.3264 Train_AUC: 0.8029, Train_AP: 0.7839 Val_AUC: 0.7108, Val_AP: 0.7394 Test_AUC: 0.6898, Test_AP: 0.7177\n",
      "Epoch: 014, Loss: 0.3223 Train_AUC: 0.8016, Train_AP: 0.7839 Val_AUC: 0.7089, Val_AP: 0.7388 Test_AUC: 0.6884, Test_AP: 0.7172\n",
      "Epoch: 015, Loss: 0.3180 Train_AUC: 0.7992, Train_AP: 0.7823 Val_AUC: 0.7078, Val_AP: 0.7373 Test_AUC: 0.6868, Test_AP: 0.7158\n",
      "Epoch: 016, Loss: 0.3164 Train_AUC: 0.7990, Train_AP: 0.7817 Val_AUC: 0.7069, Val_AP: 0.7364 Test_AUC: 0.6856, Test_AP: 0.7145\n",
      "Epoch: 017, Loss: 0.3090 Train_AUC: 0.8006, Train_AP: 0.7826 Val_AUC: 0.7078, Val_AP: 0.7367 Test_AUC: 0.6861, Test_AP: 0.7146\n",
      "Epoch: 018, Loss: 0.3061 Train_AUC: 0.7991, Train_AP: 0.7811 Val_AUC: 0.7048, Val_AP: 0.7339 Test_AUC: 0.6833, Test_AP: 0.7125\n",
      "Epoch: 019, Loss: 0.3032 Train_AUC: 0.8009, Train_AP: 0.7837 Val_AUC: 0.7056, Val_AP: 0.7348 Test_AUC: 0.6836, Test_AP: 0.7132\n",
      "Epoch: 020, Loss: 0.3010 Train_AUC: 0.8105, Train_AP: 0.7921 Val_AUC: 0.7159, Val_AP: 0.7425 Test_AUC: 0.6890, Test_AP: 0.7169\n",
      "Epoch: 021, Loss: 0.2979 Train_AUC: 0.8059, Train_AP: 0.7901 Val_AUC: 0.7101, Val_AP: 0.7380 Test_AUC: 0.6883, Test_AP: 0.7166\n",
      "Epoch: 022, Loss: 0.2953 Train_AUC: 0.8091, Train_AP: 0.7927 Val_AUC: 0.7139, Val_AP: 0.7401 Test_AUC: 0.6905, Test_AP: 0.7181\n",
      "Epoch: 023, Loss: 0.2914 Train_AUC: 0.8211, Train_AP: 0.8038 Val_AUC: 0.7254, Val_AP: 0.7497 Test_AUC: 0.6988, Test_AP: 0.7248\n",
      "Epoch: 024, Loss: 0.2890 Train_AUC: 0.8123, Train_AP: 0.7975 Val_AUC: 0.7164, Val_AP: 0.7434 Test_AUC: 0.6926, Test_AP: 0.7199\n",
      "Epoch: 025, Loss: 0.2885 Train_AUC: 0.8122, Train_AP: 0.7990 Val_AUC: 0.7160, Val_AP: 0.7434 Test_AUC: 0.6929, Test_AP: 0.7210\n",
      "Epoch: 026, Loss: 0.2851 Train_AUC: 0.8163, Train_AP: 0.8029 Val_AUC: 0.7218, Val_AP: 0.7474 Test_AUC: 0.6953, Test_AP: 0.7230\n",
      "Epoch: 027, Loss: 0.2816 Train_AUC: 0.8147, Train_AP: 0.8017 Val_AUC: 0.7221, Val_AP: 0.7473 Test_AUC: 0.6936, Test_AP: 0.7213\n",
      "Epoch: 028, Loss: 0.2808 Train_AUC: 0.8168, Train_AP: 0.8043 Val_AUC: 0.7240, Val_AP: 0.7481 Test_AUC: 0.6956, Test_AP: 0.7228\n",
      "Epoch: 029, Loss: 0.2805 Train_AUC: 0.8183, Train_AP: 0.8062 Val_AUC: 0.7244, Val_AP: 0.7474 Test_AUC: 0.6965, Test_AP: 0.7236\n",
      "Epoch: 030, Loss: 0.2800 Train_AUC: 0.8158, Train_AP: 0.8048 Val_AUC: 0.7218, Val_AP: 0.7447 Test_AUC: 0.6949, Test_AP: 0.7228\n",
      "Epoch: 031, Loss: 0.2754 Train_AUC: 0.8162, Train_AP: 0.8059 Val_AUC: 0.7225, Val_AP: 0.7448 Test_AUC: 0.6959, Test_AP: 0.7230\n",
      "Epoch: 032, Loss: 0.2739 Train_AUC: 0.8176, Train_AP: 0.8079 Val_AUC: 0.7245, Val_AP: 0.7463 Test_AUC: 0.6980, Test_AP: 0.7245\n",
      "Epoch: 033, Loss: 0.2709 Train_AUC: 0.8160, Train_AP: 0.8077 Val_AUC: 0.7224, Val_AP: 0.7453 Test_AUC: 0.6977, Test_AP: 0.7242\n",
      "Epoch: 034, Loss: 0.2706 Train_AUC: 0.8150, Train_AP: 0.8078 Val_AUC: 0.7208, Val_AP: 0.7438 Test_AUC: 0.6976, Test_AP: 0.7237\n",
      "Epoch: 035, Loss: 0.2665 Train_AUC: 0.8162, Train_AP: 0.8099 Val_AUC: 0.7213, Val_AP: 0.7435 Test_AUC: 0.6981, Test_AP: 0.7241\n",
      "Epoch: 036, Loss: 0.2682 Train_AUC: 0.8191, Train_AP: 0.8139 Val_AUC: 0.7226, Val_AP: 0.7442 Test_AUC: 0.6994, Test_AP: 0.7250\n",
      "Epoch: 037, Loss: 0.2668 Train_AUC: 0.8232, Train_AP: 0.8184 Val_AUC: 0.7254, Val_AP: 0.7459 Test_AUC: 0.7019, Test_AP: 0.7265\n",
      "Epoch: 038, Loss: 0.2627 Train_AUC: 0.8270, Train_AP: 0.8225 Val_AUC: 0.7277, Val_AP: 0.7475 Test_AUC: 0.7043, Test_AP: 0.7287\n",
      "Epoch: 039, Loss: 0.2617 Train_AUC: 0.8291, Train_AP: 0.8257 Val_AUC: 0.7296, Val_AP: 0.7502 Test_AUC: 0.7069, Test_AP: 0.7304\n",
      "Epoch: 040, Loss: 0.2597 Train_AUC: 0.8314, Train_AP: 0.8296 Val_AUC: 0.7331, Val_AP: 0.7529 Test_AUC: 0.7110, Test_AP: 0.7331\n",
      "Epoch: 041, Loss: 0.2598 Train_AUC: 0.8308, Train_AP: 0.8304 Val_AUC: 0.7327, Val_AP: 0.7525 Test_AUC: 0.7125, Test_AP: 0.7334\n",
      "Epoch: 042, Loss: 0.2577 Train_AUC: 0.8314, Train_AP: 0.8320 Val_AUC: 0.7322, Val_AP: 0.7517 Test_AUC: 0.7140, Test_AP: 0.7343\n",
      "Epoch: 043, Loss: 0.2561 Train_AUC: 0.8365, Train_AP: 0.8376 Val_AUC: 0.7337, Val_AP: 0.7522 Test_AUC: 0.7175, Test_AP: 0.7374\n",
      "Epoch: 044, Loss: 0.2559 Train_AUC: 0.8435, Train_AP: 0.8455 Val_AUC: 0.7365, Val_AP: 0.7539 Test_AUC: 0.7219, Test_AP: 0.7416\n",
      "Epoch: 045, Loss: 0.2549 Train_AUC: 0.8462, Train_AP: 0.8485 Val_AUC: 0.7369, Val_AP: 0.7542 Test_AUC: 0.7238, Test_AP: 0.7429\n",
      "Epoch: 046, Loss: 0.2547 Train_AUC: 0.8461, Train_AP: 0.8482 Val_AUC: 0.7366, Val_AP: 0.7537 Test_AUC: 0.7247, Test_AP: 0.7432\n",
      "Epoch: 047, Loss: 0.2500 Train_AUC: 0.8480, Train_AP: 0.8501 Val_AUC: 0.7376, Val_AP: 0.7547 Test_AUC: 0.7279, Test_AP: 0.7451\n",
      "Epoch: 048, Loss: 0.2513 Train_AUC: 0.8533, Train_AP: 0.8550 Val_AUC: 0.7408, Val_AP: 0.7565 Test_AUC: 0.7337, Test_AP: 0.7486\n",
      "Epoch: 049, Loss: 0.2510 Train_AUC: 0.8634, Train_AP: 0.8642 Val_AUC: 0.7472, Val_AP: 0.7610 Test_AUC: 0.7429, Test_AP: 0.7546\n",
      "Epoch: 050, Loss: 0.2485 Train_AUC: 0.8757, Train_AP: 0.8757 Val_AUC: 0.7542, Val_AP: 0.7667 Test_AUC: 0.7528, Test_AP: 0.7622\n",
      "Epoch: 051, Loss: 0.2480 Train_AUC: 0.8861, Train_AP: 0.8857 Val_AUC: 0.7599, Val_AP: 0.7724 Test_AUC: 0.7607, Test_AP: 0.7693\n",
      "Epoch: 052, Loss: 0.2493 Train_AUC: 0.8909, Train_AP: 0.8900 Val_AUC: 0.7604, Val_AP: 0.7724 Test_AUC: 0.7631, Test_AP: 0.7713\n",
      "Epoch: 053, Loss: 0.2469 Train_AUC: 0.8858, Train_AP: 0.8858 Val_AUC: 0.7545, Val_AP: 0.7672 Test_AUC: 0.7582, Test_AP: 0.7671\n",
      "Epoch: 054, Loss: 0.2455 Train_AUC: 0.8908, Train_AP: 0.8903 Val_AUC: 0.7569, Val_AP: 0.7695 Test_AUC: 0.7625, Test_AP: 0.7697\n",
      "Epoch: 055, Loss: 0.2439 Train_AUC: 0.9040, Train_AP: 0.9024 Val_AUC: 0.7649, Val_AP: 0.7766 Test_AUC: 0.7735, Test_AP: 0.7780\n",
      "Epoch: 056, Loss: 0.2428 Train_AUC: 0.9203, Train_AP: 0.9180 Val_AUC: 0.7754, Val_AP: 0.7864 Test_AUC: 0.7866, Test_AP: 0.7883\n",
      "Epoch: 057, Loss: 0.2415 Train_AUC: 0.9320, Train_AP: 0.9299 Val_AUC: 0.7817, Val_AP: 0.7931 Test_AUC: 0.7972, Test_AP: 0.7971\n",
      "Epoch: 058, Loss: 0.2410 Train_AUC: 0.9341, Train_AP: 0.9324 Val_AUC: 0.7823, Val_AP: 0.7940 Test_AUC: 0.7996, Test_AP: 0.7981\n",
      "Epoch: 059, Loss: 0.2402 Train_AUC: 0.9344, Train_AP: 0.9319 Val_AUC: 0.7803, Val_AP: 0.7929 Test_AUC: 0.7990, Test_AP: 0.7969\n",
      "Epoch: 060, Loss: 0.2398 Train_AUC: 0.9425, Train_AP: 0.9380 Val_AUC: 0.7841, Val_AP: 0.7959 Test_AUC: 0.8041, Test_AP: 0.7999\n",
      "Epoch: 061, Loss: 0.2366 Train_AUC: 0.9509, Train_AP: 0.9450 Val_AUC: 0.7883, Val_AP: 0.7997 Test_AUC: 0.8102, Test_AP: 0.8039\n",
      "Epoch: 062, Loss: 0.2376 Train_AUC: 0.9570, Train_AP: 0.9512 Val_AUC: 0.7897, Val_AP: 0.8027 Test_AUC: 0.8154, Test_AP: 0.8076\n",
      "Epoch: 063, Loss: 0.2371 Train_AUC: 0.9626, Train_AP: 0.9580 Val_AUC: 0.7934, Val_AP: 0.8075 Test_AUC: 0.8216, Test_AP: 0.8130\n",
      "Epoch: 064, Loss: 0.2352 Train_AUC: 0.9719, Train_AP: 0.9683 Val_AUC: 0.8020, Val_AP: 0.8160 Test_AUC: 0.8322, Test_AP: 0.8226\n",
      "Epoch: 065, Loss: 0.2355 Train_AUC: 0.9772, Train_AP: 0.9740 Val_AUC: 0.8070, Val_AP: 0.8204 Test_AUC: 0.8383, Test_AP: 0.8275\n",
      "Epoch: 066, Loss: 0.2336 Train_AUC: 0.9810, Train_AP: 0.9779 Val_AUC: 0.8117, Val_AP: 0.8243 Test_AUC: 0.8424, Test_AP: 0.8306\n",
      "Epoch: 067, Loss: 0.2332 Train_AUC: 0.9843, Train_AP: 0.9813 Val_AUC: 0.8161, Val_AP: 0.8277 Test_AUC: 0.8462, Test_AP: 0.8331\n",
      "Epoch: 068, Loss: 0.2309 Train_AUC: 0.9872, Train_AP: 0.9846 Val_AUC: 0.8202, Val_AP: 0.8313 Test_AUC: 0.8493, Test_AP: 0.8360\n",
      "Epoch: 069, Loss: 0.2319 Train_AUC: 0.9891, Train_AP: 0.9869 Val_AUC: 0.8223, Val_AP: 0.8327 Test_AUC: 0.8511, Test_AP: 0.8385\n",
      "Epoch: 070, Loss: 0.2306 Train_AUC: 0.9904, Train_AP: 0.9887 Val_AUC: 0.8237, Val_AP: 0.8352 Test_AUC: 0.8516, Test_AP: 0.8399\n",
      "Epoch: 071, Loss: 0.2307 Train_AUC: 0.9920, Train_AP: 0.9909 Val_AUC: 0.8254, Val_AP: 0.8385 Test_AUC: 0.8543, Test_AP: 0.8449\n",
      "Epoch: 072, Loss: 0.2297 Train_AUC: 0.9929, Train_AP: 0.9921 Val_AUC: 0.8271, Val_AP: 0.8418 Test_AUC: 0.8551, Test_AP: 0.8466\n",
      "Epoch: 073, Loss: 0.2299 Train_AUC: 0.9943, Train_AP: 0.9937 Val_AUC: 0.8305, Val_AP: 0.8445 Test_AUC: 0.8570, Test_AP: 0.8479\n",
      "Epoch: 074, Loss: 0.2280 Train_AUC: 0.9956, Train_AP: 0.9951 Val_AUC: 0.8345, Val_AP: 0.8474 Test_AUC: 0.8591, Test_AP: 0.8498\n",
      "Epoch: 075, Loss: 0.2267 Train_AUC: 0.9965, Train_AP: 0.9961 Val_AUC: 0.8376, Val_AP: 0.8518 Test_AUC: 0.8610, Test_AP: 0.8516\n",
      "Epoch: 076, Loss: 0.2268 Train_AUC: 0.9974, Train_AP: 0.9971 Val_AUC: 0.8425, Val_AP: 0.8583 Test_AUC: 0.8636, Test_AP: 0.8550\n",
      "Epoch: 077, Loss: 0.2271 Train_AUC: 0.9981, Train_AP: 0.9978 Val_AUC: 0.8466, Val_AP: 0.8637 Test_AUC: 0.8660, Test_AP: 0.8608\n",
      "Epoch: 078, Loss: 0.2253 Train_AUC: 0.9983, Train_AP: 0.9981 Val_AUC: 0.8470, Val_AP: 0.8647 Test_AUC: 0.8659, Test_AP: 0.8624\n",
      "Epoch: 079, Loss: 0.2246 Train_AUC: 0.9984, Train_AP: 0.9983 Val_AUC: 0.8469, Val_AP: 0.8654 Test_AUC: 0.8650, Test_AP: 0.8619\n",
      "Epoch: 080, Loss: 0.2224 Train_AUC: 0.9985, Train_AP: 0.9985 Val_AUC: 0.8482, Val_AP: 0.8677 Test_AUC: 0.8648, Test_AP: 0.8626\n",
      "Epoch: 081, Loss: 0.2242 Train_AUC: 0.9988, Train_AP: 0.9987 Val_AUC: 0.8500, Val_AP: 0.8686 Test_AUC: 0.8652, Test_AP: 0.8623\n",
      "Epoch: 082, Loss: 0.2232 Train_AUC: 0.9990, Train_AP: 0.9990 Val_AUC: 0.8516, Val_AP: 0.8698 Test_AUC: 0.8659, Test_AP: 0.8622\n",
      "Epoch: 083, Loss: 0.2219 Train_AUC: 0.9991, Train_AP: 0.9990 Val_AUC: 0.8512, Val_AP: 0.8678 Test_AUC: 0.8657, Test_AP: 0.8622\n",
      "Epoch: 084, Loss: 0.2216 Train_AUC: 0.9992, Train_AP: 0.9991 Val_AUC: 0.8513, Val_AP: 0.8672 Test_AUC: 0.8664, Test_AP: 0.8624\n",
      "Epoch: 085, Loss: 0.2216 Train_AUC: 0.9992, Train_AP: 0.9991 Val_AUC: 0.8505, Val_AP: 0.8658 Test_AUC: 0.8668, Test_AP: 0.8625\n",
      "Epoch: 086, Loss: 0.2215 Train_AUC: 0.9991, Train_AP: 0.9990 Val_AUC: 0.8485, Val_AP: 0.8641 Test_AUC: 0.8659, Test_AP: 0.8611\n",
      "Epoch: 087, Loss: 0.2219 Train_AUC: 0.9992, Train_AP: 0.9992 Val_AUC: 0.8509, Val_AP: 0.8690 Test_AUC: 0.8679, Test_AP: 0.8657\n",
      "Epoch: 088, Loss: 0.2192 Train_AUC: 0.9994, Train_AP: 0.9994 Val_AUC: 0.8528, Val_AP: 0.8722 Test_AUC: 0.8703, Test_AP: 0.8710\n",
      "Epoch: 089, Loss: 0.2195 Train_AUC: 0.9994, Train_AP: 0.9994 Val_AUC: 0.8531, Val_AP: 0.8724 Test_AUC: 0.8703, Test_AP: 0.8711\n",
      "Epoch: 090, Loss: 0.2194 Train_AUC: 0.9994, Train_AP: 0.9994 Val_AUC: 0.8518, Val_AP: 0.8690 Test_AUC: 0.8690, Test_AP: 0.8688\n",
      "Epoch: 091, Loss: 0.2181 Train_AUC: 0.9993, Train_AP: 0.9993 Val_AUC: 0.8510, Val_AP: 0.8656 Test_AUC: 0.8678, Test_AP: 0.8664\n",
      "Epoch: 092, Loss: 0.2172 Train_AUC: 0.9994, Train_AP: 0.9994 Val_AUC: 0.8497, Val_AP: 0.8634 Test_AUC: 0.8666, Test_AP: 0.8648\n",
      "Epoch: 093, Loss: 0.2168 Train_AUC: 0.9995, Train_AP: 0.9995 Val_AUC: 0.8512, Val_AP: 0.8664 Test_AUC: 0.8677, Test_AP: 0.8687\n",
      "Epoch: 094, Loss: 0.2178 Train_AUC: 0.9996, Train_AP: 0.9996 Val_AUC: 0.8526, Val_AP: 0.8705 Test_AUC: 0.8692, Test_AP: 0.8729\n",
      "Epoch: 095, Loss: 0.2153 Train_AUC: 0.9996, Train_AP: 0.9996 Val_AUC: 0.8539, Val_AP: 0.8733 Test_AUC: 0.8698, Test_AP: 0.8750\n",
      "Epoch: 096, Loss: 0.2177 Train_AUC: 0.9996, Train_AP: 0.9996 Val_AUC: 0.8552, Val_AP: 0.8747 Test_AUC: 0.8694, Test_AP: 0.8746\n",
      "Epoch: 097, Loss: 0.2155 Train_AUC: 0.9997, Train_AP: 0.9997 Val_AUC: 0.8551, Val_AP: 0.8737 Test_AUC: 0.8689, Test_AP: 0.8733\n",
      "Epoch: 098, Loss: 0.2155 Train_AUC: 0.9997, Train_AP: 0.9997 Val_AUC: 0.8557, Val_AP: 0.8740 Test_AUC: 0.8687, Test_AP: 0.8739\n",
      "Epoch: 099, Loss: 0.2159 Train_AUC: 0.9997, Train_AP: 0.9996 Val_AUC: 0.8546, Val_AP: 0.8724 Test_AUC: 0.8672, Test_AP: 0.8717\n",
      "Epoch: 100, Loss: 0.2131 Train_AUC: 0.9996, Train_AP: 0.9996 Val_AUC: 0.8542, Val_AP: 0.8729 Test_AUC: 0.8665, Test_AP: 0.8702\n",
      "Epoch: 101, Loss: 0.2143 Train_AUC: 0.9996, Train_AP: 0.9996 Val_AUC: 0.8552, Val_AP: 0.8746 Test_AUC: 0.8670, Test_AP: 0.8708\n",
      "Epoch: 102, Loss: 0.2139 Train_AUC: 0.9997, Train_AP: 0.9997 Val_AUC: 0.8573, Val_AP: 0.8777 Test_AUC: 0.8684, Test_AP: 0.8741\n",
      "Epoch: 103, Loss: 0.2136 Train_AUC: 0.9998, Train_AP: 0.9998 Val_AUC: 0.8571, Val_AP: 0.8777 Test_AUC: 0.8690, Test_AP: 0.8750\n",
      "Epoch: 104, Loss: 0.2134 Train_AUC: 0.9998, Train_AP: 0.9998 Val_AUC: 0.8563, Val_AP: 0.8757 Test_AUC: 0.8686, Test_AP: 0.8742\n",
      "Epoch: 105, Loss: 0.2133 Train_AUC: 0.9998, Train_AP: 0.9998 Val_AUC: 0.8558, Val_AP: 0.8764 Test_AUC: 0.8666, Test_AP: 0.8710\n",
      "Epoch: 106, Loss: 0.2125 Train_AUC: 0.9997, Train_AP: 0.9997 Val_AUC: 0.8552, Val_AP: 0.8750 Test_AUC: 0.8645, Test_AP: 0.8684\n",
      "Epoch: 107, Loss: 0.2121 Train_AUC: 0.9997, Train_AP: 0.9997 Val_AUC: 0.8565, Val_AP: 0.8768 Test_AUC: 0.8646, Test_AP: 0.8696\n",
      "Epoch: 108, Loss: 0.2120 Train_AUC: 0.9998, Train_AP: 0.9998 Val_AUC: 0.8556, Val_AP: 0.8755 Test_AUC: 0.8648, Test_AP: 0.8724\n",
      "Epoch: 109, Loss: 0.2122 Train_AUC: 0.9998, Train_AP: 0.9998 Val_AUC: 0.8553, Val_AP: 0.8744 Test_AUC: 0.8638, Test_AP: 0.8711\n",
      "Epoch: 110, Loss: 0.2109 Train_AUC: 0.9998, Train_AP: 0.9998 Val_AUC: 0.8530, Val_AP: 0.8720 Test_AUC: 0.8627, Test_AP: 0.8686\n",
      "Epoch: 111, Loss: 0.2108 Train_AUC: 0.9998, Train_AP: 0.9998 Val_AUC: 0.8542, Val_AP: 0.8750 Test_AUC: 0.8642, Test_AP: 0.8711\n",
      "Epoch: 112, Loss: 0.2112 Train_AUC: 0.9998, Train_AP: 0.9998 Val_AUC: 0.8546, Val_AP: 0.8766 Test_AUC: 0.8646, Test_AP: 0.8727\n",
      "Epoch: 113, Loss: 0.2095 Train_AUC: 0.9998, Train_AP: 0.9998 Val_AUC: 0.8554, Val_AP: 0.8779 Test_AUC: 0.8637, Test_AP: 0.8724\n",
      "Epoch: 114, Loss: 0.2085 Train_AUC: 0.9998, Train_AP: 0.9998 Val_AUC: 0.8548, Val_AP: 0.8775 Test_AUC: 0.8628, Test_AP: 0.8701\n",
      "Epoch: 115, Loss: 0.2082 Train_AUC: 0.9997, Train_AP: 0.9997 Val_AUC: 0.8521, Val_AP: 0.8745 Test_AUC: 0.8616, Test_AP: 0.8683\n",
      "Epoch: 116, Loss: 0.2082 Train_AUC: 0.9998, Train_AP: 0.9998 Val_AUC: 0.8505, Val_AP: 0.8723 Test_AUC: 0.8617, Test_AP: 0.8702\n",
      "Epoch: 117, Loss: 0.2082 Train_AUC: 0.9998, Train_AP: 0.9998 Val_AUC: 0.8516, Val_AP: 0.8726 Test_AUC: 0.8621, Test_AP: 0.8729\n",
      "Epoch: 118, Loss: 0.2082 Train_AUC: 0.9998, Train_AP: 0.9998 Val_AUC: 0.8513, Val_AP: 0.8711 Test_AUC: 0.8618, Test_AP: 0.8719\n",
      "Epoch: 119, Loss: 0.2088 Train_AUC: 0.9998, Train_AP: 0.9998 Val_AUC: 0.8504, Val_AP: 0.8686 Test_AUC: 0.8601, Test_AP: 0.8702\n",
      "Epoch: 120, Loss: 0.2084 Train_AUC: 0.9998, Train_AP: 0.9998 Val_AUC: 0.8499, Val_AP: 0.8687 Test_AUC: 0.8597, Test_AP: 0.8691\n",
      "Epoch: 121, Loss: 0.2071 Train_AUC: 0.9998, Train_AP: 0.9998 Val_AUC: 0.8492, Val_AP: 0.8681 Test_AUC: 0.8599, Test_AP: 0.8694\n",
      "Epoch: 122, Loss: 0.2066 Train_AUC: 0.9998, Train_AP: 0.9998 Val_AUC: 0.8495, Val_AP: 0.8672 Test_AUC: 0.8610, Test_AP: 0.8716\n",
      "Epoch: 123, Loss: 0.2066 Train_AUC: 0.9999, Train_AP: 0.9999 Val_AUC: 0.8478, Val_AP: 0.8637 Test_AUC: 0.8607, Test_AP: 0.8706\n",
      "Epoch: 124, Loss: 0.2049 Train_AUC: 0.9999, Train_AP: 0.9999 Val_AUC: 0.8473, Val_AP: 0.8617 Test_AUC: 0.8599, Test_AP: 0.8689\n",
      "Epoch: 125, Loss: 0.2058 Train_AUC: 0.9999, Train_AP: 0.9999 Val_AUC: 0.8467, Val_AP: 0.8600 Test_AUC: 0.8596, Test_AP: 0.8685\n",
      "Epoch: 126, Loss: 0.2049 Train_AUC: 0.9999, Train_AP: 0.9999 Val_AUC: 0.8471, Val_AP: 0.8586 Test_AUC: 0.8600, Test_AP: 0.8698\n",
      "Epoch: 127, Loss: 0.2055 Train_AUC: 0.9999, Train_AP: 0.9999 Val_AUC: 0.8485, Val_AP: 0.8617 Test_AUC: 0.8590, Test_AP: 0.8690\n",
      "Epoch: 128, Loss: 0.2038 Train_AUC: 0.9999, Train_AP: 0.9999 Val_AUC: 0.8508, Val_AP: 0.8647 Test_AUC: 0.8591, Test_AP: 0.8681\n",
      "Epoch: 129, Loss: 0.2041 Train_AUC: 0.9999, Train_AP: 0.9999 Val_AUC: 0.8515, Val_AP: 0.8668 Test_AUC: 0.8588, Test_AP: 0.8675\n",
      "Epoch: 130, Loss: 0.2050 Train_AUC: 0.9999, Train_AP: 0.9999 Val_AUC: 0.8513, Val_AP: 0.8668 Test_AUC: 0.8596, Test_AP: 0.8678\n",
      "Epoch: 131, Loss: 0.2061 Train_AUC: 0.9999, Train_AP: 0.9999 Val_AUC: 0.8504, Val_AP: 0.8670 Test_AUC: 0.8606, Test_AP: 0.8703\n",
      "Epoch: 132, Loss: 0.2047 Train_AUC: 0.9999, Train_AP: 0.9999 Val_AUC: 0.8504, Val_AP: 0.8677 Test_AUC: 0.8608, Test_AP: 0.8713\n",
      "Epoch: 133, Loss: 0.2046 Train_AUC: 0.9999, Train_AP: 0.9999 Val_AUC: 0.8510, Val_AP: 0.8676 Test_AUC: 0.8604, Test_AP: 0.8700\n",
      "Epoch: 134, Loss: 0.2033 Train_AUC: 0.9999, Train_AP: 0.9999 Val_AUC: 0.8497, Val_AP: 0.8652 Test_AUC: 0.8596, Test_AP: 0.8689\n",
      "Epoch: 135, Loss: 0.2036 Train_AUC: 0.9999, Train_AP: 0.9999 Val_AUC: 0.8502, Val_AP: 0.8658 Test_AUC: 0.8594, Test_AP: 0.8678\n",
      "Epoch: 136, Loss: 0.2030 Train_AUC: 0.9999, Train_AP: 0.9999 Val_AUC: 0.8475, Val_AP: 0.8624 Test_AUC: 0.8582, Test_AP: 0.8676\n",
      "Epoch: 137, Loss: 0.2043 Train_AUC: 0.9999, Train_AP: 0.9999 Val_AUC: 0.8468, Val_AP: 0.8611 Test_AUC: 0.8585, Test_AP: 0.8682\n",
      "Epoch: 138, Loss: 0.2045 Train_AUC: 0.9999, Train_AP: 0.9999 Val_AUC: 0.8477, Val_AP: 0.8634 Test_AUC: 0.8588, Test_AP: 0.8677\n",
      "Epoch: 139, Loss: 0.2032 Train_AUC: 0.9999, Train_AP: 0.9999 Val_AUC: 0.8481, Val_AP: 0.8665 Test_AUC: 0.8594, Test_AP: 0.8691\n",
      "Epoch: 140, Loss: 0.2026 Train_AUC: 0.9999, Train_AP: 0.9999 Val_AUC: 0.8481, Val_AP: 0.8674 Test_AUC: 0.8591, Test_AP: 0.8702\n",
      "Epoch: 141, Loss: 0.2031 Train_AUC: 0.9999, Train_AP: 0.9999 Val_AUC: 0.8485, Val_AP: 0.8676 Test_AUC: 0.8585, Test_AP: 0.8690\n",
      "Epoch: 142, Loss: 0.2015 Train_AUC: 0.9999, Train_AP: 0.9999 Val_AUC: 0.8491, Val_AP: 0.8675 Test_AUC: 0.8579, Test_AP: 0.8668\n",
      "Epoch: 143, Loss: 0.2017 Train_AUC: 0.9999, Train_AP: 0.9999 Val_AUC: 0.8482, Val_AP: 0.8660 Test_AUC: 0.8577, Test_AP: 0.8663\n",
      "Epoch: 144, Loss: 0.2014 Train_AUC: 0.9999, Train_AP: 0.9999 Val_AUC: 0.8476, Val_AP: 0.8649 Test_AUC: 0.8576, Test_AP: 0.8668\n",
      "Epoch: 145, Loss: 0.2007 Train_AUC: 0.9999, Train_AP: 0.9999 Val_AUC: 0.8472, Val_AP: 0.8642 Test_AUC: 0.8558, Test_AP: 0.8646\n",
      "Epoch: 146, Loss: 0.2007 Train_AUC: 0.9999, Train_AP: 0.9999 Val_AUC: 0.8462, Val_AP: 0.8638 Test_AUC: 0.8543, Test_AP: 0.8631\n",
      "Epoch: 147, Loss: 0.2005 Train_AUC: 0.9999, Train_AP: 0.9999 Val_AUC: 0.8464, Val_AP: 0.8644 Test_AUC: 0.8537, Test_AP: 0.8623\n",
      "Epoch: 148, Loss: 0.2000 Train_AUC: 0.9999, Train_AP: 0.9999 Val_AUC: 0.8465, Val_AP: 0.8653 Test_AUC: 0.8542, Test_AP: 0.8647\n",
      "Epoch: 149, Loss: 0.1994 Train_AUC: 0.9999, Train_AP: 0.9999 Val_AUC: 0.8457, Val_AP: 0.8656 Test_AUC: 0.8540, Test_AP: 0.8664\n",
      "Epoch: 150, Loss: 0.1997 Train_AUC: 0.9999, Train_AP: 0.9999 Val_AUC: 0.8458, Val_AP: 0.8659 Test_AUC: 0.8547, Test_AP: 0.8684\n",
      "Epoch: 151, Loss: 0.2003 Train_AUC: 0.9999, Train_AP: 0.9999 Val_AUC: 0.8450, Val_AP: 0.8639 Test_AUC: 0.8538, Test_AP: 0.8645\n",
      "Epoch: 152, Loss: 0.1995 Train_AUC: 0.9999, Train_AP: 0.9999 Val_AUC: 0.8446, Val_AP: 0.8621 Test_AUC: 0.8532, Test_AP: 0.8632\n",
      "Epoch: 153, Loss: 0.1996 Train_AUC: 0.9999, Train_AP: 0.9999 Val_AUC: 0.8435, Val_AP: 0.8604 Test_AUC: 0.8527, Test_AP: 0.8637\n",
      "Epoch: 154, Loss: 0.1993 Train_AUC: 0.9999, Train_AP: 0.9999 Val_AUC: 0.8437, Val_AP: 0.8599 Test_AUC: 0.8528, Test_AP: 0.8640\n",
      "Epoch: 155, Loss: 0.1991 Train_AUC: 0.9999, Train_AP: 0.9999 Val_AUC: 0.8433, Val_AP: 0.8607 Test_AUC: 0.8534, Test_AP: 0.8638\n",
      "Epoch: 156, Loss: 0.1976 Train_AUC: 0.9999, Train_AP: 0.9999 Val_AUC: 0.8430, Val_AP: 0.8615 Test_AUC: 0.8544, Test_AP: 0.8637\n",
      "Epoch: 157, Loss: 0.1971 Train_AUC: 0.9999, Train_AP: 0.9999 Val_AUC: 0.8432, Val_AP: 0.8632 Test_AUC: 0.8547, Test_AP: 0.8643\n",
      "Epoch: 158, Loss: 0.1974 Train_AUC: 0.9999, Train_AP: 0.9999 Val_AUC: 0.8438, Val_AP: 0.8651 Test_AUC: 0.8543, Test_AP: 0.8658\n",
      "Epoch: 159, Loss: 0.1977 Train_AUC: 0.9999, Train_AP: 0.9999 Val_AUC: 0.8450, Val_AP: 0.8672 Test_AUC: 0.8539, Test_AP: 0.8661\n",
      "Epoch: 160, Loss: 0.1979 Train_AUC: 1.0000, Train_AP: 1.0000 Val_AUC: 0.8465, Val_AP: 0.8683 Test_AUC: 0.8538, Test_AP: 0.8654\n",
      "Epoch: 161, Loss: 0.1975 Train_AUC: 0.9999, Train_AP: 0.9999 Val_AUC: 0.8457, Val_AP: 0.8673 Test_AUC: 0.8518, Test_AP: 0.8602\n",
      "Epoch: 162, Loss: 0.1975 Train_AUC: 0.9999, Train_AP: 0.9999 Val_AUC: 0.8454, Val_AP: 0.8664 Test_AUC: 0.8523, Test_AP: 0.8602\n",
      "Epoch: 163, Loss: 0.1966 Train_AUC: 1.0000, Train_AP: 1.0000 Val_AUC: 0.8448, Val_AP: 0.8644 Test_AUC: 0.8535, Test_AP: 0.8657\n",
      "Epoch: 164, Loss: 0.1966 Train_AUC: 1.0000, Train_AP: 1.0000 Val_AUC: 0.8457, Val_AP: 0.8664 Test_AUC: 0.8537, Test_AP: 0.8656\n",
      "Epoch: 165, Loss: 0.1960 Train_AUC: 1.0000, Train_AP: 1.0000 Val_AUC: 0.8474, Val_AP: 0.8681 Test_AUC: 0.8535, Test_AP: 0.8655\n",
      "Epoch: 166, Loss: 0.1965 Train_AUC: 0.9999, Train_AP: 0.9999 Val_AUC: 0.8475, Val_AP: 0.8680 Test_AUC: 0.8536, Test_AP: 0.8648\n",
      "Epoch: 167, Loss: 0.1953 Train_AUC: 0.9999, Train_AP: 0.9999 Val_AUC: 0.8472, Val_AP: 0.8675 Test_AUC: 0.8533, Test_AP: 0.8644\n",
      "Epoch: 168, Loss: 0.1964 Train_AUC: 0.9999, Train_AP: 0.9999 Val_AUC: 0.8456, Val_AP: 0.8660 Test_AUC: 0.8520, Test_AP: 0.8624\n",
      "Epoch: 169, Loss: 0.1959 Train_AUC: 0.9999, Train_AP: 0.9999 Val_AUC: 0.8436, Val_AP: 0.8637 Test_AUC: 0.8509, Test_AP: 0.8594\n",
      "Epoch: 170, Loss: 0.1957 Train_AUC: 0.9999, Train_AP: 0.9999 Val_AUC: 0.8430, Val_AP: 0.8634 Test_AUC: 0.8497, Test_AP: 0.8570\n",
      "Epoch: 171, Loss: 0.1952 Train_AUC: 1.0000, Train_AP: 1.0000 Val_AUC: 0.8434, Val_AP: 0.8631 Test_AUC: 0.8497, Test_AP: 0.8574\n",
      "Epoch: 172, Loss: 0.1942 Train_AUC: 1.0000, Train_AP: 1.0000 Val_AUC: 0.8443, Val_AP: 0.8647 Test_AUC: 0.8500, Test_AP: 0.8602\n",
      "Epoch: 173, Loss: 0.1949 Train_AUC: 0.9999, Train_AP: 0.9999 Val_AUC: 0.8445, Val_AP: 0.8644 Test_AUC: 0.8505, Test_AP: 0.8634\n",
      "Epoch: 174, Loss: 0.1936 Train_AUC: 1.0000, Train_AP: 1.0000 Val_AUC: 0.8445, Val_AP: 0.8638 Test_AUC: 0.8514, Test_AP: 0.8641\n",
      "Epoch: 175, Loss: 0.1935 Train_AUC: 1.0000, Train_AP: 1.0000 Val_AUC: 0.8435, Val_AP: 0.8615 Test_AUC: 0.8513, Test_AP: 0.8612\n",
      "Epoch: 176, Loss: 0.1950 Train_AUC: 1.0000, Train_AP: 1.0000 Val_AUC: 0.8420, Val_AP: 0.8608 Test_AUC: 0.8509, Test_AP: 0.8607\n",
      "Epoch: 177, Loss: 0.1939 Train_AUC: 1.0000, Train_AP: 1.0000 Val_AUC: 0.8412, Val_AP: 0.8607 Test_AUC: 0.8507, Test_AP: 0.8619\n",
      "Epoch: 178, Loss: 0.1951 Train_AUC: 1.0000, Train_AP: 1.0000 Val_AUC: 0.8403, Val_AP: 0.8580 Test_AUC: 0.8508, Test_AP: 0.8601\n",
      "Epoch: 179, Loss: 0.1942 Train_AUC: 1.0000, Train_AP: 1.0000 Val_AUC: 0.8383, Val_AP: 0.8535 Test_AUC: 0.8493, Test_AP: 0.8549\n",
      "Epoch: 180, Loss: 0.1939 Train_AUC: 1.0000, Train_AP: 1.0000 Val_AUC: 0.8401, Val_AP: 0.8568 Test_AUC: 0.8502, Test_AP: 0.8565\n",
      "Epoch: 181, Loss: 0.1941 Train_AUC: 1.0000, Train_AP: 1.0000 Val_AUC: 0.8418, Val_AP: 0.8601 Test_AUC: 0.8501, Test_AP: 0.8596\n",
      "Epoch: 182, Loss: 0.1936 Train_AUC: 1.0000, Train_AP: 1.0000 Val_AUC: 0.8416, Val_AP: 0.8611 Test_AUC: 0.8494, Test_AP: 0.8585\n",
      "Epoch: 183, Loss: 0.1928 Train_AUC: 1.0000, Train_AP: 1.0000 Val_AUC: 0.8423, Val_AP: 0.8628 Test_AUC: 0.8495, Test_AP: 0.8582\n",
      "Epoch: 184, Loss: 0.1921 Train_AUC: 1.0000, Train_AP: 1.0000 Val_AUC: 0.8416, Val_AP: 0.8632 Test_AUC: 0.8489, Test_AP: 0.8586\n",
      "Epoch: 185, Loss: 0.1926 Train_AUC: 1.0000, Train_AP: 1.0000 Val_AUC: 0.8418, Val_AP: 0.8627 Test_AUC: 0.8503, Test_AP: 0.8621\n",
      "Epoch: 186, Loss: 0.1922 Train_AUC: 1.0000, Train_AP: 1.0000 Val_AUC: 0.8403, Val_AP: 0.8608 Test_AUC: 0.8500, Test_AP: 0.8604\n",
      "Epoch: 187, Loss: 0.1926 Train_AUC: 1.0000, Train_AP: 1.0000 Val_AUC: 0.8394, Val_AP: 0.8593 Test_AUC: 0.8492, Test_AP: 0.8588\n",
      "Epoch: 188, Loss: 0.1928 Train_AUC: 1.0000, Train_AP: 1.0000 Val_AUC: 0.8398, Val_AP: 0.8593 Test_AUC: 0.8497, Test_AP: 0.8605\n",
      "Epoch: 189, Loss: 0.1930 Train_AUC: 1.0000, Train_AP: 1.0000 Val_AUC: 0.8409, Val_AP: 0.8597 Test_AUC: 0.8513, Test_AP: 0.8645\n",
      "Epoch: 190, Loss: 0.1930 Train_AUC: 1.0000, Train_AP: 1.0000 Val_AUC: 0.8399, Val_AP: 0.8607 Test_AUC: 0.8498, Test_AP: 0.8613\n",
      "Epoch: 191, Loss: 0.1928 Train_AUC: 0.9999, Train_AP: 0.9999 Val_AUC: 0.8391, Val_AP: 0.8599 Test_AUC: 0.8489, Test_AP: 0.8579\n",
      "Epoch: 192, Loss: 0.1939 Train_AUC: 1.0000, Train_AP: 1.0000 Val_AUC: 0.8393, Val_AP: 0.8600 Test_AUC: 0.8495, Test_AP: 0.8607\n",
      "Epoch: 193, Loss: 0.1921 Train_AUC: 1.0000, Train_AP: 1.0000 Val_AUC: 0.8383, Val_AP: 0.8588 Test_AUC: 0.8489, Test_AP: 0.8612\n",
      "Epoch: 194, Loss: 0.1914 Train_AUC: 1.0000, Train_AP: 1.0000 Val_AUC: 0.8360, Val_AP: 0.8549 Test_AUC: 0.8467, Test_AP: 0.8569\n",
      "Epoch: 195, Loss: 0.1928 Train_AUC: 1.0000, Train_AP: 1.0000 Val_AUC: 0.8355, Val_AP: 0.8522 Test_AUC: 0.8454, Test_AP: 0.8544\n",
      "Epoch: 196, Loss: 0.1932 Train_AUC: 1.0000, Train_AP: 1.0000 Val_AUC: 0.8381, Val_AP: 0.8558 Test_AUC: 0.8461, Test_AP: 0.8582\n",
      "Epoch: 197, Loss: 0.1909 Train_AUC: 1.0000, Train_AP: 1.0000 Val_AUC: 0.8396, Val_AP: 0.8572 Test_AUC: 0.8459, Test_AP: 0.8569\n",
      "Epoch: 198, Loss: 0.1913 Train_AUC: 1.0000, Train_AP: 1.0000 Val_AUC: 0.8385, Val_AP: 0.8562 Test_AUC: 0.8454, Test_AP: 0.8533\n",
      "Epoch: 199, Loss: 0.1910 Train_AUC: 1.0000, Train_AP: 1.0000 Val_AUC: 0.8373, Val_AP: 0.8546 Test_AUC: 0.8455, Test_AP: 0.8535\n",
      "Epoch: 200, Loss: 0.1901 Train_AUC: 1.0000, Train_AP: 1.0000 Val_AUC: 0.8378, Val_AP: 0.8552 Test_AUC: 0.8467, Test_AP: 0.8560\n",
      "Final Test AUC: 0.8684, AP: 0.8741\n"
     ]
    }
   ],
   "source": [
    "train_data = torch.load('./data/Cora/split/train_data.pt').to(device)\n",
    "val_data = torch.load('./data/Cora/split/val_data.pt').to(device)\n",
    "test_data = torch.load('./data/Cora/split/test_data.pt').to(device)\n",
    "\n",
    "model = ScoreGNN(train_data.num_features, args['hidden_dim'], args['output_dim'], args['num_layers'], args['dropout']).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'])\n",
    "\n",
    "final_model = final_predictor = None\n",
    "best_val_auc = final_test_auc = final_test_ap = 0\n",
    "\n",
    "model.reset_parameters()\n",
    "\n",
    "for epoch in range(1, 1 + args[\"epochs\"]):\n",
    "    loss = train(model, train_data, optimizer, args['predictor'])\n",
    "    train_auc, train_ap = test(model, train_data, args['predictor'])\n",
    "    val_auc, val_ap = test(model, val_data, args['predictor'])\n",
    "    test_auc, test_ap = test(model, test_data, args['predictor'])\n",
    "    if val_auc > best_val_auc:\n",
    "        best_val_auc = val_auc\n",
    "        final_test_auc = test_auc\n",
    "        final_test_ap = test_ap\n",
    "        final_model = copy.deepcopy(model)\n",
    "        final_predictor = copy.deepcopy(args['predictor'])\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f} '\n",
    "          f'Train_AUC: {train_auc:.4f}, Train_AP: {train_ap:.4f} '\n",
    "          f'Val_AUC: {val_auc:.4f}, Val_AP: {val_ap:.4f} '\n",
    "          f'Test_AUC: {test_auc:.4f}, Test_AP: {test_ap:.4f}')\n",
    "\n",
    "print(f'Final Test AUC: {final_test_auc:.4f}, AP: {final_test_ap:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "85f543ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    'model': final_model.state_dict(),\n",
    "    'predictor': final_predictor.state_dict()\n",
    "}, './model/scoregnn.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
