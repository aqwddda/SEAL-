{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fd51917",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T21:12:58.656745Z",
     "iopub.status.busy": "2025-05-28T21:12:58.656745Z",
     "iopub.status.idle": "2025-05-28T21:13:05.507051Z",
     "shell.execute_reply": "2025-05-28T21:13:05.507051Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import copy, os\n",
    "import logging\n",
    "import numpy as np\n",
    "import out_manager as om\n",
    "from config import Config\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.utils import negative_sampling\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from model.score_gnn import scoregnn_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90015b11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T21:13:05.508799Z",
     "iopub.status.busy": "2025-05-28T21:13:05.508799Z",
     "iopub.status.idle": "2025-05-28T21:13:05.542155Z",
     "shell.execute_reply": "2025-05-28T21:13:05.542155Z"
    }
   },
   "outputs": [],
   "source": [
    "config = Config()\n",
    "ModelClass = scoregnn_dict[config.scoregnn.gnn_type]\n",
    "out_dir = om.get_out_dir(config)\n",
    "log_path = os.path.join(out_dir, \"scoregnn_log.txt\")\n",
    "om.setup_logging(log_path)\n",
    "seed = config.seed\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "device = config.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44cc6aae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T21:13:05.542155Z",
     "iopub.status.busy": "2025-05-28T21:13:05.542155Z",
     "iopub.status.idle": "2025-05-28T21:13:05.550017Z",
     "shell.execute_reply": "2025-05-28T21:13:05.550017Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, data, optimizer, predictor):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index)\n",
    "\n",
    "    # #每次手动采样负样本，效果更好\n",
    "    # neg_edge_index = negative_sampling(\n",
    "    #     edge_index = data.edge_index,\n",
    "    #     num_nodes = data.num_nodes,\n",
    "    #     num_neg_sample = data.edge_index.size(1)\n",
    "    # )\n",
    "\n",
    "    # edge_label_index = torch.cat([\n",
    "    #     data.edge_label_index, neg_edge_index\n",
    "    #     ], dim=-1)\n",
    "    # edge_label = torch.cat([\n",
    "    #     data.edge_label, data.edge_label.new_zeros(neg_edge_index.size(1))\n",
    "    #     ],dim = 0)\n",
    "\n",
    "    edge_label_index = torch.cat([\n",
    "        data.pos_edge_label_index, data.neg_edge_label_index\n",
    "    ], dim=1)\n",
    "    edge_label = torch.cat([\n",
    "        data.pos_edge_label, data.neg_edge_label\n",
    "    ], dim=0)\n",
    "    \n",
    "    score = predictor(out, edge_label_index)\n",
    "    loss = F.binary_cross_entropy_with_logits(score, edge_label)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1be434a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T21:13:05.550017Z",
     "iopub.status.busy": "2025-05-28T21:13:05.550017Z",
     "iopub.status.idle": "2025-05-28T21:13:05.557282Z",
     "shell.execute_reply": "2025-05-28T21:13:05.557282Z"
    }
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test(model, data, predictor):\n",
    "    model.eval()\n",
    "\n",
    "    edge_label_index = torch.cat([\n",
    "        data.pos_edge_label_index, data.neg_edge_label_index\n",
    "    ], dim=1)\n",
    "    edge_label = torch.cat([\n",
    "        data.pos_edge_label, data.neg_edge_label\n",
    "    ], dim=0)\n",
    "\n",
    "    out = model(data.x, data.edge_index)\n",
    "    score = predictor(out, edge_label_index).cpu().numpy()\n",
    "    auc = roc_auc_score(edge_label.cpu().numpy(), score)\n",
    "    ap = average_precision_score(edge_label.cpu().numpy(), score)\n",
    "    return auc, ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8491527b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T21:13:05.557282Z",
     "iopub.status.busy": "2025-05-28T21:13:05.557282Z",
     "iopub.status.idle": "2025-05-28T21:13:12.926352Z",
     "shell.execute_reply": "2025-05-28T21:13:12.926352Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 0.6881 Train_AUC: 0.6967, Train_AP: 0.7162 Val_AUC: 0.6828, Val_AP: 0.7125 Test_AUC: 0.6734, Test_AP: 0.7084\n",
      "Epoch: 002, Loss: 0.6005 Train_AUC: 0.7696, Train_AP: 0.7717 Val_AUC: 0.7331, Val_AP: 0.7520 Test_AUC: 0.7361, Test_AP: 0.7552\n",
      "Epoch: 003, Loss: 0.5690 Train_AUC: 0.7175, Train_AP: 0.6831 Val_AUC: 0.7007, Val_AP: 0.6685 Test_AUC: 0.6985, Test_AP: 0.6639\n",
      "Epoch: 004, Loss: 0.5247 Train_AUC: 0.5682, Train_AP: 0.5842 Val_AUC: 0.5628, Val_AP: 0.5798 Test_AUC: 0.5540, Test_AP: 0.5688\n",
      "Epoch: 005, Loss: 0.4885 Train_AUC: 0.5277, Train_AP: 0.5710 Val_AUC: 0.5247, Val_AP: 0.5685 Test_AUC: 0.5188, Test_AP: 0.5585\n",
      "Epoch: 006, Loss: 0.4600 Train_AUC: 0.6202, Train_AP: 0.6113 Val_AUC: 0.6215, Val_AP: 0.6096 Test_AUC: 0.6103, Test_AP: 0.5995\n",
      "Epoch: 007, Loss: 0.4408 Train_AUC: 0.7689, Train_AP: 0.7604 Val_AUC: 0.7650, Val_AP: 0.7631 Test_AUC: 0.7559, Test_AP: 0.7484\n",
      "Epoch: 008, Loss: 0.4314 Train_AUC: 0.8028, Train_AP: 0.7933 Val_AUC: 0.7900, Val_AP: 0.7898 Test_AUC: 0.7880, Test_AP: 0.7832\n",
      "Epoch: 009, Loss: 0.4215 Train_AUC: 0.8133, Train_AP: 0.7907 Val_AUC: 0.7982, Val_AP: 0.7843 Test_AUC: 0.7943, Test_AP: 0.7751\n",
      "Epoch: 010, Loss: 0.4102 Train_AUC: 0.7794, Train_AP: 0.7234 Val_AUC: 0.7650, Val_AP: 0.7136 Test_AUC: 0.7588, Test_AP: 0.7043\n",
      "Epoch: 011, Loss: 0.4034 Train_AUC: 0.8011, Train_AP: 0.7434 Val_AUC: 0.7856, Val_AP: 0.7330 Test_AUC: 0.7780, Test_AP: 0.7233\n",
      "Epoch: 012, Loss: 0.3942 Train_AUC: 0.8370, Train_AP: 0.8053 Val_AUC: 0.8191, Val_AP: 0.7978 Test_AUC: 0.8132, Test_AP: 0.7887\n",
      "Epoch: 013, Loss: 0.3877 Train_AUC: 0.8196, Train_AP: 0.7852 Val_AUC: 0.8007, Val_AP: 0.7739 Test_AUC: 0.7953, Test_AP: 0.7651\n",
      "Epoch: 014, Loss: 0.3789 Train_AUC: 0.7842, Train_AP: 0.7503 Val_AUC: 0.7632, Val_AP: 0.7325 Test_AUC: 0.7611, Test_AP: 0.7287\n",
      "Epoch: 015, Loss: 0.3738 Train_AUC: 0.7479, Train_AP: 0.7207 Val_AUC: 0.7275, Val_AP: 0.7018 Test_AUC: 0.7265, Test_AP: 0.6995\n",
      "Epoch: 016, Loss: 0.3687 Train_AUC: 0.7249, Train_AP: 0.7034 Val_AUC: 0.7036, Val_AP: 0.6832 Test_AUC: 0.7038, Test_AP: 0.6829\n",
      "Epoch: 017, Loss: 0.3638 Train_AUC: 0.6969, Train_AP: 0.6823 Val_AUC: 0.6748, Val_AP: 0.6636 Test_AUC: 0.6772, Test_AP: 0.6641\n",
      "Epoch: 018, Loss: 0.3581 Train_AUC: 0.6754, Train_AP: 0.6645 Val_AUC: 0.6566, Val_AP: 0.6512 Test_AUC: 0.6595, Test_AP: 0.6508\n",
      "Epoch: 019, Loss: 0.3555 Train_AUC: 0.6557, Train_AP: 0.6530 Val_AUC: 0.6373, Val_AP: 0.6408 Test_AUC: 0.6426, Test_AP: 0.6420\n",
      "Epoch: 020, Loss: 0.3521 Train_AUC: 0.6376, Train_AP: 0.6467 Val_AUC: 0.6189, Val_AP: 0.6339 Test_AUC: 0.6259, Test_AP: 0.6369\n",
      "Epoch: 021, Loss: 0.3483 Train_AUC: 0.6305, Train_AP: 0.6450 Val_AUC: 0.6121, Val_AP: 0.6318 Test_AUC: 0.6191, Test_AP: 0.6355\n",
      "Epoch: 022, Loss: 0.3457 Train_AUC: 0.6393, Train_AP: 0.6506 Val_AUC: 0.6198, Val_AP: 0.6373 Test_AUC: 0.6275, Test_AP: 0.6410\n",
      "Epoch: 023, Loss: 0.3430 Train_AUC: 0.6519, Train_AP: 0.6591 Val_AUC: 0.6306, Val_AP: 0.6449 Test_AUC: 0.6395, Test_AP: 0.6492\n",
      "Epoch: 024, Loss: 0.3421 Train_AUC: 0.6577, Train_AP: 0.6656 Val_AUC: 0.6359, Val_AP: 0.6504 Test_AUC: 0.6446, Test_AP: 0.6547\n",
      "Epoch: 025, Loss: 0.3387 Train_AUC: 0.6575, Train_AP: 0.6698 Val_AUC: 0.6363, Val_AP: 0.6542 Test_AUC: 0.6436, Test_AP: 0.6572\n",
      "Epoch: 026, Loss: 0.3356 Train_AUC: 0.6590, Train_AP: 0.6722 Val_AUC: 0.6374, Val_AP: 0.6562 Test_AUC: 0.6443, Test_AP: 0.6589\n",
      "Epoch: 027, Loss: 0.3331 Train_AUC: 0.6629, Train_AP: 0.6739 Val_AUC: 0.6403, Val_AP: 0.6575 Test_AUC: 0.6478, Test_AP: 0.6605\n",
      "Epoch: 028, Loss: 0.3303 Train_AUC: 0.6680, Train_AP: 0.6768 Val_AUC: 0.6443, Val_AP: 0.6597 Test_AUC: 0.6523, Test_AP: 0.6633\n",
      "Epoch: 029, Loss: 0.3287 Train_AUC: 0.6756, Train_AP: 0.6836 Val_AUC: 0.6519, Val_AP: 0.6660 Test_AUC: 0.6588, Test_AP: 0.6703\n",
      "Epoch: 030, Loss: 0.3262 Train_AUC: 0.6825, Train_AP: 0.6926 Val_AUC: 0.6594, Val_AP: 0.6742 Test_AUC: 0.6650, Test_AP: 0.6797\n",
      "Epoch: 031, Loss: 0.3236 Train_AUC: 0.6867, Train_AP: 0.6980 Val_AUC: 0.6639, Val_AP: 0.6782 Test_AUC: 0.6682, Test_AP: 0.6849\n",
      "Epoch: 032, Loss: 0.3225 Train_AUC: 0.6908, Train_AP: 0.7002 Val_AUC: 0.6674, Val_AP: 0.6792 Test_AUC: 0.6707, Test_AP: 0.6864\n",
      "Epoch: 033, Loss: 0.3191 Train_AUC: 0.6923, Train_AP: 0.6999 Val_AUC: 0.6674, Val_AP: 0.6780 Test_AUC: 0.6705, Test_AP: 0.6851\n",
      "Epoch: 034, Loss: 0.3176 Train_AUC: 0.6876, Train_AP: 0.6975 Val_AUC: 0.6613, Val_AP: 0.6744 Test_AUC: 0.6641, Test_AP: 0.6818\n",
      "Epoch: 035, Loss: 0.3166 Train_AUC: 0.6823, Train_AP: 0.6971 Val_AUC: 0.6547, Val_AP: 0.6723 Test_AUC: 0.6574, Test_AP: 0.6804\n",
      "Epoch: 036, Loss: 0.3138 Train_AUC: 0.6918, Train_AP: 0.7034 Val_AUC: 0.6627, Val_AP: 0.6769 Test_AUC: 0.6648, Test_AP: 0.6849\n",
      "Epoch: 037, Loss: 0.3117 Train_AUC: 0.7118, Train_AP: 0.7150 Val_AUC: 0.6805, Val_AP: 0.6867 Test_AUC: 0.6827, Test_AP: 0.6945\n",
      "Epoch: 038, Loss: 0.3108 Train_AUC: 0.7236, Train_AP: 0.7230 Val_AUC: 0.6901, Val_AP: 0.6930 Test_AUC: 0.6922, Test_AP: 0.7004\n",
      "Epoch: 039, Loss: 0.3085 Train_AUC: 0.7224, Train_AP: 0.7240 Val_AUC: 0.6873, Val_AP: 0.6926 Test_AUC: 0.6890, Test_AP: 0.7002\n",
      "Epoch: 040, Loss: 0.3080 Train_AUC: 0.7274, Train_AP: 0.7285 Val_AUC: 0.6907, Val_AP: 0.6955 Test_AUC: 0.6920, Test_AP: 0.7030\n",
      "Epoch: 041, Loss: 0.3057 Train_AUC: 0.7400, Train_AP: 0.7367 Val_AUC: 0.7019, Val_AP: 0.7023 Test_AUC: 0.7023, Test_AP: 0.7092\n",
      "Epoch: 042, Loss: 0.3039 Train_AUC: 0.7546, Train_AP: 0.7458 Val_AUC: 0.7152, Val_AP: 0.7103 Test_AUC: 0.7145, Test_AP: 0.7162\n",
      "Epoch: 043, Loss: 0.3024 Train_AUC: 0.7573, Train_AP: 0.7483 Val_AUC: 0.7168, Val_AP: 0.7115 Test_AUC: 0.7157, Test_AP: 0.7175\n",
      "Epoch: 044, Loss: 0.3008 Train_AUC: 0.7617, Train_AP: 0.7515 Val_AUC: 0.7190, Val_AP: 0.7125 Test_AUC: 0.7178, Test_AP: 0.7182\n",
      "Epoch: 045, Loss: 0.2993 Train_AUC: 0.7717, Train_AP: 0.7593 Val_AUC: 0.7257, Val_AP: 0.7173 Test_AUC: 0.7245, Test_AP: 0.7223\n",
      "Epoch: 046, Loss: 0.2988 Train_AUC: 0.7872, Train_AP: 0.7721 Val_AUC: 0.7386, Val_AP: 0.7276 Test_AUC: 0.7364, Test_AP: 0.7314\n",
      "Epoch: 047, Loss: 0.2970 Train_AUC: 0.7968, Train_AP: 0.7802 Val_AUC: 0.7461, Val_AP: 0.7335 Test_AUC: 0.7435, Test_AP: 0.7375\n",
      "Epoch: 048, Loss: 0.2958 Train_AUC: 0.8028, Train_AP: 0.7850 Val_AUC: 0.7505, Val_AP: 0.7361 Test_AUC: 0.7476, Test_AP: 0.7401\n",
      "Epoch: 049, Loss: 0.2939 Train_AUC: 0.8099, Train_AP: 0.7905 Val_AUC: 0.7554, Val_AP: 0.7393 Test_AUC: 0.7521, Test_AP: 0.7422\n",
      "Epoch: 050, Loss: 0.2925 Train_AUC: 0.8176, Train_AP: 0.7961 Val_AUC: 0.7607, Val_AP: 0.7427 Test_AUC: 0.7569, Test_AP: 0.7444\n",
      "Epoch: 051, Loss: 0.2915 Train_AUC: 0.8277, Train_AP: 0.8042 Val_AUC: 0.7684, Val_AP: 0.7479 Test_AUC: 0.7642, Test_AP: 0.7491\n",
      "Epoch: 052, Loss: 0.2904 Train_AUC: 0.8346, Train_AP: 0.8109 Val_AUC: 0.7729, Val_AP: 0.7514 Test_AUC: 0.7687, Test_AP: 0.7531\n",
      "Epoch: 053, Loss: 0.2888 Train_AUC: 0.8428, Train_AP: 0.8187 Val_AUC: 0.7777, Val_AP: 0.7560 Test_AUC: 0.7740, Test_AP: 0.7579\n",
      "Epoch: 054, Loss: 0.2872 Train_AUC: 0.8582, Train_AP: 0.8335 Val_AUC: 0.7888, Val_AP: 0.7669 Test_AUC: 0.7854, Test_AP: 0.7680\n",
      "Epoch: 055, Loss: 0.2867 Train_AUC: 0.8662, Train_AP: 0.8403 Val_AUC: 0.7941, Val_AP: 0.7710 Test_AUC: 0.7909, Test_AP: 0.7721\n",
      "Epoch: 056, Loss: 0.2850 Train_AUC: 0.8688, Train_AP: 0.8412 Val_AUC: 0.7955, Val_AP: 0.7706 Test_AUC: 0.7926, Test_AP: 0.7720\n",
      "Epoch: 057, Loss: 0.2845 Train_AUC: 0.8715, Train_AP: 0.8430 Val_AUC: 0.7968, Val_AP: 0.7703 Test_AUC: 0.7938, Test_AP: 0.7719\n",
      "Epoch: 058, Loss: 0.2831 Train_AUC: 0.8825, Train_AP: 0.8548 Val_AUC: 0.8055, Val_AP: 0.7799 Test_AUC: 0.8019, Test_AP: 0.7804\n",
      "Epoch: 059, Loss: 0.2823 Train_AUC: 0.8910, Train_AP: 0.8649 Val_AUC: 0.8113, Val_AP: 0.7871 Test_AUC: 0.8077, Test_AP: 0.7870\n",
      "Epoch: 060, Loss: 0.2810 Train_AUC: 0.9017, Train_AP: 0.8758 Val_AUC: 0.8198, Val_AP: 0.7959 Test_AUC: 0.8163, Test_AP: 0.7954\n",
      "Epoch: 061, Loss: 0.2805 Train_AUC: 0.9126, Train_AP: 0.8880 Val_AUC: 0.8285, Val_AP: 0.8075 Test_AUC: 0.8263, Test_AP: 0.8075\n",
      "Epoch: 062, Loss: 0.2792 Train_AUC: 0.9158, Train_AP: 0.8913 Val_AUC: 0.8303, Val_AP: 0.8088 Test_AUC: 0.8276, Test_AP: 0.8079\n",
      "Epoch: 063, Loss: 0.2787 Train_AUC: 0.9181, Train_AP: 0.8938 Val_AUC: 0.8314, Val_AP: 0.8091 Test_AUC: 0.8274, Test_AP: 0.8069\n",
      "Epoch: 064, Loss: 0.2779 Train_AUC: 0.9264, Train_AP: 0.9046 Val_AUC: 0.8383, Val_AP: 0.8192 Test_AUC: 0.8339, Test_AP: 0.8166\n",
      "Epoch: 065, Loss: 0.2765 Train_AUC: 0.9328, Train_AP: 0.9121 Val_AUC: 0.8428, Val_AP: 0.8255 Test_AUC: 0.8389, Test_AP: 0.8233\n",
      "Epoch: 066, Loss: 0.2759 Train_AUC: 0.9382, Train_AP: 0.9184 Val_AUC: 0.8449, Val_AP: 0.8291 Test_AUC: 0.8420, Test_AP: 0.8272\n",
      "Epoch: 067, Loss: 0.2742 Train_AUC: 0.9433, Train_AP: 0.9245 Val_AUC: 0.8480, Val_AP: 0.8325 Test_AUC: 0.8461, Test_AP: 0.8316\n",
      "Epoch: 068, Loss: 0.2749 Train_AUC: 0.9504, Train_AP: 0.9342 Val_AUC: 0.8553, Val_AP: 0.8445 Test_AUC: 0.8541, Test_AP: 0.8437\n",
      "Epoch: 069, Loss: 0.2732 Train_AUC: 0.9499, Train_AP: 0.9328 Val_AUC: 0.8557, Val_AP: 0.8426 Test_AUC: 0.8535, Test_AP: 0.8416\n",
      "Epoch: 070, Loss: 0.2718 Train_AUC: 0.9556, Train_AP: 0.9409 Val_AUC: 0.8605, Val_AP: 0.8494 Test_AUC: 0.8584, Test_AP: 0.8476\n",
      "Epoch: 071, Loss: 0.2712 Train_AUC: 0.9616, Train_AP: 0.9493 Val_AUC: 0.8655, Val_AP: 0.8579 Test_AUC: 0.8636, Test_AP: 0.8549\n",
      "Epoch: 072, Loss: 0.2704 Train_AUC: 0.9587, Train_AP: 0.9437 Val_AUC: 0.8623, Val_AP: 0.8499 Test_AUC: 0.8613, Test_AP: 0.8485\n",
      "Epoch: 073, Loss: 0.2701 Train_AUC: 0.9652, Train_AP: 0.9539 Val_AUC: 0.8717, Val_AP: 0.8674 Test_AUC: 0.8707, Test_AP: 0.8643\n",
      "Epoch: 074, Loss: 0.2696 Train_AUC: 0.9688, Train_AP: 0.9586 Val_AUC: 0.8758, Val_AP: 0.8715 Test_AUC: 0.8743, Test_AP: 0.8683\n",
      "Epoch: 075, Loss: 0.2667 Train_AUC: 0.9699, Train_AP: 0.9595 Val_AUC: 0.8765, Val_AP: 0.8692 Test_AUC: 0.8746, Test_AP: 0.8657\n",
      "Epoch: 076, Loss: 0.2675 Train_AUC: 0.9749, Train_AP: 0.9672 Val_AUC: 0.8847, Val_AP: 0.8846 Test_AUC: 0.8834, Test_AP: 0.8814\n",
      "Epoch: 077, Loss: 0.2658 Train_AUC: 0.9743, Train_AP: 0.9659 Val_AUC: 0.8847, Val_AP: 0.8822 Test_AUC: 0.8836, Test_AP: 0.8794\n",
      "Epoch: 078, Loss: 0.2651 Train_AUC: 0.9759, Train_AP: 0.9679 Val_AUC: 0.8859, Val_AP: 0.8817 Test_AUC: 0.8840, Test_AP: 0.8781\n",
      "Epoch: 079, Loss: 0.2655 Train_AUC: 0.9806, Train_AP: 0.9744 Val_AUC: 0.8955, Val_AP: 0.8988 Test_AUC: 0.8941, Test_AP: 0.8950\n",
      "Epoch: 080, Loss: 0.2654 Train_AUC: 0.9802, Train_AP: 0.9739 Val_AUC: 0.8959, Val_AP: 0.8955 Test_AUC: 0.8937, Test_AP: 0.8916\n",
      "Epoch: 081, Loss: 0.2630 Train_AUC: 0.9808, Train_AP: 0.9746 Val_AUC: 0.8977, Val_AP: 0.8967 Test_AUC: 0.8958, Test_AP: 0.8933\n",
      "Epoch: 082, Loss: 0.2625 Train_AUC: 0.9826, Train_AP: 0.9765 Val_AUC: 0.9033, Val_AP: 0.9060 Test_AUC: 0.9022, Test_AP: 0.9027\n",
      "Epoch: 083, Loss: 0.2623 Train_AUC: 0.9839, Train_AP: 0.9785 Val_AUC: 0.9052, Val_AP: 0.9061 Test_AUC: 0.9032, Test_AP: 0.9026\n",
      "Epoch: 084, Loss: 0.2599 Train_AUC: 0.9838, Train_AP: 0.9783 Val_AUC: 0.9061, Val_AP: 0.9047 Test_AUC: 0.9035, Test_AP: 0.9009\n",
      "Epoch: 085, Loss: 0.2604 Train_AUC: 0.9843, Train_AP: 0.9784 Val_AUC: 0.9102, Val_AP: 0.9111 Test_AUC: 0.9084, Test_AP: 0.9080\n",
      "Epoch: 086, Loss: 0.2595 Train_AUC: 0.9852, Train_AP: 0.9801 Val_AUC: 0.9091, Val_AP: 0.9088 Test_AUC: 0.9070, Test_AP: 0.9045\n",
      "Epoch: 087, Loss: 0.2595 Train_AUC: 0.9858, Train_AP: 0.9806 Val_AUC: 0.9114, Val_AP: 0.9109 Test_AUC: 0.9093, Test_AP: 0.9076\n",
      "Epoch: 088, Loss: 0.2579 Train_AUC: 0.9855, Train_AP: 0.9796 Val_AUC: 0.9142, Val_AP: 0.9129 Test_AUC: 0.9125, Test_AP: 0.9112\n",
      "Epoch: 089, Loss: 0.2585 Train_AUC: 0.9868, Train_AP: 0.9819 Val_AUC: 0.9121, Val_AP: 0.9089 Test_AUC: 0.9092, Test_AP: 0.9044\n",
      "Epoch: 090, Loss: 0.2584 Train_AUC: 0.9858, Train_AP: 0.9796 Val_AUC: 0.9169, Val_AP: 0.9164 Test_AUC: 0.9150, Test_AP: 0.9140\n",
      "Epoch: 091, Loss: 0.2582 Train_AUC: 0.9866, Train_AP: 0.9813 Val_AUC: 0.9163, Val_AP: 0.9141 Test_AUC: 0.9138, Test_AP: 0.9111\n",
      "Epoch: 092, Loss: 0.2558 Train_AUC: 0.9871, Train_AP: 0.9820 Val_AUC: 0.9156, Val_AP: 0.9126 Test_AUC: 0.9131, Test_AP: 0.9098\n",
      "Epoch: 093, Loss: 0.2562 Train_AUC: 0.9873, Train_AP: 0.9822 Val_AUC: 0.9173, Val_AP: 0.9165 Test_AUC: 0.9152, Test_AP: 0.9133\n",
      "Epoch: 094, Loss: 0.2557 Train_AUC: 0.9882, Train_AP: 0.9834 Val_AUC: 0.9185, Val_AP: 0.9168 Test_AUC: 0.9162, Test_AP: 0.9140\n",
      "Epoch: 095, Loss: 0.2540 Train_AUC: 0.9885, Train_AP: 0.9839 Val_AUC: 0.9194, Val_AP: 0.9164 Test_AUC: 0.9169, Test_AP: 0.9137\n",
      "Epoch: 096, Loss: 0.2536 Train_AUC: 0.9883, Train_AP: 0.9837 Val_AUC: 0.9210, Val_AP: 0.9199 Test_AUC: 0.9192, Test_AP: 0.9182\n",
      "Epoch: 097, Loss: 0.2536 Train_AUC: 0.9892, Train_AP: 0.9848 Val_AUC: 0.9195, Val_AP: 0.9159 Test_AUC: 0.9174, Test_AP: 0.9125\n",
      "Epoch: 098, Loss: 0.2531 Train_AUC: 0.9885, Train_AP: 0.9834 Val_AUC: 0.9207, Val_AP: 0.9177 Test_AUC: 0.9194, Test_AP: 0.9170\n",
      "Epoch: 099, Loss: 0.2524 Train_AUC: 0.9887, Train_AP: 0.9842 Val_AUC: 0.9214, Val_AP: 0.9195 Test_AUC: 0.9202, Test_AP: 0.9184\n",
      "Epoch: 100, Loss: 0.2525 Train_AUC: 0.9897, Train_AP: 0.9858 Val_AUC: 0.9200, Val_AP: 0.9169 Test_AUC: 0.9182, Test_AP: 0.9137\n",
      "Epoch: 101, Loss: 0.2513 Train_AUC: 0.9885, Train_AP: 0.9829 Val_AUC: 0.9225, Val_AP: 0.9201 Test_AUC: 0.9213, Test_AP: 0.9198\n",
      "Epoch: 102, Loss: 0.2523 Train_AUC: 0.9899, Train_AP: 0.9854 Val_AUC: 0.9215, Val_AP: 0.9180 Test_AUC: 0.9196, Test_AP: 0.9163\n",
      "Epoch: 103, Loss: 0.2497 Train_AUC: 0.9900, Train_AP: 0.9860 Val_AUC: 0.9198, Val_AP: 0.9160 Test_AUC: 0.9184, Test_AP: 0.9140\n",
      "Epoch: 104, Loss: 0.2500 Train_AUC: 0.9881, Train_AP: 0.9827 Val_AUC: 0.9221, Val_AP: 0.9198 Test_AUC: 0.9212, Test_AP: 0.9197\n",
      "Epoch: 105, Loss: 0.2502 Train_AUC: 0.9896, Train_AP: 0.9850 Val_AUC: 0.9203, Val_AP: 0.9149 Test_AUC: 0.9182, Test_AP: 0.9126\n",
      "Epoch: 106, Loss: 0.2504 Train_AUC: 0.9900, Train_AP: 0.9861 Val_AUC: 0.9228, Val_AP: 0.9203 Test_AUC: 0.9207, Test_AP: 0.9185\n",
      "Epoch: 107, Loss: 0.2476 Train_AUC: 0.9896, Train_AP: 0.9855 Val_AUC: 0.9222, Val_AP: 0.9204 Test_AUC: 0.9208, Test_AP: 0.9195\n",
      "Epoch: 108, Loss: 0.2482 Train_AUC: 0.9899, Train_AP: 0.9853 Val_AUC: 0.9193, Val_AP: 0.9137 Test_AUC: 0.9178, Test_AP: 0.9125\n",
      "Epoch: 109, Loss: 0.2488 Train_AUC: 0.9891, Train_AP: 0.9838 Val_AUC: 0.9230, Val_AP: 0.9204 Test_AUC: 0.9220, Test_AP: 0.9202\n",
      "Epoch: 110, Loss: 0.2470 Train_AUC: 0.9909, Train_AP: 0.9872 Val_AUC: 0.9224, Val_AP: 0.9201 Test_AUC: 0.9205, Test_AP: 0.9176\n",
      "Epoch: 111, Loss: 0.2451 Train_AUC: 0.9912, Train_AP: 0.9873 Val_AUC: 0.9203, Val_AP: 0.9166 Test_AUC: 0.9178, Test_AP: 0.9134\n",
      "Epoch: 112, Loss: 0.2462 Train_AUC: 0.9892, Train_AP: 0.9840 Val_AUC: 0.9219, Val_AP: 0.9189 Test_AUC: 0.9204, Test_AP: 0.9189\n",
      "Epoch: 113, Loss: 0.2464 Train_AUC: 0.9907, Train_AP: 0.9865 Val_AUC: 0.9215, Val_AP: 0.9189 Test_AUC: 0.9194, Test_AP: 0.9173\n",
      "Epoch: 114, Loss: 0.2445 Train_AUC: 0.9912, Train_AP: 0.9874 Val_AUC: 0.9203, Val_AP: 0.9169 Test_AUC: 0.9181, Test_AP: 0.9144\n",
      "Epoch: 115, Loss: 0.2454 Train_AUC: 0.9893, Train_AP: 0.9841 Val_AUC: 0.9226, Val_AP: 0.9183 Test_AUC: 0.9210, Test_AP: 0.9182\n",
      "Epoch: 116, Loss: 0.2452 Train_AUC: 0.9911, Train_AP: 0.9870 Val_AUC: 0.9218, Val_AP: 0.9173 Test_AUC: 0.9193, Test_AP: 0.9158\n",
      "Epoch: 117, Loss: 0.2431 Train_AUC: 0.9915, Train_AP: 0.9881 Val_AUC: 0.9216, Val_AP: 0.9194 Test_AUC: 0.9192, Test_AP: 0.9163\n",
      "Epoch: 118, Loss: 0.2433 Train_AUC: 0.9904, Train_AP: 0.9855 Val_AUC: 0.9222, Val_AP: 0.9199 Test_AUC: 0.9199, Test_AP: 0.9183\n",
      "Epoch: 119, Loss: 0.2432 Train_AUC: 0.9909, Train_AP: 0.9866 Val_AUC: 0.9215, Val_AP: 0.9184 Test_AUC: 0.9186, Test_AP: 0.9162\n",
      "Epoch: 120, Loss: 0.2419 Train_AUC: 0.9913, Train_AP: 0.9879 Val_AUC: 0.9212, Val_AP: 0.9181 Test_AUC: 0.9184, Test_AP: 0.9161\n",
      "Epoch: 121, Loss: 0.2429 Train_AUC: 0.9914, Train_AP: 0.9876 Val_AUC: 0.9217, Val_AP: 0.9201 Test_AUC: 0.9190, Test_AP: 0.9177\n",
      "Epoch: 122, Loss: 0.2406 Train_AUC: 0.9918, Train_AP: 0.9877 Val_AUC: 0.9216, Val_AP: 0.9182 Test_AUC: 0.9195, Test_AP: 0.9167\n",
      "Epoch: 123, Loss: 0.2408 Train_AUC: 0.9919, Train_AP: 0.9881 Val_AUC: 0.9235, Val_AP: 0.9199 Test_AUC: 0.9216, Test_AP: 0.9186\n",
      "Epoch: 124, Loss: 0.2405 Train_AUC: 0.9924, Train_AP: 0.9890 Val_AUC: 0.9236, Val_AP: 0.9201 Test_AUC: 0.9211, Test_AP: 0.9170\n",
      "Epoch: 125, Loss: 0.2394 Train_AUC: 0.9916, Train_AP: 0.9874 Val_AUC: 0.9238, Val_AP: 0.9201 Test_AUC: 0.9221, Test_AP: 0.9198\n",
      "Epoch: 126, Loss: 0.2399 Train_AUC: 0.9920, Train_AP: 0.9883 Val_AUC: 0.9226, Val_AP: 0.9182 Test_AUC: 0.9212, Test_AP: 0.9186\n",
      "Epoch: 127, Loss: 0.2391 Train_AUC: 0.9924, Train_AP: 0.9891 Val_AUC: 0.9240, Val_AP: 0.9200 Test_AUC: 0.9223, Test_AP: 0.9203\n",
      "Epoch: 128, Loss: 0.2385 Train_AUC: 0.9925, Train_AP: 0.9890 Val_AUC: 0.9253, Val_AP: 0.9216 Test_AUC: 0.9232, Test_AP: 0.9213\n",
      "Epoch: 129, Loss: 0.2379 Train_AUC: 0.9923, Train_AP: 0.9886 Val_AUC: 0.9225, Val_AP: 0.9174 Test_AUC: 0.9191, Test_AP: 0.9148\n",
      "Epoch: 130, Loss: 0.2391 Train_AUC: 0.9899, Train_AP: 0.9854 Val_AUC: 0.9232, Val_AP: 0.9208 Test_AUC: 0.9214, Test_AP: 0.9202\n",
      "Epoch: 131, Loss: 0.2410 Train_AUC: 0.9917, Train_AP: 0.9881 Val_AUC: 0.9187, Val_AP: 0.9128 Test_AUC: 0.9149, Test_AP: 0.9089\n",
      "Epoch: 132, Loss: 0.2425 Train_AUC: 0.9893, Train_AP: 0.9840 Val_AUC: 0.9236, Val_AP: 0.9205 Test_AUC: 0.9222, Test_AP: 0.9208\n",
      "Epoch: 133, Loss: 0.2416 Train_AUC: 0.9925, Train_AP: 0.9889 Val_AUC: 0.9247, Val_AP: 0.9220 Test_AUC: 0.9224, Test_AP: 0.9208\n",
      "Epoch: 134, Loss: 0.2358 Train_AUC: 0.9926, Train_AP: 0.9894 Val_AUC: 0.9218, Val_AP: 0.9167 Test_AUC: 0.9188, Test_AP: 0.9140\n",
      "Epoch: 135, Loss: 0.2393 Train_AUC: 0.9903, Train_AP: 0.9856 Val_AUC: 0.9249, Val_AP: 0.9201 Test_AUC: 0.9236, Test_AP: 0.9215\n",
      "Epoch: 136, Loss: 0.2394 Train_AUC: 0.9925, Train_AP: 0.9889 Val_AUC: 0.9259, Val_AP: 0.9210 Test_AUC: 0.9234, Test_AP: 0.9216\n",
      "Epoch: 137, Loss: 0.2359 Train_AUC: 0.9923, Train_AP: 0.9891 Val_AUC: 0.9211, Val_AP: 0.9144 Test_AUC: 0.9178, Test_AP: 0.9116\n",
      "Epoch: 138, Loss: 0.2375 Train_AUC: 0.9929, Train_AP: 0.9896 Val_AUC: 0.9255, Val_AP: 0.9210 Test_AUC: 0.9230, Test_AP: 0.9225\n",
      "Epoch: 139, Loss: 0.2352 Train_AUC: 0.9918, Train_AP: 0.9879 Val_AUC: 0.9250, Val_AP: 0.9204 Test_AUC: 0.9234, Test_AP: 0.9230\n",
      "Epoch: 140, Loss: 0.2367 Train_AUC: 0.9928, Train_AP: 0.9895 Val_AUC: 0.9223, Val_AP: 0.9164 Test_AUC: 0.9189, Test_AP: 0.9134\n",
      "Epoch: 141, Loss: 0.2356 Train_AUC: 0.9935, Train_AP: 0.9904 Val_AUC: 0.9250, Val_AP: 0.9207 Test_AUC: 0.9222, Test_AP: 0.9196\n",
      "Epoch: 142, Loss: 0.2333 Train_AUC: 0.9924, Train_AP: 0.9888 Val_AUC: 0.9256, Val_AP: 0.9222 Test_AUC: 0.9245, Test_AP: 0.9239\n",
      "Epoch: 143, Loss: 0.2348 Train_AUC: 0.9934, Train_AP: 0.9901 Val_AUC: 0.9239, Val_AP: 0.9191 Test_AUC: 0.9214, Test_AP: 0.9186\n",
      "Epoch: 144, Loss: 0.2337 Train_AUC: 0.9932, Train_AP: 0.9899 Val_AUC: 0.9214, Val_AP: 0.9157 Test_AUC: 0.9188, Test_AP: 0.9147\n",
      "Epoch: 145, Loss: 0.2339 Train_AUC: 0.9922, Train_AP: 0.9884 Val_AUC: 0.9231, Val_AP: 0.9202 Test_AUC: 0.9222, Test_AP: 0.9223\n",
      "Epoch: 146, Loss: 0.2350 Train_AUC: 0.9939, Train_AP: 0.9909 Val_AUC: 0.9219, Val_AP: 0.9183 Test_AUC: 0.9195, Test_AP: 0.9183\n",
      "Epoch: 147, Loss: 0.2322 Train_AUC: 0.9939, Train_AP: 0.9908 Val_AUC: 0.9221, Val_AP: 0.9179 Test_AUC: 0.9203, Test_AP: 0.9179\n",
      "Epoch: 148, Loss: 0.2322 Train_AUC: 0.9933, Train_AP: 0.9898 Val_AUC: 0.9244, Val_AP: 0.9216 Test_AUC: 0.9246, Test_AP: 0.9244\n",
      "Epoch: 149, Loss: 0.2331 Train_AUC: 0.9940, Train_AP: 0.9909 Val_AUC: 0.9228, Val_AP: 0.9193 Test_AUC: 0.9214, Test_AP: 0.9207\n",
      "Epoch: 150, Loss: 0.2313 Train_AUC: 0.9935, Train_AP: 0.9901 Val_AUC: 0.9194, Val_AP: 0.9155 Test_AUC: 0.9175, Test_AP: 0.9169\n",
      "Epoch: 151, Loss: 0.2321 Train_AUC: 0.9916, Train_AP: 0.9869 Val_AUC: 0.9214, Val_AP: 0.9187 Test_AUC: 0.9206, Test_AP: 0.9205\n",
      "Epoch: 152, Loss: 0.2328 Train_AUC: 0.9940, Train_AP: 0.9909 Val_AUC: 0.9213, Val_AP: 0.9184 Test_AUC: 0.9190, Test_AP: 0.9172\n",
      "Epoch: 153, Loss: 0.2308 Train_AUC: 0.9932, Train_AP: 0.9896 Val_AUC: 0.9201, Val_AP: 0.9151 Test_AUC: 0.9182, Test_AP: 0.9163\n",
      "Epoch: 154, Loss: 0.2321 Train_AUC: 0.9918, Train_AP: 0.9870 Val_AUC: 0.9226, Val_AP: 0.9199 Test_AUC: 0.9226, Test_AP: 0.9224\n",
      "Epoch: 155, Loss: 0.2313 Train_AUC: 0.9938, Train_AP: 0.9910 Val_AUC: 0.9222, Val_AP: 0.9201 Test_AUC: 0.9207, Test_AP: 0.9197\n",
      "Epoch: 156, Loss: 0.2301 Train_AUC: 0.9938, Train_AP: 0.9907 Val_AUC: 0.9206, Val_AP: 0.9159 Test_AUC: 0.9186, Test_AP: 0.9171\n",
      "Epoch: 157, Loss: 0.2304 Train_AUC: 0.9939, Train_AP: 0.9907 Val_AUC: 0.9230, Val_AP: 0.9203 Test_AUC: 0.9206, Test_AP: 0.9192\n",
      "Epoch: 158, Loss: 0.2288 Train_AUC: 0.9928, Train_AP: 0.9893 Val_AUC: 0.9235, Val_AP: 0.9213 Test_AUC: 0.9223, Test_AP: 0.9210\n",
      "Epoch: 159, Loss: 0.2300 Train_AUC: 0.9939, Train_AP: 0.9905 Val_AUC: 0.9223, Val_AP: 0.9183 Test_AUC: 0.9205, Test_AP: 0.9184\n",
      "Epoch: 160, Loss: 0.2284 Train_AUC: 0.9943, Train_AP: 0.9911 Val_AUC: 0.9221, Val_AP: 0.9186 Test_AUC: 0.9207, Test_AP: 0.9195\n",
      "Epoch: 161, Loss: 0.2270 Train_AUC: 0.9936, Train_AP: 0.9905 Val_AUC: 0.9217, Val_AP: 0.9195 Test_AUC: 0.9209, Test_AP: 0.9208\n",
      "Epoch: 162, Loss: 0.2289 Train_AUC: 0.9942, Train_AP: 0.9908 Val_AUC: 0.9219, Val_AP: 0.9177 Test_AUC: 0.9207, Test_AP: 0.9201\n",
      "Epoch: 163, Loss: 0.2268 Train_AUC: 0.9942, Train_AP: 0.9909 Val_AUC: 0.9219, Val_AP: 0.9171 Test_AUC: 0.9203, Test_AP: 0.9181\n",
      "Epoch: 164, Loss: 0.2276 Train_AUC: 0.9934, Train_AP: 0.9902 Val_AUC: 0.9237, Val_AP: 0.9212 Test_AUC: 0.9228, Test_AP: 0.9217\n",
      "Epoch: 165, Loss: 0.2288 Train_AUC: 0.9948, Train_AP: 0.9921 Val_AUC: 0.9226, Val_AP: 0.9185 Test_AUC: 0.9201, Test_AP: 0.9188\n",
      "Epoch: 166, Loss: 0.2264 Train_AUC: 0.9942, Train_AP: 0.9910 Val_AUC: 0.9232, Val_AP: 0.9184 Test_AUC: 0.9215, Test_AP: 0.9208\n",
      "Epoch: 167, Loss: 0.2263 Train_AUC: 0.9942, Train_AP: 0.9911 Val_AUC: 0.9240, Val_AP: 0.9207 Test_AUC: 0.9229, Test_AP: 0.9224\n",
      "Epoch: 168, Loss: 0.2266 Train_AUC: 0.9946, Train_AP: 0.9920 Val_AUC: 0.9237, Val_AP: 0.9198 Test_AUC: 0.9221, Test_AP: 0.9214\n",
      "Epoch: 169, Loss: 0.2254 Train_AUC: 0.9942, Train_AP: 0.9910 Val_AUC: 0.9253, Val_AP: 0.9202 Test_AUC: 0.9237, Test_AP: 0.9226\n",
      "Epoch: 170, Loss: 0.2261 Train_AUC: 0.9942, Train_AP: 0.9911 Val_AUC: 0.9255, Val_AP: 0.9222 Test_AUC: 0.9245, Test_AP: 0.9242\n",
      "Epoch: 171, Loss: 0.2257 Train_AUC: 0.9948, Train_AP: 0.9923 Val_AUC: 0.9227, Val_AP: 0.9194 Test_AUC: 0.9216, Test_AP: 0.9212\n",
      "Epoch: 172, Loss: 0.2250 Train_AUC: 0.9936, Train_AP: 0.9901 Val_AUC: 0.9229, Val_AP: 0.9178 Test_AUC: 0.9228, Test_AP: 0.9221\n",
      "Epoch: 173, Loss: 0.2260 Train_AUC: 0.9948, Train_AP: 0.9921 Val_AUC: 0.9241, Val_AP: 0.9213 Test_AUC: 0.9236, Test_AP: 0.9229\n",
      "Epoch: 174, Loss: 0.2247 Train_AUC: 0.9941, Train_AP: 0.9910 Val_AUC: 0.9242, Val_AP: 0.9224 Test_AUC: 0.9230, Test_AP: 0.9224\n",
      "Epoch: 175, Loss: 0.2255 Train_AUC: 0.9939, Train_AP: 0.9907 Val_AUC: 0.9209, Val_AP: 0.9168 Test_AUC: 0.9196, Test_AP: 0.9203\n",
      "Epoch: 176, Loss: 0.2254 Train_AUC: 0.9948, Train_AP: 0.9924 Val_AUC: 0.9214, Val_AP: 0.9181 Test_AUC: 0.9203, Test_AP: 0.9208\n",
      "Epoch: 177, Loss: 0.2237 Train_AUC: 0.9942, Train_AP: 0.9908 Val_AUC: 0.9245, Val_AP: 0.9218 Test_AUC: 0.9237, Test_AP: 0.9239\n",
      "Epoch: 178, Loss: 0.2242 Train_AUC: 0.9946, Train_AP: 0.9916 Val_AUC: 0.9221, Val_AP: 0.9174 Test_AUC: 0.9208, Test_AP: 0.9198\n",
      "Epoch: 179, Loss: 0.2240 Train_AUC: 0.9939, Train_AP: 0.9910 Val_AUC: 0.9210, Val_AP: 0.9174 Test_AUC: 0.9208, Test_AP: 0.9219\n",
      "Epoch: 180, Loss: 0.2240 Train_AUC: 0.9950, Train_AP: 0.9929 Val_AUC: 0.9169, Val_AP: 0.9133 Test_AUC: 0.9148, Test_AP: 0.9142\n",
      "Epoch: 181, Loss: 0.2242 Train_AUC: 0.9935, Train_AP: 0.9895 Val_AUC: 0.9237, Val_AP: 0.9193 Test_AUC: 0.9217, Test_AP: 0.9214\n",
      "Epoch: 182, Loss: 0.2258 Train_AUC: 0.9951, Train_AP: 0.9927 Val_AUC: 0.9236, Val_AP: 0.9202 Test_AUC: 0.9213, Test_AP: 0.9205\n",
      "Epoch: 183, Loss: 0.2225 Train_AUC: 0.9949, Train_AP: 0.9928 Val_AUC: 0.9213, Val_AP: 0.9182 Test_AUC: 0.9202, Test_AP: 0.9202\n",
      "Epoch: 184, Loss: 0.2236 Train_AUC: 0.9943, Train_AP: 0.9913 Val_AUC: 0.9208, Val_AP: 0.9173 Test_AUC: 0.9193, Test_AP: 0.9206\n",
      "Epoch: 185, Loss: 0.2221 Train_AUC: 0.9949, Train_AP: 0.9923 Val_AUC: 0.9213, Val_AP: 0.9186 Test_AUC: 0.9188, Test_AP: 0.9175\n",
      "Epoch: 186, Loss: 0.2226 Train_AUC: 0.9951, Train_AP: 0.9928 Val_AUC: 0.9241, Val_AP: 0.9216 Test_AUC: 0.9221, Test_AP: 0.9219\n",
      "Epoch: 187, Loss: 0.2218 Train_AUC: 0.9951, Train_AP: 0.9925 Val_AUC: 0.9240, Val_AP: 0.9207 Test_AUC: 0.9219, Test_AP: 0.9226\n",
      "Epoch: 188, Loss: 0.2217 Train_AUC: 0.9946, Train_AP: 0.9916 Val_AUC: 0.9222, Val_AP: 0.9200 Test_AUC: 0.9205, Test_AP: 0.9225\n",
      "Epoch: 189, Loss: 0.2216 Train_AUC: 0.9953, Train_AP: 0.9934 Val_AUC: 0.9190, Val_AP: 0.9159 Test_AUC: 0.9167, Test_AP: 0.9163\n",
      "Epoch: 190, Loss: 0.2218 Train_AUC: 0.9951, Train_AP: 0.9926 Val_AUC: 0.9236, Val_AP: 0.9191 Test_AUC: 0.9225, Test_AP: 0.9231\n",
      "Epoch: 191, Loss: 0.2220 Train_AUC: 0.9954, Train_AP: 0.9931 Val_AUC: 0.9212, Val_AP: 0.9157 Test_AUC: 0.9188, Test_AP: 0.9166\n",
      "Epoch: 192, Loss: 0.2214 Train_AUC: 0.9946, Train_AP: 0.9919 Val_AUC: 0.9197, Val_AP: 0.9170 Test_AUC: 0.9178, Test_AP: 0.9176\n",
      "Epoch: 193, Loss: 0.2231 Train_AUC: 0.9956, Train_AP: 0.9935 Val_AUC: 0.9208, Val_AP: 0.9170 Test_AUC: 0.9197, Test_AP: 0.9201\n",
      "Epoch: 194, Loss: 0.2202 Train_AUC: 0.9954, Train_AP: 0.9932 Val_AUC: 0.9222, Val_AP: 0.9167 Test_AUC: 0.9216, Test_AP: 0.9208\n",
      "Epoch: 195, Loss: 0.2215 Train_AUC: 0.9945, Train_AP: 0.9914 Val_AUC: 0.9245, Val_AP: 0.9232 Test_AUC: 0.9237, Test_AP: 0.9247\n",
      "Epoch: 196, Loss: 0.2214 Train_AUC: 0.9956, Train_AP: 0.9935 Val_AUC: 0.9197, Val_AP: 0.9173 Test_AUC: 0.9171, Test_AP: 0.9165\n",
      "Epoch: 197, Loss: 0.2201 Train_AUC: 0.9952, Train_AP: 0.9928 Val_AUC: 0.9218, Val_AP: 0.9190 Test_AUC: 0.9203, Test_AP: 0.9210\n",
      "Epoch: 198, Loss: 0.2200 Train_AUC: 0.9951, Train_AP: 0.9926 Val_AUC: 0.9234, Val_AP: 0.9222 Test_AUC: 0.9220, Test_AP: 0.9229\n",
      "Epoch: 199, Loss: 0.2204 Train_AUC: 0.9957, Train_AP: 0.9936 Val_AUC: 0.9212, Val_AP: 0.9191 Test_AUC: 0.9186, Test_AP: 0.9186\n",
      "Epoch: 200, Loss: 0.2193 Train_AUC: 0.9943, Train_AP: 0.9912 Val_AUC: 0.9225, Val_AP: 0.9185 Test_AUC: 0.9209, Test_AP: 0.9217\n",
      "Final Test AUC: 0.9234, AP: 0.9216\n"
     ]
    }
   ],
   "source": [
    "train_data = torch.load(f'./data/{config.dataset}/split/train_data.pt').to(device)\n",
    "val_data = torch.load(f'./data/{config.dataset}/split/val_data.pt').to(device)\n",
    "test_data = torch.load(f'./data/{config.dataset}/split/test_data.pt').to(device)\n",
    "\n",
    "model = ModelClass(config.data_init_num_features, hidden_dim = config.scoregnn.hidden_dim, \n",
    "                 output_dim = config.scoregnn.output_dim , num_layers = config.scoregnn.num_layers, \n",
    "                 dropout = config.scoregnn.dropout).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config.scoregnn.lr)\n",
    "\n",
    "final_model = final_predictor = None\n",
    "best_val_auc = final_test_auc = final_test_ap = 0\n",
    "\n",
    "model.reset_parameters()\n",
    "\n",
    "for epoch in range(1, 1 + config.scoregnn.epochs):\n",
    "    loss = train(model, train_data, optimizer, config.scoregnn.predictor)\n",
    "    train_auc, train_ap = test(model, train_data, config.scoregnn.predictor)\n",
    "    val_auc, val_ap = test(model, val_data, config.scoregnn.predictor)\n",
    "    test_auc, test_ap = test(model, test_data, config.scoregnn.predictor)\n",
    "    if val_auc > best_val_auc:\n",
    "        best_val_auc = val_auc\n",
    "        final_test_auc = test_auc\n",
    "        final_test_ap = test_ap\n",
    "        final_model = copy.deepcopy(model)\n",
    "        final_predictor = copy.deepcopy(config.scoregnn.predictor)\n",
    "\n",
    "    logging.info(f'Epoch: {epoch:03d}, Loss: {loss:.4f} '\n",
    "             f'Train_AUC: {train_auc:.4f}, Train_AP: {train_ap:.4f} '\n",
    "             f'Val_AUC: {val_auc:.4f}, Val_AP: {val_ap:.4f} '\n",
    "             f'Test_AUC: {test_auc:.4f}, Test_AP: {test_ap:.4f}')\n",
    "    \n",
    "logging.info(f'Final Test AUC: {final_test_auc:.4f}, AP: {final_test_ap:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85f543ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T21:13:12.926352Z",
     "iopub.status.busy": "2025-05-28T21:13:12.926352Z",
     "iopub.status.idle": "2025-05-28T21:13:12.937868Z",
     "shell.execute_reply": "2025-05-28T21:13:12.937868Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    'model': final_model.state_dict(),\n",
    "    'predictor': final_predictor.state_dict()\n",
    "}, './model/scoregnn.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
