{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89fb66b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T21:13:19.585891Z",
     "iopub.status.busy": "2025-05-28T21:13:19.585891Z",
     "iopub.status.idle": "2025-05-28T21:13:25.337655Z",
     "shell.execute_reply": "2025-05-28T21:13:25.337655Z"
    }
   },
   "outputs": [],
   "source": [
    "import os, time\n",
    "import numpy as np\n",
    "import gc, random\n",
    "import torch\n",
    "import networkx as nx\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "import psutil\n",
    "import out_manager as om\n",
    "import torch.nn.functional as F\n",
    "from itertools import chain\n",
    "from config import Config\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import k_hop_subgraph, to_scipy_sparse_matrix\n",
    "from model.score_gnn import scoregnn_dict\n",
    "from scipy.sparse.csgraph import shortest_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67f15e69",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T21:13:25.337655Z",
     "iopub.status.busy": "2025-05-28T21:13:25.337655Z",
     "iopub.status.idle": "2025-05-28T21:13:25.368688Z",
     "shell.execute_reply": "2025-05-28T21:13:25.368688Z"
    }
   },
   "outputs": [],
   "source": [
    "config = Config()\n",
    "ModelClass = scoregnn_dict[config.scoregnn.gnn_type]\n",
    "out_dir = om.get_existing_out_dir(config)\n",
    "om.setup_logging(os.path.join(out_dir, \"sample_log.txt\"))\n",
    "seed = config.seed\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "device = config.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f202e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubgraphBatchSampler:\n",
    "    def __init__(self, model, predictor, k_min, num_hops, save_dir, alpha = 40, beta = 20, gamma = 2, device = device):\n",
    "        self.model = model.eval()\n",
    "        self.predictor = predictor.eval()\n",
    "        self.k_min = k_min\n",
    "        self.num_hops = num_hops\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.save_dir = save_dir\n",
    "        self.device = device\n",
    "        super().__init__()\n",
    "\n",
    "        # æ³¨å†Œæ‰€æœ‰å¯é€‰æ‰“åˆ†å‡½æ•°\n",
    "        self.score_fn_dict = {\n",
    "            \"gnn\": self.get_subgraph_scores_gnn,\n",
    "            \"pagerank\": self.get_subgraph_scores_pagerank,\n",
    "            \"adamic-adar\": self.get_subgraph_scores_adamicadar,\n",
    "        }\n",
    "\n",
    "    # def get_max_z(self, data, edge_label_index, y, batch_size=1000):\n",
    "    #     random.seed(2025)\n",
    "    #     num_samples = edge_label_index.size(1)\n",
    "    #     # ä½¿ç”¨tqdmæ·»åŠ è¿›åº¦æ¡\n",
    "    #     for i in tqdm(range(0, num_samples, batch_size), desc=\"æ‰«æ max_z\", unit=\"batch\"):\n",
    "    #         batch_idx = edge_label_index[:, i:i+batch_size]\n",
    "    #         batch_data_list = self.sample_all_edges(data, batch_idx, y)\n",
    "    #         for batch_data in batch_data_list:\n",
    "    #             zmax = batch_data.z.max().item()\n",
    "    #             if zmax > self._max_z:\n",
    "    #                 print(f\"âš ï¸ æ›´æ–° _max_z: {self._max_z} -> {zmax} (batch {i//batch_size}, y={y})\")\n",
    "    #                 self._max_z = zmax\n",
    "    #         del batch_data_list\n",
    "    #         gc.collect()\n",
    "\n",
    "\n",
    "    def get_max_z(self, data, edge_label_index, y, batch_size=1000):\n",
    "        num_samples = edge_label_index.size(1)\n",
    "        # ä½¿ç”¨tqdmæ·»åŠ è¿›åº¦æ¡\n",
    "        for i in tqdm(range(0, num_samples, batch_size), desc=\"æ‰«æ max_z\", unit=\"batch\"):\n",
    "            batch_idx = edge_label_index[:, i:i+batch_size]\n",
    "            batch_data_list = self.sample_all_edges(data, batch_idx, y)\n",
    "            del batch_data_list\n",
    "            gc.collect()\n",
    "    \n",
    "    def save_batches(self, data, edge_label_index, y, out_prefix, max_z, batch_size=100):\n",
    "        random.seed(2025)\n",
    "        os.makedirs(os.path.dirname(out_prefix), exist_ok=True)\n",
    "        num_samples = edge_label_index.size(1)\n",
    "        idx = 0\n",
    "        # ä½¿ç”¨tqdmæ·»åŠ è¿›åº¦æ¡\n",
    "        for i in tqdm(range(0, num_samples, batch_size), desc=f\"ä¿å­˜ {out_prefix} åˆ†æ‰¹æ–‡ä»¶\", unit=\"batch\"):\n",
    "            batch_idx = edge_label_index[:, i:i+batch_size]\n",
    "            batch_data_list = self.sample_all_edges(data, batch_idx, y)\n",
    "            for batch_data in batch_data_list:\n",
    "                batch_data.x = F.one_hot(batch_data.z, max_z + 1).to(torch.float)\n",
    "                torch.save(batch_data_list, f\"{out_prefix}_batch{idx}.pt\")\n",
    "            del batch_data_list\n",
    "            gc.collect()\n",
    "            idx += 1\n",
    "\n",
    "    def merge_batches(self, batch_prefix, out_file):\n",
    "        batch_files = sorted(glob.glob(f\"{batch_prefix}_batch*.pt\"),\n",
    "                            key=lambda x: int(x.split('_batch')[-1].split('.pt')[0]))\n",
    "        all_data = []\n",
    "        for batch_file in batch_files:\n",
    "            data_list = torch.load(batch_file, map_location='cpu')  # ğŸ‘ˆ å¼ºåˆ¶æ”¾åˆ° CPU\n",
    "            all_data.extend(data_list)\n",
    "            print(f\"åˆå¹¶äº† {batch_file}ï¼Œå½“å‰æ€»é‡ï¼š{len(all_data)}\")\n",
    "            del data_list\n",
    "            gc.collect()\n",
    "            # CPU ä¸Šä¸ç”¨æ˜¾å­˜é‡Šæ”¾äº†\n",
    "        torch.save(all_data, out_file)\n",
    "        print(f\"ä¿å­˜åˆ° {out_file}ï¼Œæ€»è®¡ {len(all_data)} æ¡æ•°æ®\")\n",
    "        del all_data\n",
    "        gc.collect()\n",
    "\n",
    "    def merge_pos_neg(self, pos_file, neg_file, out_file):\n",
    "        pos_data = torch.load(pos_file, map_location='cpu')  # ğŸ‘ˆ æ”¾åœ¨ CPU\n",
    "        neg_data = torch.load(neg_file, map_location='cpu')  # ğŸ‘ˆ æ”¾åœ¨ CPU\n",
    "        all_data = pos_data + neg_data\n",
    "        torch.save(all_data, out_file)\n",
    "        print(f\"æœ€ç»ˆåˆå¹¶ {out_file}ï¼Œæ€»è®¡ {len(all_data)} æ¡ï¼ˆæ­£ä¾‹ {len(pos_data)}ï¼Œè´Ÿä¾‹ {len(neg_data)}ï¼‰\")\n",
    "        del pos_data, neg_data, all_data\n",
    "        gc.collect()\n",
    "\n",
    "    \n",
    "    def process(self):\n",
    "\n",
    "        seed = 2025\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "        train_data = torch.load(f'./data/{config.dataset}/split/train_data.pt')\n",
    "        val_data = torch.load(f'./data/{config.dataset}/split/val_data.pt')\n",
    "        test_data = torch.load(f'./data/{config.dataset}/split/test_data.pt')\n",
    "\n",
    "        train_data = train_data.to(self.device)\n",
    "        val_data = val_data.to(self.device)\n",
    "        test_data = test_data.to(self.device)\n",
    "\n",
    "        #ç¬¬ä¸€æ¬¡æ‰«æç»Ÿè®¡maxz\n",
    "        self._max_z = 0\n",
    "        self.get_max_z(train_data, train_data.pos_edge_label_index, 1),\n",
    "        print(self._max_z)\n",
    "        self.get_max_z(train_data, train_data.neg_edge_label_index, 0)\n",
    "        print(self._max_z)\n",
    "        self.get_max_z(val_data, val_data.pos_edge_label_index, 1),\n",
    "        print(self._max_z)\n",
    "        self.get_max_z(val_data, val_data.neg_edge_label_index, 0)\n",
    "        self.get_max_z(test_data, test_data.pos_edge_label_index, 1),\n",
    "        self.get_max_z(test_data, test_data.neg_edge_label_index, 0)\n",
    "        print(self._max_z)\n",
    "\n",
    "        # 2. åˆ†æ‰¹æ¬¡one-hotå’Œä¿å­˜ï¼Œç»ä¸æ±‡æ€»åˆ°å†…å­˜\n",
    "        save_dir = self.save_dir\n",
    "        train_pos_path = os.path.join(save_dir, \"SSSEAL_train_pos\")\n",
    "        train_neg_path = os.path.join(save_dir, \"SSSEAL_train_neg\")\n",
    "        val_pos_path = os.path.join(save_dir, \"SSSEAL_val_pos\")\n",
    "        val_neg_path = os.path.join(save_dir, \"SSSEAL_val_neg\")\n",
    "        test_pos_path = os.path.join(save_dir, \"SSSEAL_test_pos\")\n",
    "        test_neg_path = os.path.join(save_dir, \"SSSEAL_test_neg\")\n",
    "\n",
    "        print(\"ä¿å­˜ train åˆ†æ‰¹æ–‡ä»¶\")\n",
    "        self.save_batches(train_data, train_data.pos_edge_label_index, 1, train_pos_path, self._max_z)\n",
    "        self.save_batches(train_data, train_data.neg_edge_label_index, 0, train_neg_path, self._max_z)\n",
    "\n",
    "        print(\"ä¿å­˜ val åˆ†æ‰¹æ–‡ä»¶\")\n",
    "        self.save_batches(val_data, val_data.pos_edge_label_index, 1, val_pos_path, self._max_z)\n",
    "        self.save_batches(val_data, val_data.neg_edge_label_index, 0, val_neg_path, self._max_z)\n",
    "\n",
    "        print(\"ä¿å­˜ test åˆ†æ‰¹æ–‡ä»¶\")\n",
    "        self.save_batches(test_data, test_data.pos_edge_label_index, 1, test_pos_path, self._max_z)\n",
    "        self.save_batches(test_data, test_data.neg_edge_label_index, 0, test_neg_path, self._max_z)\n",
    "\n",
    "        print(\"æ‰€æœ‰åˆ†æ‰¹å¤„ç†å’Œä¿å­˜å·²å®Œæˆï¼ğŸš€\")\n",
    "\n",
    "        del train_data, val_data, test_data\n",
    "        gc.collect()\n",
    "    \n",
    "    def cancat_pos(self):\n",
    "        split_dir = self.save_dir\n",
    "        for prefix in [\"train\", \"val\", \"test\"]:\n",
    "            pos_prefix = os.path.join(split_dir, f\"SSSEAL_{prefix}_pos\")\n",
    "            pos_data_list = pos_prefix + \"_data_list.pt\"\n",
    "            merged_data_list = pos_data_list  # ç›´æ¥ç”¨ pos_data_list ä½œä¸ºç»“æœ\n",
    "\n",
    "            print(f\"\\n--- åˆå¹¶ {prefix} pos batch ---\")\n",
    "            self.merge_batches(pos_prefix, merged_data_list)\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    def cancat_neg(self):\n",
    "        split_dir = self.save_dir\n",
    "        for prefix in [\"train\", \"val\", \"test\"]:\n",
    "            neg_prefix = os.path.join(split_dir, f\"SSSEAL_{prefix}_neg\")\n",
    "            neg_data_list = neg_prefix + \"_data_list.pt\"\n",
    "            merged_data_list = neg_data_list  # ç›´æ¥ç”¨ neg_data_list ä½œä¸ºç»“æœ\n",
    "\n",
    "            print(f\"\\n--- åˆå¹¶ {prefix} neg batch ---\")\n",
    "            self.merge_batches(neg_prefix, merged_data_list)\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    def cancat_pos_neg(self):\n",
    "        split_dir = self.save_dir\n",
    "        for prefix in [\"train\", \"val\", \"test\"]:\n",
    "            pos_data_list = os.path.join(split_dir, f\"SSSEAL_{prefix}_pos_data_list.pt\")\n",
    "            neg_data_list = os.path.join(split_dir, f\"SSSEAL_{prefix}_neg_data_list.pt\")\n",
    "            merged_data_list = os.path.join(f\"./data/{config.dataset}/split/ssseal_{prefix}_data_k{self.k_min}_h{self.num_hops}_{config.version}.pt\")\n",
    "\n",
    "            print(f\"\\n--- åˆå¹¶ {prefix} pos+neg ä¸ºæ€» data_list ---\")\n",
    "            self.merge_pos_neg(pos_data_list, neg_data_list, merged_data_list)\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        print(\"æ‰€æœ‰ pos+neg åˆå¹¶å·²å®Œæˆï¼ğŸš€\")\n",
    "\n",
    "    def cancat(self):\n",
    "        split_dir = self.save_dir\n",
    "        for prefix in [\"train\", \"val\", \"test\"]:\n",
    "            pos_prefix = os.path.join(split_dir, f\"SSSEAL_{prefix}_pos\")\n",
    "            neg_prefix = os.path.join(split_dir, f\"SSSEAL_{prefix}_neg\")\n",
    "            pos_data_list = pos_prefix + \"_data_list.pt\"\n",
    "            neg_data_list = neg_prefix + \"_data_list.pt\"\n",
    "            merged_data_list = os.path.join(split_dir, f\"ssseal_{prefix}_data_k{self.k_min}_h{self.num_hops}_{config.version}.pt\")\n",
    "\n",
    "            print(f\"\\n--- åˆå¹¶ {prefix} pos batch ---\")\n",
    "            self.merge_batches(pos_prefix, pos_data_list)\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()  # ğŸ‘ˆ pos åˆå¹¶åæ¸…ç†æ˜¾å­˜\n",
    "\n",
    "            print(f\"--- åˆå¹¶ {prefix} neg batch ---\")\n",
    "            self.merge_batches(neg_prefix, neg_data_list)\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()  # ğŸ‘ˆ neg åˆå¹¶åæ¸…ç†æ˜¾å­˜\n",
    "\n",
    "            print(f\"--- åˆå¹¶ {prefix} pos+neg ä¸ºæ€» data_list ---\")\n",
    "            self.merge_pos_neg(pos_data_list, neg_data_list, merged_data_list)\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()  # ğŸ‘ˆ pos+neg åˆå¹¶å®Œå†æ¸…ç†\n",
    "\n",
    "        print(\"æ‰€æœ‰åˆ†æ‰¹å¤„ç†ã€åˆå¹¶å·²å®Œæˆï¼ğŸš€\")\n",
    "\n",
    "        # --------- è‡ªåŠ¨åˆ é™¤æ‰€æœ‰ batch æ–‡ä»¶ ----------\n",
    "        pattern = os.path.join(self.save_dir, \"SSSEAL_*_batch*.pt\")\n",
    "        batch_files = glob.glob(pattern)\n",
    "        for file in batch_files:\n",
    "            try:\n",
    "                os.remove(file)\n",
    "                print(f\"å·²åˆ é™¤ {file}\")\n",
    "            except Exception as e:\n",
    "                print(f\"åˆ é™¤ {file} å¤±è´¥ï¼š{e}\")\n",
    "        \n",
    "        target_files = [\n",
    "            \"SSSEAL_test_neg_data_list.pt\",\n",
    "            \"SSSEAL_test_pos_data_list.pt\",\n",
    "            \"SSSEAL_val_neg_data_list.pt\",\n",
    "            \"SSSEAL_val_pos_data_list.pt\",\n",
    "            \"SSSEAL_train_neg_data_list.pt\",\n",
    "            \"SSSEAL_train_pos_data_list.pt\"\n",
    "        ]\n",
    "\n",
    "        for filename in target_files:\n",
    "            file_path = os.path.join(self.save_dir, filename)\n",
    "            if os.path.exists(file_path):\n",
    "                try:\n",
    "                    os.remove(file_path)\n",
    "                    print(f\"å·²åˆ é™¤ {file_path}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"åˆ é™¤ {file_path} å¤±è´¥ï¼š{e}\")\n",
    "\n",
    "        print('æ‰€æœ‰æ•°æ®å·²ä¿å­˜å¹¶æ¸…ç†ä¸´æ—¶ batch æ–‡ä»¶')\n",
    "\n",
    "    \n",
    "    def sample_all_edges(self, data, edge_label_index, y):\n",
    "        data_list = []\n",
    "        for src, dst in edge_label_index.t().tolist():\n",
    "            data_list.append(self.sample_subgraph(src, dst, data, y))\n",
    "        return data_list\n",
    "    \n",
    "    def sample_all_edges_in_batches(self, data, edge_label_index, y, batch_size=256):\n",
    "        data_list = []\n",
    "        num_edges = edge_label_index.size(1)\n",
    "        for start in range(0, num_edges, batch_size):\n",
    "            end = min(start + batch_size, num_edges)\n",
    "            batch_edges = edge_label_index[:, start:end]\n",
    "            for src, dst in batch_edges.t().tolist():\n",
    "                subgraph = self.sample_subgraph(src, dst, data, y)\n",
    "                data_list.append(subgraph.cpu())\n",
    "                del subgraph\n",
    "            gc.collect() \n",
    "            torch.cuda.empty_cache()  # æ¸…ç†ç¼“å­˜\n",
    "        return data_list\n",
    "\n",
    "    \n",
    "    def sample_subgraph(self, src, dst, data, y):\n",
    "         \n",
    "        # # é‡‡k-hopå­å›¾ï¼Œå¾—åˆ°å­å›¾èŠ‚ç‚¹çš„æ–°ç¼–å·ã€å­å›¾å†…è¾¹ã€mapping\n",
    "        sub_node_index, sub_edge_index, mapping, _ = k_hop_subgraph(\n",
    "            [src, dst], self.num_hops, data.edge_index, relabel_nodes=True)\n",
    "        \n",
    "        if len(sub_node_index) <= self.k_min:\n",
    "            sub_node_index, sub_edge_index, mapping, _ = k_hop_subgraph(\n",
    "            [src, dst], self.num_hops + self.gamma, data.edge_index, relabel_nodes=True)\n",
    "\n",
    "            sub_x = data.x[sub_node_index]\n",
    "            sub_src, sub_dst = mapping.tolist()\n",
    "\n",
    "            final_nodes = list(range(len(sub_x)))\n",
    "\n",
    "        else:\n",
    "            #å­å›¾å…¨éƒ¨èŠ‚ç‚¹åˆå§‹ç‰¹å¾å‘é‡(sub.num_of_node, data.x.size(1))\n",
    "            # sub_x = data.x[sub_node_index].to(self.device)\n",
    "            sub_edge_index = sub_edge_index.to(self.device)\n",
    "            sub_x = data.x[sub_node_index]\n",
    "            sub_src, sub_dst = mapping.tolist()\n",
    "            \n",
    "            #æ„å»ºå­å›¾çš„data\n",
    "            sub_data = Data(x = sub_x,edge_index = sub_edge_index).to(self.device)\n",
    "            #è·å–å­å›¾çš„æ‰€æœ‰èŠ‚ç‚¹åˆ†æ•°å­—å…¸ï¼ˆä¸åŒ…å«srcå’Œdstï¼‰\n",
    "            # scores_dist, sub_node_emb = self.get_subgraph_scores(sub_src, sub_dst, sub_data)\n",
    "            \n",
    "            # ====== æ”¹è¿›ç‚¹ï¼šç”¨torch.topkæ›¿ä»£heapq.nlargestï¼Œé€Ÿåº¦æ›´å¿« =======\n",
    "\n",
    "            # # åˆ†æ•°ä»é«˜åˆ°ä½å–å‰top_k\n",
    "            # scores_dist = self.get_subgraph_scores(sub_src, sub_dst, sub_data)\n",
    "            # topk_neighbors = heapq.nlargest(self.k_min, scores_dist, key=scores_dist.get)\n",
    "\n",
    "            candidates_tensor, scores = self.get_subgraph_scores(sub_src, sub_dst, sub_data)\n",
    "\n",
    "            k = max(1, int(len(scores) * self.alpha // 100))\n",
    "            _, topk_indices = torch.topk(scores, min(k, scores.size(0)))\n",
    "            topk_neighbors = candidates_tensor[topk_indices].tolist()\n",
    "            # ===========================================================\n",
    "            # ä»å‰©ä¸‹çš„å€™é€‰èŠ‚ç‚¹ä¸­éšæœºé€‰æ‹©20%çš„èŠ‚ç‚¹\n",
    "            remaining_candidates = [i for i in candidates_tensor.tolist() if i not in topk_neighbors]\n",
    "            num_random_select = min(int(len(scores) * self.beta // 100), len(remaining_candidates))\n",
    "            random_neighbors = random.sample(remaining_candidates, num_random_select)\n",
    "\n",
    "            # åˆå¹¶å‰40%å’Œéšæœºé€‰æ‹©çš„èŠ‚ç‚¹ï¼Œå¾—åˆ°æœ€ç»ˆçš„topk_neighbors\n",
    "            final_neighbors = topk_neighbors + random_neighbors\n",
    "            # æºç‚¹å’Œç›®æ ‡ç‚¹åœ¨å­å›¾çš„ç¼–å·\n",
    "            final_nodes = [sub_src, sub_dst] + final_neighbors\n",
    "        \n",
    "        final_nodes = list(set(final_nodes))  # é˜²æ­¢é‡å¤\n",
    "        final_nodes.sort()  # æ–¹ä¾¿åé¢é‡æ–°æ˜ å°„\n",
    "\n",
    "        # æ—§ç¼–å·åˆ°æ–°ç¼–å·çš„æ˜ å°„\n",
    "        node_id_map = {old: new for new, old in enumerate(final_nodes)}\n",
    "\n",
    "        # æ–°çš„x\n",
    "        final_x = sub_x[final_nodes]\n",
    "\n",
    "        # maskè¾¹ï¼šåªä¿ç•™ä¸¤ä¸ªç«¯ç‚¹éƒ½åœ¨final_nodeså†…çš„è¾¹\n",
    "        final_nodes_tensor = torch.tensor(final_nodes, device=self.device)\n",
    "        mask = torch.isin(sub_edge_index[0], final_nodes_tensor) & \\\n",
    "            torch.isin(sub_edge_index[1], final_nodes_tensor)\n",
    "        final_edge_index = sub_edge_index[:, mask]\n",
    "\n",
    "        # é‡æ–°ç¼–å·edge_index\n",
    "        final_edge_index = torch.stack([\n",
    "            torch.tensor([node_id_map[int(i)] for i in final_edge_index[0].tolist()], device=self.device),\n",
    "            torch.tensor([node_id_map[int(i)] for i in final_edge_index[1].tolist()], device=self.device)\n",
    "        ], dim=0)\n",
    "\n",
    "        #å»é™¤ src-dst ä¹‹é—´çš„è¾¹ï¼ˆæ— å‘å›¾è®°å¾—ä¸¤ä¸ªæ–¹å‘éƒ½åˆ ï¼ï¼‰\n",
    "        src_new = node_id_map[sub_src]\n",
    "        dst_new = node_id_map[sub_dst]\n",
    "        mask1 = (final_edge_index[0] != src_new) | (final_edge_index[1] != dst_new)\n",
    "        mask2 = (final_edge_index[0] != dst_new) | (final_edge_index[1] != src_new)\n",
    "        mask = mask1 & mask2\n",
    "        final_edge_index = final_edge_index[:, mask]\n",
    "\n",
    "        z = self.drnl_node_labeling(final_edge_index, src_new, dst_new, num_nodes = len(final_nodes))\n",
    "\n",
    "        final_sub_data = Data(x = final_x, z = z, edge_index = final_edge_index, y = y)\n",
    "        final_sub_data = final_sub_data.to(next(self.model.parameters()).device)\n",
    "        return final_sub_data\n",
    "    \n",
    "    def get_subgraph_scores(self, src, dst, data):\n",
    "        fn = self.score_fn_dict.get(config.scoresampler.score_fn, self.get_subgraph_scores_gnn)  # é»˜è®¤GNN\n",
    "        return fn(src, dst, data)\n",
    "    \n",
    "    def get_subgraph_scores_gnn(self, src, dst, data):\n",
    "        with torch.no_grad():\n",
    "            device = next(self.model.parameters()).device\n",
    "            data = data.to(device)\n",
    "            node_emb =self.model(data.x, data.edge_index)\n",
    "\n",
    "            #æ„å»ºæ‰€æœ‰srcå’Œdståˆ†åˆ«åˆ°å­å›¾æ‰€æœ‰èŠ‚ç‚¹çš„ç»„åˆ(ä¸åŒ…å«äº’ç›¸)\n",
    "            candidates = [i for i in range(data.num_nodes) if i != src and i != dst]\n",
    "            candidates_tensor = torch.tensor(candidates, device=self.device, dtype=torch.long)\n",
    "\n",
    "            src_1 = torch.tensor([src] * len(candidates), dtype=torch.long, device=self.device)\n",
    "            dst_1 = torch.tensor(candidates, dtype=torch.long, device=self.device)\n",
    "            src_2 = torch.tensor([dst] * len(candidates), dtype=torch.long, device=self.device)\n",
    "            dst_2 = torch.tensor(candidates, dtype=torch.long, device=self.device)\n",
    "            \n",
    "            edge_label_index_1 = torch.stack([src_1, dst_1], dim=0)\n",
    "            edge_label_index_2 = torch.stack([src_2, dst_2], dim=0)\n",
    "\n",
    "            scores_1 = self.predictor(node_emb, edge_label_index_1)\n",
    "            scores_2 = self.predictor(node_emb, edge_label_index_2)\n",
    "            scores = (scores_1 + scores_2) / 2\n",
    "            # scores_dist = {i: float(score) for i, score in zip(candidates, scores)}\n",
    "        return candidates_tensor, scores\n",
    "    \n",
    "    def get_subgraph_scores_adamicadar(self, src, dst, data):\n",
    "        edge_index = data.edge_index.cpu().numpy()\n",
    "        G = nx.Graph()\n",
    "        G.add_edges_from(edge_index.T.tolist())\n",
    "        G.add_nodes_from(range(data.num_nodes))  # ä¿è¯èŠ‚ç‚¹éƒ½åœ¨\n",
    "\n",
    "        # åªè€ƒè™‘æœ‰è¾¹çš„èŠ‚ç‚¹ä½œä¸ºå€™é€‰\n",
    "        candidates = [i for i in range(data.num_nodes) if i != src and i != dst and G.degree(i) > 0]\n",
    "        candidates_tensor = torch.tensor(candidates,device=self.device, dtype=torch.long)\n",
    "\n",
    "        # å¦‚æœsrcæˆ–dstæœ¬èº«ä¹Ÿæ˜¯å­¤ç«‹èŠ‚ç‚¹ï¼Œä¹Ÿè·³è¿‡/ç›´æ¥è¿”å›ç©º\n",
    "        if G.degree(src) == 0 or G.degree(dst) == 0:\n",
    "            return candidates_tensor, torch.zeros_like(candidates_tensor, dtype=torch.float)\n",
    "\n",
    "        aa_src = {(u, v): s for u, v, s in nx.adamic_adar_index(G, [(src, i) for i in candidates])}\n",
    "        aa_dst = {(u, v): s for u, v, s in nx.adamic_adar_index(G, [(dst, i) for i in candidates])}\n",
    "\n",
    "        scores = []\n",
    "        for i in candidates:\n",
    "            s1 = aa_src.get((src, i), 0.0)\n",
    "            s2 = aa_dst.get((dst, i), 0.0)\n",
    "            s = (s1 + s2) / 2\n",
    "            scores.append(s)\n",
    "\n",
    "        scores = torch.tensor(scores, device=self.device, dtype=torch.float)\n",
    "        return candidates_tensor, scores\n",
    "        \n",
    "    def get_subgraph_scores_pagerank(self, src, dst, data):\n",
    "        # 1. edge_indexè½¬æˆnetworkxå›¾ï¼ŒèŠ‚ç‚¹ç¼–å·æ˜¯å±€éƒ¨ç¼–å·\n",
    "        edge_index = data.edge_index.cpu().numpy()\n",
    "        G = nx.Graph()\n",
    "        G.add_edges_from(edge_index.T.tolist())\n",
    "        G.add_nodes_from(range(data.num_nodes))  # ç¡®ä¿æ‰€æœ‰èŠ‚ç‚¹éƒ½åœ¨Gä¸­\n",
    "\n",
    "        # 2. åªè€ƒè™‘æœ‰è¾¹çš„èŠ‚ç‚¹\n",
    "        candidates = [i for i in range(data.num_nodes) if i != src and i != dst and G.degree(i) > 0]\n",
    "        candidates_tensor = torch.tensor(candidates, device=self.device, dtype=torch.long)\n",
    "\n",
    "        # å¦‚æœsrcæˆ–dstæœ¬èº«æ˜¯å­¤ç«‹èŠ‚ç‚¹ï¼Œç›´æ¥è¿”å›é›¶åˆ†\n",
    "        if G.degree(src) == 0 or G.degree(dst) == 0:\n",
    "            return candidates_tensor, torch.zeros(len(candidates), device=self.device, dtype=torch.float)\n",
    "\n",
    "        # 3. Personalized PageRankï¼ˆä»¥srcå’Œdstä¸ºä¸ªæ€§åŒ–èµ·ç‚¹ï¼Œå„ç®—ä¸€æ¬¡ï¼‰\n",
    "        personalization_src = {n: 0 for n in G.nodes}\n",
    "        personalization_src[src] = 1\n",
    "        pr_src = nx.pagerank(G, personalization=personalization_src)\n",
    "\n",
    "        personalization_dst = {n: 0 for n in G.nodes}\n",
    "        personalization_dst[dst] = 1\n",
    "        pr_dst = nx.pagerank(G, personalization=personalization_dst)\n",
    "\n",
    "        # 4. å¯¹æ¯ä¸ªå€™é€‰èŠ‚ç‚¹ï¼Œåˆ†åˆ«æŸ¥srcå’Œdstä¸ªæ€§åŒ–pagerankçš„åˆ†æ•°ï¼Œåšå¹³å‡\n",
    "        scores = []\n",
    "        for i in candidates:\n",
    "            s = (pr_src.get(i, 0.0) + pr_dst.get(i, 0.0)) / 2\n",
    "            scores.append(s)\n",
    "\n",
    "        # è½¬æˆtorchå¼ é‡\n",
    "        scores = torch.tensor(scores, device=self.device, dtype=torch.float)\n",
    "        return candidates_tensor, scores\n",
    "\n",
    "    def drnl_node_labeling(self, edge_index, src, dst, num_nodes=None):\n",
    "        # Double-radius node labeling (DRNL).\n",
    "        src, dst = (dst, src) if src > dst else (src, dst)\n",
    "        adj = to_scipy_sparse_matrix(edge_index, num_nodes=num_nodes).tocsr()\n",
    "\n",
    "        idx = list(range(src)) + list(range(src + 1, adj.shape[0]))\n",
    "        adj_wo_src = adj[idx, :][:, idx]\n",
    "\n",
    "        idx = list(range(dst)) + list(range(dst + 1, adj.shape[0]))\n",
    "        adj_wo_dst = adj[idx, :][:, idx]\n",
    "\n",
    "        dist2src = shortest_path(adj_wo_dst, directed=False, unweighted=True,\n",
    "                                 indices=src)\n",
    "        dist2src = np.insert(dist2src, dst, 0, axis=0)\n",
    "        dist2src = torch.from_numpy(dist2src)\n",
    "\n",
    "        dist2dst = shortest_path(adj_wo_src, directed=False, unweighted=True,\n",
    "                                 indices=dst - 1)\n",
    "        dist2dst = np.insert(dist2dst, src, 0, axis=0)\n",
    "        dist2dst = torch.from_numpy(dist2dst)\n",
    "\n",
    "        dist = dist2src + dist2dst\n",
    "        dist_over_2, dist_mod_2 = dist // 2, dist % 2\n",
    "\n",
    "        z = 1 + torch.min(dist2src, dist2dst)\n",
    "        z += dist_over_2 * (dist_over_2 + dist_mod_2 - 1)\n",
    "        z[src] = 1.\n",
    "        z[dst] = 1.\n",
    "        z[torch.isnan(z)] = 0.\n",
    "\n",
    "        self._max_z = max(int(z.max()), self._max_z)\n",
    "\n",
    "        return z.to(torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15c8cff2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T21:13:25.368688Z",
     "iopub.status.busy": "2025-05-28T21:13:25.368688Z",
     "iopub.status.idle": "2025-05-28T21:13:25.397099Z",
     "shell.execute_reply": "2025-05-28T21:13:25.397099Z"
    }
   },
   "outputs": [],
   "source": [
    "class SubgraphSampler:\n",
    "    def __init__(self, model, predictor, k_min, num_hops, alpha = 40, beta = 20, gamma = 2, device = device):\n",
    "        self.model = model.eval()\n",
    "        self.predictor = predictor.eval()\n",
    "        self.k_min = k_min\n",
    "        self.num_hops = num_hops\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.device = device\n",
    "\n",
    "        # æ³¨å†Œæ‰€æœ‰å¯é€‰æ‰“åˆ†å‡½æ•°\n",
    "        self.score_fn_dict = {\n",
    "            \"gnn\": self.get_subgraph_scores_gnn,\n",
    "            \"pagerank\": self.get_subgraph_scores_pagerank,\n",
    "            \"adamic-adar\": self.get_subgraph_scores_adamicadar,\n",
    "        }\n",
    "\n",
    "    \n",
    "    def process(self):\n",
    "        train_data = torch.load(f'./data/{config.dataset}/split/train_data.pt')\n",
    "        val_data = torch.load(f'./data/{config.dataset}/split/val_data.pt')\n",
    "        test_data = torch.load(f'./data/{config.dataset}/split/test_data.pt')\n",
    "\n",
    "        train_data = train_data.to(self.device)\n",
    "        val_data = val_data.to(self.device)\n",
    "        test_data = test_data.to(self.device)\n",
    "\n",
    "        self._max_z = 0\n",
    "\n",
    "        train_pos_data_list = self.sample_all_edges_in_batches(\n",
    "        train_data, train_data.pos_edge_label_index, 1)\n",
    "        train_neg_data_list = self.sample_all_edges_in_batches(\n",
    "        train_data, train_data.neg_edge_label_index, 0)\n",
    "\n",
    "        val_pos_data_list = self.sample_all_edges_in_batches(\n",
    "        val_data, val_data.pos_edge_label_index, 1)\n",
    "        val_neg_data_list = self.sample_all_edges_in_batches(\n",
    "        val_data, val_data.neg_edge_label_index, 0)\n",
    "\n",
    "        test_pos_data_list = self.sample_all_edges_in_batches(\n",
    "        test_data, test_data.pos_edge_label_index, 1)\n",
    "        test_neg_data_list = self.sample_all_edges_in_batches(\n",
    "        test_data, test_data.neg_edge_label_index, 0)\n",
    "\n",
    "        for data in chain(train_pos_data_list, train_neg_data_list,\n",
    "                          val_pos_data_list, val_neg_data_list,\n",
    "                          test_pos_data_list, test_neg_data_list):\n",
    "            # We solely learn links from structure, dropping any node features:\n",
    "            data.x = F.one_hot(data.z, self._max_z + 1).to(torch.float)\n",
    "\n",
    "        train_data_list = train_pos_data_list + train_neg_data_list\n",
    "        val_data_list = val_pos_data_list + val_neg_data_list\n",
    "        test_data_list = test_pos_data_list + test_neg_data_list\n",
    "\n",
    "        torch.save(train_data_list, f'./data/{config.dataset}/split/ssseal_train_data_k{self.k_min}_h{self.num_hops}_{config.version}.pt')\n",
    "        torch.save(val_data_list, f'./data/{config.dataset}/split/ssseal_val_data_k{self.k_min}_h{self.num_hops}_{config.version}.pt')\n",
    "        torch.save(test_data_list, f'./data/{config.dataset}/split/ssseal_test_data_k{self.k_min}_h{self.num_hops}_{config.version}.pt')\n",
    "        print(\"All processed data have been saved.\")\n",
    "    \n",
    "    def sample_all_edges(self, data, edge_label_index, y):\n",
    "        data_list = []\n",
    "        for src, dst in edge_label_index.t().tolist():\n",
    "            data_list.append(self.sample_subgraph(src, dst, data, y))\n",
    "        return data_list\n",
    "    \n",
    "    def sample_all_edges_in_batches(self, data, edge_label_index, y, batch_size=256):\n",
    "        data_list = []\n",
    "        num_edges = edge_label_index.size(1)\n",
    "        for start in range(0, num_edges, batch_size):\n",
    "            end = min(start + batch_size, num_edges)\n",
    "            batch_edges = edge_label_index[:, start:end]\n",
    "            for src, dst in batch_edges.t().tolist():\n",
    "                subgraph = self.sample_subgraph(src, dst, data, y)\n",
    "                data_list.append(subgraph.cpu())\n",
    "                del subgraph\n",
    "            gc.collect() \n",
    "            torch.cuda.empty_cache()  # æ¸…ç†ç¼“å­˜\n",
    "        return data_list\n",
    "\n",
    "    \n",
    "    def sample_subgraph(self, src, dst, data, y):\n",
    "        \n",
    "        # # é‡‡k-hopå­å›¾ï¼Œå¾—åˆ°å­å›¾èŠ‚ç‚¹çš„æ–°ç¼–å·ã€å­å›¾å†…è¾¹ã€mapping\n",
    "        sub_node_index, sub_edge_index, mapping, _ = k_hop_subgraph(\n",
    "            [src, dst], self.num_hops, data.edge_index, relabel_nodes=True)\n",
    "        \n",
    "        if len(sub_node_index) <= self.k_min:\n",
    "            sub_node_index, sub_edge_index, mapping, _ = k_hop_subgraph(\n",
    "            [src, dst], self.num_hops + self.gamma, data.edge_index, relabel_nodes=True)\n",
    "\n",
    "            sub_x = data.x[sub_node_index]\n",
    "            sub_src, sub_dst = mapping.tolist()\n",
    "\n",
    "            final_nodes = list(range(len(sub_x)))\n",
    "\n",
    "        else:\n",
    "            sub_edge_index = sub_edge_index.to(self.device)\n",
    "            #å­å›¾å…¨éƒ¨èŠ‚ç‚¹åˆå§‹ç‰¹å¾å‘é‡(sub.num_of_node, data.x.size(1))\n",
    "            # sub_x = data.x[sub_node_index].to(self.device)\n",
    "            sub_x = data.x[sub_node_index]\n",
    "            sub_src, sub_dst = mapping.tolist()\n",
    "            \n",
    "            #æ„å»ºå­å›¾çš„data\n",
    "            sub_data = Data(x = sub_x,edge_index = sub_edge_index).to(self.device)\n",
    "            #è·å–å­å›¾çš„æ‰€æœ‰èŠ‚ç‚¹åˆ†æ•°å­—å…¸ï¼ˆä¸åŒ…å«srcå’Œdstï¼‰\n",
    "            # scores_dist, sub_node_emb = self.get_subgraph_scores(sub_src, sub_dst, sub_data)\n",
    "            \n",
    "            # ====== æ”¹è¿›ç‚¹ï¼šç”¨torch.topkæ›¿ä»£heapq.nlargestï¼Œé€Ÿåº¦æ›´å¿« =======\n",
    "\n",
    "            # # åˆ†æ•°ä»é«˜åˆ°ä½å–å‰top_k\n",
    "            # scores_dist = self.get_subgraph_scores(sub_src, sub_dst, sub_data)\n",
    "            # topk_neighbors = heapq.nlargest(self.k_min, scores_dist, key=scores_dist.get)\n",
    "\n",
    "            candidates_tensor, scores = self.get_subgraph_scores(sub_src, sub_dst, sub_data)\n",
    "\n",
    "            k = max(1, int(len(scores) * self.alpha // 100))\n",
    "            _, topk_indices = torch.topk(scores, min(k, scores.size(0)))\n",
    "            topk_neighbors = candidates_tensor[topk_indices].tolist()\n",
    "            # ===========================================================\n",
    "            # ä»å‰©ä¸‹çš„å€™é€‰èŠ‚ç‚¹ä¸­éšæœºé€‰æ‹©20%çš„èŠ‚ç‚¹\n",
    "            remaining_candidates = [i for i in candidates_tensor.tolist() if i not in topk_neighbors]\n",
    "            num_random_select = min(int(len(scores) * self.beta // 100), len(remaining_candidates))\n",
    "            random_neighbors = random.sample(remaining_candidates, num_random_select)\n",
    "\n",
    "            # åˆå¹¶å‰40%å’Œéšæœºé€‰æ‹©çš„èŠ‚ç‚¹ï¼Œå¾—åˆ°æœ€ç»ˆçš„topk_neighbors\n",
    "            final_neighbors = topk_neighbors + random_neighbors\n",
    "            # æºç‚¹å’Œç›®æ ‡ç‚¹åœ¨å­å›¾çš„ç¼–å·\n",
    "            final_nodes = [sub_src, sub_dst] + final_neighbors\n",
    "        \n",
    "        final_nodes = list(set(final_nodes))  # é˜²æ­¢é‡å¤\n",
    "        final_nodes.sort()  # æ–¹ä¾¿åé¢é‡æ–°æ˜ å°„\n",
    "\n",
    "        # æ—§ç¼–å·åˆ°æ–°ç¼–å·çš„æ˜ å°„\n",
    "        node_id_map = {old: new for new, old in enumerate(final_nodes)}\n",
    "\n",
    "        # æ–°çš„x\n",
    "        final_x = sub_x[final_nodes]\n",
    "\n",
    "        # maskè¾¹ï¼šåªä¿ç•™ä¸¤ä¸ªç«¯ç‚¹éƒ½åœ¨final_nodeså†…çš„è¾¹\n",
    "        final_nodes_tensor = torch.tensor(final_nodes, device=self.device)\n",
    "        mask = torch.isin(sub_edge_index[0], final_nodes_tensor) & \\\n",
    "            torch.isin(sub_edge_index[1], final_nodes_tensor)\n",
    "        final_edge_index = sub_edge_index[:, mask]\n",
    "\n",
    "        # é‡æ–°ç¼–å·edge_index\n",
    "        final_edge_index = torch.stack([\n",
    "            torch.tensor([node_id_map[int(i)] for i in final_edge_index[0].tolist()], device=self.device),\n",
    "            torch.tensor([node_id_map[int(i)] for i in final_edge_index[1].tolist()], device=self.device)\n",
    "        ], dim=0)\n",
    "\n",
    "        #å»é™¤ src-dst ä¹‹é—´çš„è¾¹ï¼ˆæ— å‘å›¾è®°å¾—ä¸¤ä¸ªæ–¹å‘éƒ½åˆ ï¼ï¼‰\n",
    "        src_new = node_id_map[sub_src]\n",
    "        dst_new = node_id_map[sub_dst]\n",
    "        mask1 = (final_edge_index[0] != src_new) | (final_edge_index[1] != dst_new)\n",
    "        mask2 = (final_edge_index[0] != dst_new) | (final_edge_index[1] != src_new)\n",
    "        mask = mask1 & mask2\n",
    "        final_edge_index = final_edge_index[:, mask]\n",
    "\n",
    "        z = self.drnl_node_labeling(final_edge_index, src_new, dst_new, num_nodes = len(final_nodes))\n",
    "\n",
    "        final_sub_data = Data(x = final_x, z = z, edge_index = final_edge_index, y = y)\n",
    "        return final_sub_data\n",
    "    \n",
    "    def get_subgraph_scores(self, src, dst, data):\n",
    "        fn = self.score_fn_dict.get(config.scoresampler.score_fn, self.get_subgraph_scores_gnn)  # é»˜è®¤GNN\n",
    "        return fn(src, dst, data)\n",
    "    \n",
    "    def get_subgraph_scores_gnn(self, src, dst, data):\n",
    "        with torch.no_grad():\n",
    "            node_emb =self.model(data.x, data.edge_index)\n",
    "\n",
    "            #æ„å»ºæ‰€æœ‰srcå’Œdståˆ†åˆ«åˆ°å­å›¾æ‰€æœ‰èŠ‚ç‚¹çš„ç»„åˆ(ä¸åŒ…å«äº’ç›¸)\n",
    "            candidates = [i for i in range(data.num_nodes) if i != src and i != dst]\n",
    "            candidates_tensor = torch.tensor(candidates, device=self.device, dtype=torch.long)\n",
    "\n",
    "            src_1 = torch.tensor([src] * len(candidates), dtype=torch.long, device=self.device)\n",
    "            dst_1 = torch.tensor(candidates, dtype=torch.long, device=self.device)\n",
    "            src_2 = torch.tensor([dst] * len(candidates), dtype=torch.long, device=self.device)\n",
    "            dst_2 = torch.tensor(candidates, dtype=torch.long, device=self.device)\n",
    "            \n",
    "            edge_label_index_1 = torch.stack([src_1, dst_1], dim=0)\n",
    "            edge_label_index_2 = torch.stack([src_2, dst_2], dim=0)\n",
    "\n",
    "            scores_1 = self.predictor(node_emb, edge_label_index_1)\n",
    "            scores_2 = self.predictor(node_emb, edge_label_index_2)\n",
    "            scores = (scores_1 + scores_2) / 2\n",
    "            # scores_dist = {i: float(score) for i, score in zip(candidates, scores)}\n",
    "        return candidates_tensor, scores\n",
    "    \n",
    "    def get_subgraph_scores_adamicadar(self, src, dst, data):\n",
    "        edge_index = data.edge_index.cpu().numpy()\n",
    "        G = nx.Graph()\n",
    "        G.add_edges_from(edge_index.T.tolist())\n",
    "        G.add_nodes_from(range(data.num_nodes))  # ä¿è¯èŠ‚ç‚¹éƒ½åœ¨\n",
    "\n",
    "        # åªè€ƒè™‘æœ‰è¾¹çš„èŠ‚ç‚¹ä½œä¸ºå€™é€‰\n",
    "        candidates = [i for i in range(data.num_nodes) if i != src and i != dst and G.degree(i) > 0]\n",
    "        candidates_tensor = torch.tensor(candidates, device=self.device, dtype=torch.long)\n",
    "\n",
    "        # å¦‚æœsrcæˆ–dstæœ¬èº«ä¹Ÿæ˜¯å­¤ç«‹èŠ‚ç‚¹ï¼Œä¹Ÿè·³è¿‡/ç›´æ¥è¿”å›ç©º\n",
    "        if G.degree(src) == 0 or G.degree(dst) == 0:\n",
    "            return candidates_tensor, torch.zeros_like(candidates_tensor, dtype=torch.float, device=self.device)\n",
    "\n",
    "        aa_src = {(u, v): s for u, v, s in nx.adamic_adar_index(G, [(src, i) for i in candidates])}\n",
    "        aa_dst = {(u, v): s for u, v, s in nx.adamic_adar_index(G, [(dst, i) for i in candidates])}\n",
    "\n",
    "        scores = []\n",
    "        for i in candidates:\n",
    "            s1 = aa_src.get((src, i), 0.0)\n",
    "            s2 = aa_dst.get((dst, i), 0.0)\n",
    "            s = (s1 + s2) / 2\n",
    "            scores.append(s)\n",
    "\n",
    "        scores = torch.tensor(scores, device=self.device, dtype=torch.float)\n",
    "        return candidates_tensor, scores\n",
    "        \n",
    "    def get_subgraph_scores_pagerank(self, src, dst, data):\n",
    "        # 1. edge_indexè½¬æˆnetworkxå›¾ï¼ŒèŠ‚ç‚¹ç¼–å·æ˜¯å±€éƒ¨ç¼–å·\n",
    "        edge_index = data.edge_index.cpu().numpy()\n",
    "        G = nx.Graph()\n",
    "        G.add_edges_from(edge_index.T.tolist())\n",
    "        G.add_nodes_from(range(data.num_nodes))  # ç¡®ä¿æ‰€æœ‰èŠ‚ç‚¹éƒ½åœ¨Gä¸­\n",
    "\n",
    "        # 2. åªè€ƒè™‘æœ‰è¾¹çš„èŠ‚ç‚¹\n",
    "        candidates = [i for i in range(data.num_nodes) if i != src and i != dst and G.degree(i) > 0]\n",
    "        candidates_tensor = torch.tensor(candidates, device=self.device, dtype=torch.long)\n",
    "\n",
    "        # å¦‚æœsrcæˆ–dstæœ¬èº«æ˜¯å­¤ç«‹èŠ‚ç‚¹ï¼Œç›´æ¥è¿”å›é›¶åˆ†\n",
    "        if G.degree(src) == 0 or G.degree(dst) == 0:\n",
    "            return candidates_tensor, torch.zeros(len(candidates), device=self.device, dtype=torch.float)\n",
    "\n",
    "        # 3. Personalized PageRankï¼ˆä»¥srcå’Œdstä¸ºä¸ªæ€§åŒ–èµ·ç‚¹ï¼Œå„ç®—ä¸€æ¬¡ï¼‰\n",
    "        personalization_src = {n: 0 for n in G.nodes}\n",
    "        personalization_src[src] = 1\n",
    "        pr_src = nx.pagerank(G, personalization=personalization_src)\n",
    "\n",
    "        personalization_dst = {n: 0 for n in G.nodes}\n",
    "        personalization_dst[dst] = 1\n",
    "        pr_dst = nx.pagerank(G, personalization=personalization_dst)\n",
    "\n",
    "        # 4. å¯¹æ¯ä¸ªå€™é€‰èŠ‚ç‚¹ï¼Œåˆ†åˆ«æŸ¥srcå’Œdstä¸ªæ€§åŒ–pagerankçš„åˆ†æ•°ï¼Œåšå¹³å‡\n",
    "        scores = []\n",
    "        for i in candidates:\n",
    "            s = (pr_src.get(i, 0.0) + pr_dst.get(i, 0.0)) / 2\n",
    "            scores.append(s)\n",
    "\n",
    "        # è½¬æˆtorchå¼ é‡\n",
    "        scores = torch.tensor(scores, device=self.device, dtype=torch.float)\n",
    "        return candidates_tensor, scores\n",
    "\n",
    "    def drnl_node_labeling(self, edge_index, src, dst, num_nodes=None):\n",
    "        # Double-radius node labeling (DRNL).\n",
    "        src, dst = (dst, src) if src > dst else (src, dst)\n",
    "        adj = to_scipy_sparse_matrix(edge_index, num_nodes=num_nodes).tocsr()\n",
    "\n",
    "        idx = list(range(src)) + list(range(src + 1, adj.shape[0]))\n",
    "        adj_wo_src = adj[idx, :][:, idx]\n",
    "\n",
    "        idx = list(range(dst)) + list(range(dst + 1, adj.shape[0]))\n",
    "        adj_wo_dst = adj[idx, :][:, idx]\n",
    "\n",
    "        dist2src = shortest_path(adj_wo_dst, directed=False, unweighted=True,\n",
    "                                 indices=src)\n",
    "        dist2src = np.insert(dist2src, dst, 0, axis=0)\n",
    "        dist2src = torch.from_numpy(dist2src)\n",
    "\n",
    "        dist2dst = shortest_path(adj_wo_src, directed=False, unweighted=True,\n",
    "                                 indices=dst - 1)\n",
    "        dist2dst = np.insert(dist2dst, src, 0, axis=0)\n",
    "        dist2dst = torch.from_numpy(dist2dst)\n",
    "\n",
    "        dist = dist2src + dist2dst\n",
    "        dist_over_2, dist_mod_2 = dist // 2, dist % 2\n",
    "\n",
    "        z = 1 + torch.min(dist2src, dist2dst)\n",
    "        z += dist_over_2 * (dist_over_2 + dist_mod_2 - 1)\n",
    "        z[src] = 1.\n",
    "        z[dst] = 1.\n",
    "        z[torch.isnan(z)] = 0.\n",
    "\n",
    "        self._max_z = max(int(z.max()), self._max_z)\n",
    "\n",
    "        return z.to(torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae430094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and predictor loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "model = ModelClass(config.data_init_num_features, hidden_dim = config.scoregnn.hidden_dim, \n",
    "                 output_dim = config.scoregnn.output_dim , num_layers = config.scoregnn.num_layers, \n",
    "                 dropout = config.scoregnn.dropout).to(device)\n",
    "predictor = config.scoregnn.predictor.to(device)\n",
    "\n",
    "# åŠ è½½å‚æ•°ï¼ˆå‡è®¾ä½ çš„æ–‡ä»¶ç»“æ„æ˜¯è¿™æ ·ä¿å­˜çš„ï¼‰\n",
    "checkpoint = torch.load('./model/scoregnn.pth', map_location=device)\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "predictor.load_state_dict(checkpoint['predictor'])\n",
    "\n",
    "model.eval()\n",
    "predictor.eval()\n",
    "print(\"Model and predictor loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45f70e5c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T21:13:25.397099Z",
     "iopub.status.busy": "2025-05-28T21:13:25.397099Z",
     "iopub.status.idle": "2025-05-28T21:13:25.483677Z",
     "shell.execute_reply": "2025-05-28T21:13:25.483677Z"
    }
   },
   "outputs": [],
   "source": [
    "# model = ModelClass(config.data_init_num_features, hidden_dim = config.scoregnn.hidden_dim, \n",
    "#                  output_dim = config.scoregnn.output_dim , num_layers = config.scoregnn.num_layers, \n",
    "#                  dropout = config.scoregnn.dropout).to(device)\n",
    "# predictor = config.scoregnn.predictor\n",
    "\n",
    "# # åŠ è½½å‚æ•°ï¼ˆå‡è®¾ä½ çš„æ–‡ä»¶ç»“æ„æ˜¯è¿™æ ·ä¿å­˜çš„ï¼‰\n",
    "# checkpoint = torch.load('./model/scoregnn.pth', map_location=device)\n",
    "# model.load_state_dict(checkpoint['model'])\n",
    "# predictor.load_state_dict(checkpoint['predictor'])\n",
    "\n",
    "# model.eval()\n",
    "# predictor.eval()\n",
    "# print(\"Model and predictor loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2fc0b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "æ‰«æ max_z: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [19:47<00:00, 74.24s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "æ‰«æ max_z: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [14:02<00:00, 52.65s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "æ‰«æ max_z: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:08<00:00, 68.34s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "æ‰«æ max_z: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:49<00:00, 49.99s/batch]\n",
      "æ‰«æ max_z: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [02:33<00:00, 76.74s/batch]\n",
      "æ‰«æ max_z: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [01:42<00:00, 51.11s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n",
      "ä¿å­˜ train åˆ†æ‰¹æ–‡ä»¶\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ä¿å­˜ ./data/Github/split\\SSSEAL_train_pos åˆ†æ‰¹æ–‡ä»¶: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 158/158 [1:51:38<00:00, 42.39s/batch]\n",
      "ä¿å­˜ ./data/Github/split\\SSSEAL_train_neg åˆ†æ‰¹æ–‡ä»¶:  30%|â–ˆâ–ˆâ–‰       | 47/158 [25:41<1:00:41, 32.81s/batch]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m      2\u001b[0m sampler \u001b[38;5;241m=\u001b[39m SubgraphBatchSampler(model \u001b[38;5;241m=\u001b[39m model, predictor \u001b[38;5;241m=\u001b[39m predictor, k_min \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mscoresampler\u001b[38;5;241m.\u001b[39mk_min, \n\u001b[0;32m      3\u001b[0m                           num_hops \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mscoresampler\u001b[38;5;241m.\u001b[39mnum_hops,save_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/split\u001b[39m\u001b[38;5;124m'\u001b[39m, alpha \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mscoresampler\u001b[38;5;241m.\u001b[39malpha, \n\u001b[0;32m      4\u001b[0m                           beta \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mscoresampler\u001b[38;5;241m.\u001b[39mbeta, gamma \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mscoresampler\u001b[38;5;241m.\u001b[39mgamma)\n\u001b[1;32m----> 5\u001b[0m \u001b[43msampler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m      7\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSample time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_time\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[3], line 127\u001b[0m, in \u001b[0;36mSubgraphBatchSampler.process\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mä¿å­˜ train åˆ†æ‰¹æ–‡ä»¶\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_batches(train_data, train_data\u001b[38;5;241m.\u001b[39mpos_edge_label_index, \u001b[38;5;241m1\u001b[39m, train_pos_path, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_z)\n\u001b[1;32m--> 127\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_batches\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mneg_edge_label_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_neg_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_max_z\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mä¿å­˜ val åˆ†æ‰¹æ–‡ä»¶\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_batches(val_data, val_data\u001b[38;5;241m.\u001b[39mpos_edge_label_index, \u001b[38;5;241m1\u001b[39m, val_pos_path, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_z)\n",
      "Cell \u001b[1;32mIn[3], line 57\u001b[0m, in \u001b[0;36mSubgraphBatchSampler.save_batches\u001b[1;34m(self, data, edge_label_index, y, out_prefix, max_z, batch_size)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_data \u001b[38;5;129;01min\u001b[39;00m batch_data_list:\n\u001b[0;32m     56\u001b[0m     batch_data\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mone_hot(batch_data\u001b[38;5;241m.\u001b[39mz, max_z \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat)\n\u001b[1;32m---> 57\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_data_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mout_prefix\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_batch\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43midx\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m batch_data_list\n\u001b[0;32m     59\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect()\n",
      "File \u001b[1;32mc:\\Users\\86186\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\serialization.py:628\u001b[0m, in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[0;32m    626\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m    627\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m--> 628\u001b[0m         \u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_disable_byteorder_record\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    629\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\86186\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\serialization.py:859\u001b[0m, in \u001b[0;36m_save\u001b[1;34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[0m\n\u001b[0;32m    855\u001b[0m \u001b[38;5;66;03m# given that we copy things around anyway, we might use storage.cpu()\u001b[39;00m\n\u001b[0;32m    856\u001b[0m \u001b[38;5;66;03m# this means to that to get tensors serialized, you need to implement\u001b[39;00m\n\u001b[0;32m    857\u001b[0m \u001b[38;5;66;03m# .cpu() on the underlying Storage\u001b[39;00m\n\u001b[0;32m    858\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m storage\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 859\u001b[0m     storage \u001b[38;5;241m=\u001b[39m \u001b[43mstorage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[38;5;66;03m# Now that it is on the CPU we can directly copy it into the zip file\u001b[39;00m\n\u001b[0;32m    861\u001b[0m num_bytes \u001b[38;5;241m=\u001b[39m storage\u001b[38;5;241m.\u001b[39mnbytes()\n",
      "File \u001b[1;32mc:\\Users\\86186\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\storage.py:137\u001b[0m, in \u001b[0;36m_StorageBase.cpu\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return a CPU copy of this storage if it's not already on the CPU.\"\"\"\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mUntypedStorage\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "sampler = SubgraphBatchSampler(model = model, predictor = predictor, k_min = config.scoresampler.k_min, \n",
    "                          num_hops = config.scoresampler.num_hops,save_dir = f'./data/{config.dataset}/split', alpha = config.scoresampler.alpha, \n",
    "                          beta = config.scoresampler.beta, gamma = config.scoresampler.gamma)\n",
    "sampler.process()\n",
    "end_time = time.time()\n",
    "logging.info(f'Sample time: {end_time - start_time}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0821f5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = SubgraphBatchSampler(model = model, predictor = predictor, k_min = config.scoresampler.k_min, \n",
    "                          num_hops = config.scoresampler.num_hops,save_dir = f'./data/{config.dataset}/split', alpha = config.scoresampler.alpha, \n",
    "                          beta = config.scoresampler.beta, gamma = config.scoresampler.gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9165258a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampler.cancat_pos_neg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c19215e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampler.cancat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbb03a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T21:13:25.483677Z",
     "iopub.status.busy": "2025-05-28T21:13:25.483677Z",
     "iopub.status.idle": "2025-05-28T21:15:53.508600Z",
     "shell.execute_reply": "2025-05-28T21:15:53.508600Z"
    }
   },
   "outputs": [],
   "source": [
    "# start_time = time.time()\n",
    "# sampler = SubgraphSampler(model = model, predictor = predictor, k_min = config.scoresampler.k_min, \n",
    "#                           num_hops = config.scoresampler.num_hops, alpha = config.scoresampler.alpha, \n",
    "#                           beta = config.scoresampler.beta, gamma = config.scoresampler.gamma)\n",
    "# sampler.process()\n",
    "# end_time = time.time()\n",
    "# logging.info(f'Sample time: {end_time - start_time}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
