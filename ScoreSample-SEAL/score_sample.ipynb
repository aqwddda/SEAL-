{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89fb66b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T21:13:19.585891Z",
     "iopub.status.busy": "2025-05-28T21:13:19.585891Z",
     "iopub.status.idle": "2025-05-28T21:13:25.337655Z",
     "shell.execute_reply": "2025-05-28T21:13:25.337655Z"
    }
   },
   "outputs": [],
   "source": [
    "import os, time\n",
    "import numpy as np\n",
    "import gc, random\n",
    "import torch\n",
    "import networkx as nx\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "import psutil\n",
    "import out_manager as om\n",
    "import torch.nn.functional as F\n",
    "from itertools import chain\n",
    "from config import Config\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import k_hop_subgraph, to_scipy_sparse_matrix\n",
    "from model.score_gnn import scoregnn_dict\n",
    "from scipy.sparse.csgraph import shortest_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67f15e69",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T21:13:25.337655Z",
     "iopub.status.busy": "2025-05-28T21:13:25.337655Z",
     "iopub.status.idle": "2025-05-28T21:13:25.368688Z",
     "shell.execute_reply": "2025-05-28T21:13:25.368688Z"
    }
   },
   "outputs": [],
   "source": [
    "config = Config()\n",
    "ModelClass = scoregnn_dict[config.scoregnn.gnn_type]\n",
    "out_dir = om.get_existing_out_dir(config)\n",
    "om.setup_logging(os.path.join(out_dir, \"sample_log.txt\"))\n",
    "seed = config.seed\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "device = config.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f202e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubgraphBatchSampler:\n",
    "    def __init__(self, model, predictor, k_min, num_hops, save_dir, alpha = 40, beta = 20, gamma = 2, device = device):\n",
    "        self.model = model.eval()\n",
    "        self.predictor = predictor.eval()\n",
    "        self.k_min = k_min\n",
    "        self.num_hops = num_hops\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.save_dir = save_dir\n",
    "        self.device = device\n",
    "        super().__init__()\n",
    "\n",
    "        # 注册所有可选打分函数\n",
    "        self.score_fn_dict = {\n",
    "            \"gnn\": self.get_subgraph_scores_gnn,\n",
    "            \"pagerank\": self.get_subgraph_scores_pagerank,\n",
    "            \"adamic-adar\": self.get_subgraph_scores_adamicadar,\n",
    "        }\n",
    "\n",
    "    # def get_max_z(self, data, edge_label_index, y, batch_size=1000):\n",
    "    #     random.seed(2025)\n",
    "    #     num_samples = edge_label_index.size(1)\n",
    "    #     # 使用tqdm添加进度条\n",
    "    #     for i in tqdm(range(0, num_samples, batch_size), desc=\"扫描 max_z\", unit=\"batch\"):\n",
    "    #         batch_idx = edge_label_index[:, i:i+batch_size]\n",
    "    #         batch_data_list = self.sample_all_edges(data, batch_idx, y)\n",
    "    #         for batch_data in batch_data_list:\n",
    "    #             zmax = batch_data.z.max().item()\n",
    "    #             if zmax > self._max_z:\n",
    "    #                 print(f\"⚠️ 更新 _max_z: {self._max_z} -> {zmax} (batch {i//batch_size}, y={y})\")\n",
    "    #                 self._max_z = zmax\n",
    "    #         del batch_data_list\n",
    "    #         gc.collect()\n",
    "\n",
    "\n",
    "    def get_max_z(self, data, edge_label_index, y, batch_size=1000):\n",
    "        num_samples = edge_label_index.size(1)\n",
    "        # 使用tqdm添加进度条\n",
    "        for i in tqdm(range(0, num_samples, batch_size), desc=\"扫描 max_z\", unit=\"batch\"):\n",
    "            batch_idx = edge_label_index[:, i:i+batch_size]\n",
    "            batch_data_list = self.sample_all_edges(data, batch_idx, y)\n",
    "            del batch_data_list\n",
    "            gc.collect()\n",
    "    \n",
    "    def save_batches(self, data, edge_label_index, y, out_prefix, max_z, batch_size=100):\n",
    "        random.seed(2025)\n",
    "        os.makedirs(os.path.dirname(out_prefix), exist_ok=True)\n",
    "        num_samples = edge_label_index.size(1)\n",
    "        idx = 0\n",
    "        # 使用tqdm添加进度条\n",
    "        for i in tqdm(range(0, num_samples, batch_size), desc=f\"保存 {out_prefix} 分批文件\", unit=\"batch\"):\n",
    "            batch_idx = edge_label_index[:, i:i+batch_size]\n",
    "            batch_data_list = self.sample_all_edges(data, batch_idx, y)\n",
    "            for batch_data in batch_data_list:\n",
    "                batch_data.x = F.one_hot(batch_data.z, max_z + 1).to(torch.float)\n",
    "                torch.save(batch_data_list, f\"{out_prefix}_batch{idx}.pt\")\n",
    "            del batch_data_list\n",
    "            gc.collect()\n",
    "            idx += 1\n",
    "\n",
    "    def merge_batches(self, batch_prefix, out_file):\n",
    "        batch_files = sorted(glob.glob(f\"{batch_prefix}_batch*.pt\"),\n",
    "                            key=lambda x: int(x.split('_batch')[-1].split('.pt')[0]))\n",
    "        all_data = []\n",
    "        for batch_file in batch_files:\n",
    "            data_list = torch.load(batch_file, map_location='cpu')  # 👈 强制放到 CPU\n",
    "            all_data.extend(data_list)\n",
    "            print(f\"合并了 {batch_file}，当前总量：{len(all_data)}\")\n",
    "            del data_list\n",
    "            gc.collect()\n",
    "            # CPU 上不用显存释放了\n",
    "        torch.save(all_data, out_file)\n",
    "        print(f\"保存到 {out_file}，总计 {len(all_data)} 条数据\")\n",
    "        del all_data\n",
    "        gc.collect()\n",
    "\n",
    "    def merge_pos_neg(self, pos_file, neg_file, out_file):\n",
    "        pos_data = torch.load(pos_file, map_location='cpu')  # 👈 放在 CPU\n",
    "        neg_data = torch.load(neg_file, map_location='cpu')  # 👈 放在 CPU\n",
    "        all_data = pos_data + neg_data\n",
    "        torch.save(all_data, out_file)\n",
    "        print(f\"最终合并 {out_file}，总计 {len(all_data)} 条（正例 {len(pos_data)}，负例 {len(neg_data)}）\")\n",
    "        del pos_data, neg_data, all_data\n",
    "        gc.collect()\n",
    "\n",
    "    \n",
    "    def process(self):\n",
    "\n",
    "        seed = 2025\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "        train_data = torch.load(f'./data/{config.dataset}/split/train_data.pt')\n",
    "        val_data = torch.load(f'./data/{config.dataset}/split/val_data.pt')\n",
    "        test_data = torch.load(f'./data/{config.dataset}/split/test_data.pt')\n",
    "\n",
    "        train_data = train_data.to(self.device)\n",
    "        val_data = val_data.to(self.device)\n",
    "        test_data = test_data.to(self.device)\n",
    "\n",
    "        #第一次扫描统计maxz\n",
    "        self._max_z = 0\n",
    "        self.get_max_z(train_data, train_data.pos_edge_label_index, 1),\n",
    "        print(self._max_z)\n",
    "        self.get_max_z(train_data, train_data.neg_edge_label_index, 0)\n",
    "        print(self._max_z)\n",
    "        self.get_max_z(val_data, val_data.pos_edge_label_index, 1),\n",
    "        print(self._max_z)\n",
    "        self.get_max_z(val_data, val_data.neg_edge_label_index, 0)\n",
    "        self.get_max_z(test_data, test_data.pos_edge_label_index, 1),\n",
    "        self.get_max_z(test_data, test_data.neg_edge_label_index, 0)\n",
    "        print(self._max_z)\n",
    "\n",
    "        # 2. 分批次one-hot和保存，绝不汇总到内存\n",
    "        save_dir = self.save_dir\n",
    "        train_pos_path = os.path.join(save_dir, \"SSSEAL_train_pos\")\n",
    "        train_neg_path = os.path.join(save_dir, \"SSSEAL_train_neg\")\n",
    "        val_pos_path = os.path.join(save_dir, \"SSSEAL_val_pos\")\n",
    "        val_neg_path = os.path.join(save_dir, \"SSSEAL_val_neg\")\n",
    "        test_pos_path = os.path.join(save_dir, \"SSSEAL_test_pos\")\n",
    "        test_neg_path = os.path.join(save_dir, \"SSSEAL_test_neg\")\n",
    "\n",
    "        print(\"保存 train 分批文件\")\n",
    "        self.save_batches(train_data, train_data.pos_edge_label_index, 1, train_pos_path, self._max_z)\n",
    "        self.save_batches(train_data, train_data.neg_edge_label_index, 0, train_neg_path, self._max_z)\n",
    "\n",
    "        print(\"保存 val 分批文件\")\n",
    "        self.save_batches(val_data, val_data.pos_edge_label_index, 1, val_pos_path, self._max_z)\n",
    "        self.save_batches(val_data, val_data.neg_edge_label_index, 0, val_neg_path, self._max_z)\n",
    "\n",
    "        print(\"保存 test 分批文件\")\n",
    "        self.save_batches(test_data, test_data.pos_edge_label_index, 1, test_pos_path, self._max_z)\n",
    "        self.save_batches(test_data, test_data.neg_edge_label_index, 0, test_neg_path, self._max_z)\n",
    "\n",
    "        print(\"所有分批处理和保存已完成！🚀\")\n",
    "\n",
    "        del train_data, val_data, test_data\n",
    "        gc.collect()\n",
    "    \n",
    "    def cancat_pos(self):\n",
    "        split_dir = self.save_dir\n",
    "        for prefix in [\"train\", \"val\", \"test\"]:\n",
    "            pos_prefix = os.path.join(split_dir, f\"SSSEAL_{prefix}_pos\")\n",
    "            pos_data_list = pos_prefix + \"_data_list.pt\"\n",
    "            merged_data_list = pos_data_list  # 直接用 pos_data_list 作为结果\n",
    "\n",
    "            print(f\"\\n--- 合并 {prefix} pos batch ---\")\n",
    "            self.merge_batches(pos_prefix, merged_data_list)\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    def cancat_neg(self):\n",
    "        split_dir = self.save_dir\n",
    "        for prefix in [\"train\", \"val\", \"test\"]:\n",
    "            neg_prefix = os.path.join(split_dir, f\"SSSEAL_{prefix}_neg\")\n",
    "            neg_data_list = neg_prefix + \"_data_list.pt\"\n",
    "            merged_data_list = neg_data_list  # 直接用 neg_data_list 作为结果\n",
    "\n",
    "            print(f\"\\n--- 合并 {prefix} neg batch ---\")\n",
    "            self.merge_batches(neg_prefix, merged_data_list)\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    def cancat_pos_neg(self):\n",
    "        split_dir = self.save_dir\n",
    "        for prefix in [\"train\", \"val\", \"test\"]:\n",
    "            pos_data_list = os.path.join(split_dir, f\"SSSEAL_{prefix}_pos_data_list.pt\")\n",
    "            neg_data_list = os.path.join(split_dir, f\"SSSEAL_{prefix}_neg_data_list.pt\")\n",
    "            merged_data_list = os.path.join(f\"./data/{config.dataset}/split/ssseal_{prefix}_data_k{self.k_min}_h{self.num_hops}_{config.version}.pt\")\n",
    "\n",
    "            print(f\"\\n--- 合并 {prefix} pos+neg 为总 data_list ---\")\n",
    "            self.merge_pos_neg(pos_data_list, neg_data_list, merged_data_list)\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        print(\"所有 pos+neg 合并已完成！🚀\")\n",
    "\n",
    "    def cancat(self):\n",
    "        split_dir = self.save_dir\n",
    "        for prefix in [\"train\", \"val\", \"test\"]:\n",
    "            pos_prefix = os.path.join(split_dir, f\"SSSEAL_{prefix}_pos\")\n",
    "            neg_prefix = os.path.join(split_dir, f\"SSSEAL_{prefix}_neg\")\n",
    "            pos_data_list = pos_prefix + \"_data_list.pt\"\n",
    "            neg_data_list = neg_prefix + \"_data_list.pt\"\n",
    "            merged_data_list = os.path.join(split_dir, f\"ssseal_{prefix}_data_k{self.k_min}_h{self.num_hops}_{config.version}.pt\")\n",
    "\n",
    "            print(f\"\\n--- 合并 {prefix} pos batch ---\")\n",
    "            self.merge_batches(pos_prefix, pos_data_list)\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()  # 👈 pos 合并后清理显存\n",
    "\n",
    "            print(f\"--- 合并 {prefix} neg batch ---\")\n",
    "            self.merge_batches(neg_prefix, neg_data_list)\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()  # 👈 neg 合并后清理显存\n",
    "\n",
    "            print(f\"--- 合并 {prefix} pos+neg 为总 data_list ---\")\n",
    "            self.merge_pos_neg(pos_data_list, neg_data_list, merged_data_list)\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()  # 👈 pos+neg 合并完再清理\n",
    "\n",
    "        print(\"所有分批处理、合并已完成！🚀\")\n",
    "\n",
    "        # --------- 自动删除所有 batch 文件 ----------\n",
    "        pattern = os.path.join(self.save_dir, \"SSSEAL_*_batch*.pt\")\n",
    "        batch_files = glob.glob(pattern)\n",
    "        for file in batch_files:\n",
    "            try:\n",
    "                os.remove(file)\n",
    "                print(f\"已删除 {file}\")\n",
    "            except Exception as e:\n",
    "                print(f\"删除 {file} 失败：{e}\")\n",
    "        \n",
    "        target_files = [\n",
    "            \"SSSEAL_test_neg_data_list.pt\",\n",
    "            \"SSSEAL_test_pos_data_list.pt\",\n",
    "            \"SSSEAL_val_neg_data_list.pt\",\n",
    "            \"SSSEAL_val_pos_data_list.pt\",\n",
    "            \"SSSEAL_train_neg_data_list.pt\",\n",
    "            \"SSSEAL_train_pos_data_list.pt\"\n",
    "        ]\n",
    "\n",
    "        for filename in target_files:\n",
    "            file_path = os.path.join(self.save_dir, filename)\n",
    "            if os.path.exists(file_path):\n",
    "                try:\n",
    "                    os.remove(file_path)\n",
    "                    print(f\"已删除 {file_path}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"删除 {file_path} 失败：{e}\")\n",
    "\n",
    "        print('所有数据已保存并清理临时 batch 文件')\n",
    "\n",
    "    \n",
    "    def sample_all_edges(self, data, edge_label_index, y):\n",
    "        data_list = []\n",
    "        for src, dst in edge_label_index.t().tolist():\n",
    "            data_list.append(self.sample_subgraph(src, dst, data, y))\n",
    "        return data_list\n",
    "    \n",
    "    def sample_all_edges_in_batches(self, data, edge_label_index, y, batch_size=256):\n",
    "        data_list = []\n",
    "        num_edges = edge_label_index.size(1)\n",
    "        for start in range(0, num_edges, batch_size):\n",
    "            end = min(start + batch_size, num_edges)\n",
    "            batch_edges = edge_label_index[:, start:end]\n",
    "            for src, dst in batch_edges.t().tolist():\n",
    "                subgraph = self.sample_subgraph(src, dst, data, y)\n",
    "                data_list.append(subgraph.cpu())\n",
    "                del subgraph\n",
    "            gc.collect() \n",
    "            torch.cuda.empty_cache()  # 清理缓存\n",
    "        return data_list\n",
    "\n",
    "    \n",
    "    def sample_subgraph(self, src, dst, data, y):\n",
    "         \n",
    "        # # 采k-hop子图，得到子图节点的新编号、子图内边、mapping\n",
    "        sub_node_index, sub_edge_index, mapping, _ = k_hop_subgraph(\n",
    "            [src, dst], self.num_hops, data.edge_index, relabel_nodes=True)\n",
    "        \n",
    "        if len(sub_node_index) <= self.k_min:\n",
    "            sub_node_index, sub_edge_index, mapping, _ = k_hop_subgraph(\n",
    "            [src, dst], self.num_hops + self.gamma, data.edge_index, relabel_nodes=True)\n",
    "\n",
    "            sub_x = data.x[sub_node_index]\n",
    "            sub_src, sub_dst = mapping.tolist()\n",
    "\n",
    "            final_nodes = list(range(len(sub_x)))\n",
    "\n",
    "        else:\n",
    "            #子图全部节点初始特征向量(sub.num_of_node, data.x.size(1))\n",
    "            # sub_x = data.x[sub_node_index].to(self.device)\n",
    "            sub_edge_index = sub_edge_index.to(self.device)\n",
    "            sub_x = data.x[sub_node_index]\n",
    "            sub_src, sub_dst = mapping.tolist()\n",
    "            \n",
    "            #构建子图的data\n",
    "            sub_data = Data(x = sub_x,edge_index = sub_edge_index).to(self.device)\n",
    "            #获取子图的所有节点分数字典（不包含src和dst）\n",
    "            # scores_dist, sub_node_emb = self.get_subgraph_scores(sub_src, sub_dst, sub_data)\n",
    "            \n",
    "            # ====== 改进点：用torch.topk替代heapq.nlargest，速度更快 =======\n",
    "\n",
    "            # # 分数从高到低取前top_k\n",
    "            # scores_dist = self.get_subgraph_scores(sub_src, sub_dst, sub_data)\n",
    "            # topk_neighbors = heapq.nlargest(self.k_min, scores_dist, key=scores_dist.get)\n",
    "\n",
    "            candidates_tensor, scores = self.get_subgraph_scores(sub_src, sub_dst, sub_data)\n",
    "\n",
    "            k = max(1, int(len(scores) * self.alpha // 100))\n",
    "            _, topk_indices = torch.topk(scores, min(k, scores.size(0)))\n",
    "            topk_neighbors = candidates_tensor[topk_indices].tolist()\n",
    "            # ===========================================================\n",
    "            # 从剩下的候选节点中随机选择20%的节点\n",
    "            remaining_candidates = [i for i in candidates_tensor.tolist() if i not in topk_neighbors]\n",
    "            num_random_select = min(int(len(scores) * self.beta // 100), len(remaining_candidates))\n",
    "            random_neighbors = random.sample(remaining_candidates, num_random_select)\n",
    "\n",
    "            # 合并前40%和随机选择的节点，得到最终的topk_neighbors\n",
    "            final_neighbors = topk_neighbors + random_neighbors\n",
    "            # 源点和目标点在子图的编号\n",
    "            final_nodes = [sub_src, sub_dst] + final_neighbors\n",
    "        \n",
    "        final_nodes = list(set(final_nodes))  # 防止重复\n",
    "        final_nodes.sort()  # 方便后面重新映射\n",
    "\n",
    "        # 旧编号到新编号的映射\n",
    "        node_id_map = {old: new for new, old in enumerate(final_nodes)}\n",
    "\n",
    "        # 新的x\n",
    "        final_x = sub_x[final_nodes]\n",
    "\n",
    "        # mask边：只保留两个端点都在final_nodes内的边\n",
    "        final_nodes_tensor = torch.tensor(final_nodes, device=self.device)\n",
    "        mask = torch.isin(sub_edge_index[0], final_nodes_tensor) & \\\n",
    "            torch.isin(sub_edge_index[1], final_nodes_tensor)\n",
    "        final_edge_index = sub_edge_index[:, mask]\n",
    "\n",
    "        # 重新编号edge_index\n",
    "        final_edge_index = torch.stack([\n",
    "            torch.tensor([node_id_map[int(i)] for i in final_edge_index[0].tolist()], device=self.device),\n",
    "            torch.tensor([node_id_map[int(i)] for i in final_edge_index[1].tolist()], device=self.device)\n",
    "        ], dim=0)\n",
    "\n",
    "        #去除 src-dst 之间的边（无向图记得两个方向都删！）\n",
    "        src_new = node_id_map[sub_src]\n",
    "        dst_new = node_id_map[sub_dst]\n",
    "        mask1 = (final_edge_index[0] != src_new) | (final_edge_index[1] != dst_new)\n",
    "        mask2 = (final_edge_index[0] != dst_new) | (final_edge_index[1] != src_new)\n",
    "        mask = mask1 & mask2\n",
    "        final_edge_index = final_edge_index[:, mask]\n",
    "\n",
    "        z = self.drnl_node_labeling(final_edge_index, src_new, dst_new, num_nodes = len(final_nodes))\n",
    "\n",
    "        final_sub_data = Data(x = final_x, z = z, edge_index = final_edge_index, y = y)\n",
    "        final_sub_data = final_sub_data.to(next(self.model.parameters()).device)\n",
    "        return final_sub_data\n",
    "    \n",
    "    def get_subgraph_scores(self, src, dst, data):\n",
    "        fn = self.score_fn_dict.get(config.scoresampler.score_fn, self.get_subgraph_scores_gnn)  # 默认GNN\n",
    "        return fn(src, dst, data)\n",
    "    \n",
    "    def get_subgraph_scores_gnn(self, src, dst, data):\n",
    "        with torch.no_grad():\n",
    "            device = next(self.model.parameters()).device\n",
    "            data = data.to(device)\n",
    "            node_emb =self.model(data.x, data.edge_index)\n",
    "\n",
    "            #构建所有src和dst分别到子图所有节点的组合(不包含互相)\n",
    "            candidates = [i for i in range(data.num_nodes) if i != src and i != dst]\n",
    "            candidates_tensor = torch.tensor(candidates, device=self.device, dtype=torch.long)\n",
    "\n",
    "            src_1 = torch.tensor([src] * len(candidates), dtype=torch.long, device=self.device)\n",
    "            dst_1 = torch.tensor(candidates, dtype=torch.long, device=self.device)\n",
    "            src_2 = torch.tensor([dst] * len(candidates), dtype=torch.long, device=self.device)\n",
    "            dst_2 = torch.tensor(candidates, dtype=torch.long, device=self.device)\n",
    "            \n",
    "            edge_label_index_1 = torch.stack([src_1, dst_1], dim=0)\n",
    "            edge_label_index_2 = torch.stack([src_2, dst_2], dim=0)\n",
    "\n",
    "            scores_1 = self.predictor(node_emb, edge_label_index_1)\n",
    "            scores_2 = self.predictor(node_emb, edge_label_index_2)\n",
    "            scores = (scores_1 + scores_2) / 2\n",
    "            # scores_dist = {i: float(score) for i, score in zip(candidates, scores)}\n",
    "        return candidates_tensor, scores\n",
    "    \n",
    "    def get_subgraph_scores_adamicadar(self, src, dst, data):\n",
    "        edge_index = data.edge_index.cpu().numpy()\n",
    "        G = nx.Graph()\n",
    "        G.add_edges_from(edge_index.T.tolist())\n",
    "        G.add_nodes_from(range(data.num_nodes))  # 保证节点都在\n",
    "\n",
    "        # 只考虑有边的节点作为候选\n",
    "        candidates = [i for i in range(data.num_nodes) if i != src and i != dst and G.degree(i) > 0]\n",
    "        candidates_tensor = torch.tensor(candidates,device=self.device, dtype=torch.long)\n",
    "\n",
    "        # 如果src或dst本身也是孤立节点，也跳过/直接返回空\n",
    "        if G.degree(src) == 0 or G.degree(dst) == 0:\n",
    "            return candidates_tensor, torch.zeros_like(candidates_tensor, dtype=torch.float)\n",
    "\n",
    "        aa_src = {(u, v): s for u, v, s in nx.adamic_adar_index(G, [(src, i) for i in candidates])}\n",
    "        aa_dst = {(u, v): s for u, v, s in nx.adamic_adar_index(G, [(dst, i) for i in candidates])}\n",
    "\n",
    "        scores = []\n",
    "        for i in candidates:\n",
    "            s1 = aa_src.get((src, i), 0.0)\n",
    "            s2 = aa_dst.get((dst, i), 0.0)\n",
    "            s = (s1 + s2) / 2\n",
    "            scores.append(s)\n",
    "\n",
    "        scores = torch.tensor(scores, device=self.device, dtype=torch.float)\n",
    "        return candidates_tensor, scores\n",
    "        \n",
    "    def get_subgraph_scores_pagerank(self, src, dst, data):\n",
    "        # 1. edge_index转成networkx图，节点编号是局部编号\n",
    "        edge_index = data.edge_index.cpu().numpy()\n",
    "        G = nx.Graph()\n",
    "        G.add_edges_from(edge_index.T.tolist())\n",
    "        G.add_nodes_from(range(data.num_nodes))  # 确保所有节点都在G中\n",
    "\n",
    "        # 2. 只考虑有边的节点\n",
    "        candidates = [i for i in range(data.num_nodes) if i != src and i != dst and G.degree(i) > 0]\n",
    "        candidates_tensor = torch.tensor(candidates, device=self.device, dtype=torch.long)\n",
    "\n",
    "        # 如果src或dst本身是孤立节点，直接返回零分\n",
    "        if G.degree(src) == 0 or G.degree(dst) == 0:\n",
    "            return candidates_tensor, torch.zeros(len(candidates), device=self.device, dtype=torch.float)\n",
    "\n",
    "        # 3. Personalized PageRank（以src和dst为个性化起点，各算一次）\n",
    "        personalization_src = {n: 0 for n in G.nodes}\n",
    "        personalization_src[src] = 1\n",
    "        pr_src = nx.pagerank(G, personalization=personalization_src)\n",
    "\n",
    "        personalization_dst = {n: 0 for n in G.nodes}\n",
    "        personalization_dst[dst] = 1\n",
    "        pr_dst = nx.pagerank(G, personalization=personalization_dst)\n",
    "\n",
    "        # 4. 对每个候选节点，分别查src和dst个性化pagerank的分数，做平均\n",
    "        scores = []\n",
    "        for i in candidates:\n",
    "            s = (pr_src.get(i, 0.0) + pr_dst.get(i, 0.0)) / 2\n",
    "            scores.append(s)\n",
    "\n",
    "        # 转成torch张量\n",
    "        scores = torch.tensor(scores, device=self.device, dtype=torch.float)\n",
    "        return candidates_tensor, scores\n",
    "\n",
    "    def drnl_node_labeling(self, edge_index, src, dst, num_nodes=None):\n",
    "        # Double-radius node labeling (DRNL).\n",
    "        src, dst = (dst, src) if src > dst else (src, dst)\n",
    "        adj = to_scipy_sparse_matrix(edge_index, num_nodes=num_nodes).tocsr()\n",
    "\n",
    "        idx = list(range(src)) + list(range(src + 1, adj.shape[0]))\n",
    "        adj_wo_src = adj[idx, :][:, idx]\n",
    "\n",
    "        idx = list(range(dst)) + list(range(dst + 1, adj.shape[0]))\n",
    "        adj_wo_dst = adj[idx, :][:, idx]\n",
    "\n",
    "        dist2src = shortest_path(adj_wo_dst, directed=False, unweighted=True,\n",
    "                                 indices=src)\n",
    "        dist2src = np.insert(dist2src, dst, 0, axis=0)\n",
    "        dist2src = torch.from_numpy(dist2src)\n",
    "\n",
    "        dist2dst = shortest_path(adj_wo_src, directed=False, unweighted=True,\n",
    "                                 indices=dst - 1)\n",
    "        dist2dst = np.insert(dist2dst, src, 0, axis=0)\n",
    "        dist2dst = torch.from_numpy(dist2dst)\n",
    "\n",
    "        dist = dist2src + dist2dst\n",
    "        dist_over_2, dist_mod_2 = dist // 2, dist % 2\n",
    "\n",
    "        z = 1 + torch.min(dist2src, dist2dst)\n",
    "        z += dist_over_2 * (dist_over_2 + dist_mod_2 - 1)\n",
    "        z[src] = 1.\n",
    "        z[dst] = 1.\n",
    "        z[torch.isnan(z)] = 0.\n",
    "\n",
    "        self._max_z = max(int(z.max()), self._max_z)\n",
    "\n",
    "        return z.to(torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15c8cff2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T21:13:25.368688Z",
     "iopub.status.busy": "2025-05-28T21:13:25.368688Z",
     "iopub.status.idle": "2025-05-28T21:13:25.397099Z",
     "shell.execute_reply": "2025-05-28T21:13:25.397099Z"
    }
   },
   "outputs": [],
   "source": [
    "class SubgraphSampler:\n",
    "    def __init__(self, model, predictor, k_min, num_hops, alpha = 40, beta = 20, gamma = 2, device = device):\n",
    "        self.model = model.eval()\n",
    "        self.predictor = predictor.eval()\n",
    "        self.k_min = k_min\n",
    "        self.num_hops = num_hops\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.device = device\n",
    "\n",
    "        # 注册所有可选打分函数\n",
    "        self.score_fn_dict = {\n",
    "            \"gnn\": self.get_subgraph_scores_gnn,\n",
    "            \"pagerank\": self.get_subgraph_scores_pagerank,\n",
    "            \"adamic-adar\": self.get_subgraph_scores_adamicadar,\n",
    "        }\n",
    "\n",
    "    \n",
    "    def process(self):\n",
    "        train_data = torch.load(f'./data/{config.dataset}/split/train_data.pt')\n",
    "        val_data = torch.load(f'./data/{config.dataset}/split/val_data.pt')\n",
    "        test_data = torch.load(f'./data/{config.dataset}/split/test_data.pt')\n",
    "\n",
    "        train_data = train_data.to(self.device)\n",
    "        val_data = val_data.to(self.device)\n",
    "        test_data = test_data.to(self.device)\n",
    "\n",
    "        self._max_z = 0\n",
    "\n",
    "        train_pos_data_list = self.sample_all_edges_in_batches(\n",
    "        train_data, train_data.pos_edge_label_index, 1)\n",
    "        train_neg_data_list = self.sample_all_edges_in_batches(\n",
    "        train_data, train_data.neg_edge_label_index, 0)\n",
    "\n",
    "        val_pos_data_list = self.sample_all_edges_in_batches(\n",
    "        val_data, val_data.pos_edge_label_index, 1)\n",
    "        val_neg_data_list = self.sample_all_edges_in_batches(\n",
    "        val_data, val_data.neg_edge_label_index, 0)\n",
    "\n",
    "        test_pos_data_list = self.sample_all_edges_in_batches(\n",
    "        test_data, test_data.pos_edge_label_index, 1)\n",
    "        test_neg_data_list = self.sample_all_edges_in_batches(\n",
    "        test_data, test_data.neg_edge_label_index, 0)\n",
    "\n",
    "        for data in chain(train_pos_data_list, train_neg_data_list,\n",
    "                          val_pos_data_list, val_neg_data_list,\n",
    "                          test_pos_data_list, test_neg_data_list):\n",
    "            # We solely learn links from structure, dropping any node features:\n",
    "            data.x = F.one_hot(data.z, self._max_z + 1).to(torch.float)\n",
    "\n",
    "        train_data_list = train_pos_data_list + train_neg_data_list\n",
    "        val_data_list = val_pos_data_list + val_neg_data_list\n",
    "        test_data_list = test_pos_data_list + test_neg_data_list\n",
    "\n",
    "        torch.save(train_data_list, f'./data/{config.dataset}/split/ssseal_train_data_k{self.k_min}_h{self.num_hops}_{config.version}.pt')\n",
    "        torch.save(val_data_list, f'./data/{config.dataset}/split/ssseal_val_data_k{self.k_min}_h{self.num_hops}_{config.version}.pt')\n",
    "        torch.save(test_data_list, f'./data/{config.dataset}/split/ssseal_test_data_k{self.k_min}_h{self.num_hops}_{config.version}.pt')\n",
    "        print(\"All processed data have been saved.\")\n",
    "    \n",
    "    def sample_all_edges(self, data, edge_label_index, y):\n",
    "        data_list = []\n",
    "        for src, dst in edge_label_index.t().tolist():\n",
    "            data_list.append(self.sample_subgraph(src, dst, data, y))\n",
    "        return data_list\n",
    "    \n",
    "    def sample_all_edges_in_batches(self, data, edge_label_index, y, batch_size=256):\n",
    "        data_list = []\n",
    "        num_edges = edge_label_index.size(1)\n",
    "        for start in range(0, num_edges, batch_size):\n",
    "            end = min(start + batch_size, num_edges)\n",
    "            batch_edges = edge_label_index[:, start:end]\n",
    "            for src, dst in batch_edges.t().tolist():\n",
    "                subgraph = self.sample_subgraph(src, dst, data, y)\n",
    "                data_list.append(subgraph.cpu())\n",
    "                del subgraph\n",
    "            gc.collect() \n",
    "            torch.cuda.empty_cache()  # 清理缓存\n",
    "        return data_list\n",
    "\n",
    "    \n",
    "    def sample_subgraph(self, src, dst, data, y):\n",
    "        \n",
    "        # # 采k-hop子图，得到子图节点的新编号、子图内边、mapping\n",
    "        sub_node_index, sub_edge_index, mapping, _ = k_hop_subgraph(\n",
    "            [src, dst], self.num_hops, data.edge_index, relabel_nodes=True)\n",
    "        \n",
    "        if len(sub_node_index) <= self.k_min:\n",
    "            sub_node_index, sub_edge_index, mapping, _ = k_hop_subgraph(\n",
    "            [src, dst], self.num_hops + self.gamma, data.edge_index, relabel_nodes=True)\n",
    "\n",
    "            sub_x = data.x[sub_node_index]\n",
    "            sub_src, sub_dst = mapping.tolist()\n",
    "\n",
    "            final_nodes = list(range(len(sub_x)))\n",
    "\n",
    "        else:\n",
    "            sub_edge_index = sub_edge_index.to(self.device)\n",
    "            #子图全部节点初始特征向量(sub.num_of_node, data.x.size(1))\n",
    "            # sub_x = data.x[sub_node_index].to(self.device)\n",
    "            sub_x = data.x[sub_node_index]\n",
    "            sub_src, sub_dst = mapping.tolist()\n",
    "            \n",
    "            #构建子图的data\n",
    "            sub_data = Data(x = sub_x,edge_index = sub_edge_index).to(self.device)\n",
    "            #获取子图的所有节点分数字典（不包含src和dst）\n",
    "            # scores_dist, sub_node_emb = self.get_subgraph_scores(sub_src, sub_dst, sub_data)\n",
    "            \n",
    "            # ====== 改进点：用torch.topk替代heapq.nlargest，速度更快 =======\n",
    "\n",
    "            # # 分数从高到低取前top_k\n",
    "            # scores_dist = self.get_subgraph_scores(sub_src, sub_dst, sub_data)\n",
    "            # topk_neighbors = heapq.nlargest(self.k_min, scores_dist, key=scores_dist.get)\n",
    "\n",
    "            candidates_tensor, scores = self.get_subgraph_scores(sub_src, sub_dst, sub_data)\n",
    "\n",
    "            k = max(1, int(len(scores) * self.alpha // 100))\n",
    "            _, topk_indices = torch.topk(scores, min(k, scores.size(0)))\n",
    "            topk_neighbors = candidates_tensor[topk_indices].tolist()\n",
    "            # ===========================================================\n",
    "            # 从剩下的候选节点中随机选择20%的节点\n",
    "            remaining_candidates = [i for i in candidates_tensor.tolist() if i not in topk_neighbors]\n",
    "            num_random_select = min(int(len(scores) * self.beta // 100), len(remaining_candidates))\n",
    "            random_neighbors = random.sample(remaining_candidates, num_random_select)\n",
    "\n",
    "            # 合并前40%和随机选择的节点，得到最终的topk_neighbors\n",
    "            final_neighbors = topk_neighbors + random_neighbors\n",
    "            # 源点和目标点在子图的编号\n",
    "            final_nodes = [sub_src, sub_dst] + final_neighbors\n",
    "        \n",
    "        final_nodes = list(set(final_nodes))  # 防止重复\n",
    "        final_nodes.sort()  # 方便后面重新映射\n",
    "\n",
    "        # 旧编号到新编号的映射\n",
    "        node_id_map = {old: new for new, old in enumerate(final_nodes)}\n",
    "\n",
    "        # 新的x\n",
    "        final_x = sub_x[final_nodes]\n",
    "\n",
    "        # mask边：只保留两个端点都在final_nodes内的边\n",
    "        final_nodes_tensor = torch.tensor(final_nodes, device=self.device)\n",
    "        mask = torch.isin(sub_edge_index[0], final_nodes_tensor) & \\\n",
    "            torch.isin(sub_edge_index[1], final_nodes_tensor)\n",
    "        final_edge_index = sub_edge_index[:, mask]\n",
    "\n",
    "        # 重新编号edge_index\n",
    "        final_edge_index = torch.stack([\n",
    "            torch.tensor([node_id_map[int(i)] for i in final_edge_index[0].tolist()], device=self.device),\n",
    "            torch.tensor([node_id_map[int(i)] for i in final_edge_index[1].tolist()], device=self.device)\n",
    "        ], dim=0)\n",
    "\n",
    "        #去除 src-dst 之间的边（无向图记得两个方向都删！）\n",
    "        src_new = node_id_map[sub_src]\n",
    "        dst_new = node_id_map[sub_dst]\n",
    "        mask1 = (final_edge_index[0] != src_new) | (final_edge_index[1] != dst_new)\n",
    "        mask2 = (final_edge_index[0] != dst_new) | (final_edge_index[1] != src_new)\n",
    "        mask = mask1 & mask2\n",
    "        final_edge_index = final_edge_index[:, mask]\n",
    "\n",
    "        z = self.drnl_node_labeling(final_edge_index, src_new, dst_new, num_nodes = len(final_nodes))\n",
    "\n",
    "        final_sub_data = Data(x = final_x, z = z, edge_index = final_edge_index, y = y)\n",
    "        return final_sub_data\n",
    "    \n",
    "    def get_subgraph_scores(self, src, dst, data):\n",
    "        fn = self.score_fn_dict.get(config.scoresampler.score_fn, self.get_subgraph_scores_gnn)  # 默认GNN\n",
    "        return fn(src, dst, data)\n",
    "    \n",
    "    def get_subgraph_scores_gnn(self, src, dst, data):\n",
    "        with torch.no_grad():\n",
    "            node_emb =self.model(data.x, data.edge_index)\n",
    "\n",
    "            #构建所有src和dst分别到子图所有节点的组合(不包含互相)\n",
    "            candidates = [i for i in range(data.num_nodes) if i != src and i != dst]\n",
    "            candidates_tensor = torch.tensor(candidates, device=self.device, dtype=torch.long)\n",
    "\n",
    "            src_1 = torch.tensor([src] * len(candidates), dtype=torch.long, device=self.device)\n",
    "            dst_1 = torch.tensor(candidates, dtype=torch.long, device=self.device)\n",
    "            src_2 = torch.tensor([dst] * len(candidates), dtype=torch.long, device=self.device)\n",
    "            dst_2 = torch.tensor(candidates, dtype=torch.long, device=self.device)\n",
    "            \n",
    "            edge_label_index_1 = torch.stack([src_1, dst_1], dim=0)\n",
    "            edge_label_index_2 = torch.stack([src_2, dst_2], dim=0)\n",
    "\n",
    "            scores_1 = self.predictor(node_emb, edge_label_index_1)\n",
    "            scores_2 = self.predictor(node_emb, edge_label_index_2)\n",
    "            scores = (scores_1 + scores_2) / 2\n",
    "            # scores_dist = {i: float(score) for i, score in zip(candidates, scores)}\n",
    "        return candidates_tensor, scores\n",
    "    \n",
    "    def get_subgraph_scores_adamicadar(self, src, dst, data):\n",
    "        edge_index = data.edge_index.cpu().numpy()\n",
    "        G = nx.Graph()\n",
    "        G.add_edges_from(edge_index.T.tolist())\n",
    "        G.add_nodes_from(range(data.num_nodes))  # 保证节点都在\n",
    "\n",
    "        # 只考虑有边的节点作为候选\n",
    "        candidates = [i for i in range(data.num_nodes) if i != src and i != dst and G.degree(i) > 0]\n",
    "        candidates_tensor = torch.tensor(candidates, device=self.device, dtype=torch.long)\n",
    "\n",
    "        # 如果src或dst本身也是孤立节点，也跳过/直接返回空\n",
    "        if G.degree(src) == 0 or G.degree(dst) == 0:\n",
    "            return candidates_tensor, torch.zeros_like(candidates_tensor, dtype=torch.float, device=self.device)\n",
    "\n",
    "        aa_src = {(u, v): s for u, v, s in nx.adamic_adar_index(G, [(src, i) for i in candidates])}\n",
    "        aa_dst = {(u, v): s for u, v, s in nx.adamic_adar_index(G, [(dst, i) for i in candidates])}\n",
    "\n",
    "        scores = []\n",
    "        for i in candidates:\n",
    "            s1 = aa_src.get((src, i), 0.0)\n",
    "            s2 = aa_dst.get((dst, i), 0.0)\n",
    "            s = (s1 + s2) / 2\n",
    "            scores.append(s)\n",
    "\n",
    "        scores = torch.tensor(scores, device=self.device, dtype=torch.float)\n",
    "        return candidates_tensor, scores\n",
    "        \n",
    "    def get_subgraph_scores_pagerank(self, src, dst, data):\n",
    "        # 1. edge_index转成networkx图，节点编号是局部编号\n",
    "        edge_index = data.edge_index.cpu().numpy()\n",
    "        G = nx.Graph()\n",
    "        G.add_edges_from(edge_index.T.tolist())\n",
    "        G.add_nodes_from(range(data.num_nodes))  # 确保所有节点都在G中\n",
    "\n",
    "        # 2. 只考虑有边的节点\n",
    "        candidates = [i for i in range(data.num_nodes) if i != src and i != dst and G.degree(i) > 0]\n",
    "        candidates_tensor = torch.tensor(candidates, device=self.device, dtype=torch.long)\n",
    "\n",
    "        # 如果src或dst本身是孤立节点，直接返回零分\n",
    "        if G.degree(src) == 0 or G.degree(dst) == 0:\n",
    "            return candidates_tensor, torch.zeros(len(candidates), device=self.device, dtype=torch.float)\n",
    "\n",
    "        # 3. Personalized PageRank（以src和dst为个性化起点，各算一次）\n",
    "        personalization_src = {n: 0 for n in G.nodes}\n",
    "        personalization_src[src] = 1\n",
    "        pr_src = nx.pagerank(G, personalization=personalization_src)\n",
    "\n",
    "        personalization_dst = {n: 0 for n in G.nodes}\n",
    "        personalization_dst[dst] = 1\n",
    "        pr_dst = nx.pagerank(G, personalization=personalization_dst)\n",
    "\n",
    "        # 4. 对每个候选节点，分别查src和dst个性化pagerank的分数，做平均\n",
    "        scores = []\n",
    "        for i in candidates:\n",
    "            s = (pr_src.get(i, 0.0) + pr_dst.get(i, 0.0)) / 2\n",
    "            scores.append(s)\n",
    "\n",
    "        # 转成torch张量\n",
    "        scores = torch.tensor(scores, device=self.device, dtype=torch.float)\n",
    "        return candidates_tensor, scores\n",
    "\n",
    "    def drnl_node_labeling(self, edge_index, src, dst, num_nodes=None):\n",
    "        # Double-radius node labeling (DRNL).\n",
    "        src, dst = (dst, src) if src > dst else (src, dst)\n",
    "        adj = to_scipy_sparse_matrix(edge_index, num_nodes=num_nodes).tocsr()\n",
    "\n",
    "        idx = list(range(src)) + list(range(src + 1, adj.shape[0]))\n",
    "        adj_wo_src = adj[idx, :][:, idx]\n",
    "\n",
    "        idx = list(range(dst)) + list(range(dst + 1, adj.shape[0]))\n",
    "        adj_wo_dst = adj[idx, :][:, idx]\n",
    "\n",
    "        dist2src = shortest_path(adj_wo_dst, directed=False, unweighted=True,\n",
    "                                 indices=src)\n",
    "        dist2src = np.insert(dist2src, dst, 0, axis=0)\n",
    "        dist2src = torch.from_numpy(dist2src)\n",
    "\n",
    "        dist2dst = shortest_path(adj_wo_src, directed=False, unweighted=True,\n",
    "                                 indices=dst - 1)\n",
    "        dist2dst = np.insert(dist2dst, src, 0, axis=0)\n",
    "        dist2dst = torch.from_numpy(dist2dst)\n",
    "\n",
    "        dist = dist2src + dist2dst\n",
    "        dist_over_2, dist_mod_2 = dist // 2, dist % 2\n",
    "\n",
    "        z = 1 + torch.min(dist2src, dist2dst)\n",
    "        z += dist_over_2 * (dist_over_2 + dist_mod_2 - 1)\n",
    "        z[src] = 1.\n",
    "        z[dst] = 1.\n",
    "        z[torch.isnan(z)] = 0.\n",
    "\n",
    "        self._max_z = max(int(z.max()), self._max_z)\n",
    "\n",
    "        return z.to(torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae430094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and predictor loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "model = ModelClass(config.data_init_num_features, hidden_dim = config.scoregnn.hidden_dim, \n",
    "                 output_dim = config.scoregnn.output_dim , num_layers = config.scoregnn.num_layers, \n",
    "                 dropout = config.scoregnn.dropout).to(device)\n",
    "predictor = config.scoregnn.predictor.to(device)\n",
    "\n",
    "# 加载参数（假设你的文件结构是这样保存的）\n",
    "checkpoint = torch.load('./model/scoregnn.pth', map_location=device)\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "predictor.load_state_dict(checkpoint['predictor'])\n",
    "\n",
    "model.eval()\n",
    "predictor.eval()\n",
    "print(\"Model and predictor loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45f70e5c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T21:13:25.397099Z",
     "iopub.status.busy": "2025-05-28T21:13:25.397099Z",
     "iopub.status.idle": "2025-05-28T21:13:25.483677Z",
     "shell.execute_reply": "2025-05-28T21:13:25.483677Z"
    }
   },
   "outputs": [],
   "source": [
    "# model = ModelClass(config.data_init_num_features, hidden_dim = config.scoregnn.hidden_dim, \n",
    "#                  output_dim = config.scoregnn.output_dim , num_layers = config.scoregnn.num_layers, \n",
    "#                  dropout = config.scoregnn.dropout).to(device)\n",
    "# predictor = config.scoregnn.predictor\n",
    "\n",
    "# # 加载参数（假设你的文件结构是这样保存的）\n",
    "# checkpoint = torch.load('./model/scoregnn.pth', map_location=device)\n",
    "# model.load_state_dict(checkpoint['model'])\n",
    "# predictor.load_state_dict(checkpoint['predictor'])\n",
    "\n",
    "# model.eval()\n",
    "# predictor.eval()\n",
    "# print(\"Model and predictor loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2fc0b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "扫描 max_z: 100%|██████████| 16/16 [19:47<00:00, 74.24s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "扫描 max_z: 100%|██████████| 16/16 [14:02<00:00, 52.65s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "扫描 max_z: 100%|██████████| 1/1 [01:08<00:00, 68.34s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "扫描 max_z: 100%|██████████| 1/1 [00:49<00:00, 49.99s/batch]\n",
      "扫描 max_z: 100%|██████████| 2/2 [02:33<00:00, 76.74s/batch]\n",
      "扫描 max_z: 100%|██████████| 2/2 [01:42<00:00, 51.11s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n",
      "保存 train 分批文件\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "保存 ./data/Github/split\\SSSEAL_train_pos 分批文件: 100%|██████████| 158/158 [1:51:38<00:00, 42.39s/batch]\n",
      "保存 ./data/Github/split\\SSSEAL_train_neg 分批文件:  30%|██▉       | 47/158 [25:41<1:00:41, 32.81s/batch]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m      2\u001b[0m sampler \u001b[38;5;241m=\u001b[39m SubgraphBatchSampler(model \u001b[38;5;241m=\u001b[39m model, predictor \u001b[38;5;241m=\u001b[39m predictor, k_min \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mscoresampler\u001b[38;5;241m.\u001b[39mk_min, \n\u001b[0;32m      3\u001b[0m                           num_hops \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mscoresampler\u001b[38;5;241m.\u001b[39mnum_hops,save_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/split\u001b[39m\u001b[38;5;124m'\u001b[39m, alpha \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mscoresampler\u001b[38;5;241m.\u001b[39malpha, \n\u001b[0;32m      4\u001b[0m                           beta \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mscoresampler\u001b[38;5;241m.\u001b[39mbeta, gamma \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mscoresampler\u001b[38;5;241m.\u001b[39mgamma)\n\u001b[1;32m----> 5\u001b[0m \u001b[43msampler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m      7\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSample time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_time\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[3], line 127\u001b[0m, in \u001b[0;36mSubgraphBatchSampler.process\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m保存 train 分批文件\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_batches(train_data, train_data\u001b[38;5;241m.\u001b[39mpos_edge_label_index, \u001b[38;5;241m1\u001b[39m, train_pos_path, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_z)\n\u001b[1;32m--> 127\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_batches\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mneg_edge_label_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_neg_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_max_z\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m保存 val 分批文件\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_batches(val_data, val_data\u001b[38;5;241m.\u001b[39mpos_edge_label_index, \u001b[38;5;241m1\u001b[39m, val_pos_path, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_z)\n",
      "Cell \u001b[1;32mIn[3], line 57\u001b[0m, in \u001b[0;36mSubgraphBatchSampler.save_batches\u001b[1;34m(self, data, edge_label_index, y, out_prefix, max_z, batch_size)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_data \u001b[38;5;129;01min\u001b[39;00m batch_data_list:\n\u001b[0;32m     56\u001b[0m     batch_data\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mone_hot(batch_data\u001b[38;5;241m.\u001b[39mz, max_z \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat)\n\u001b[1;32m---> 57\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_data_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mout_prefix\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_batch\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43midx\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m batch_data_list\n\u001b[0;32m     59\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect()\n",
      "File \u001b[1;32mc:\\Users\\86186\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\serialization.py:628\u001b[0m, in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[0;32m    626\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m    627\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m--> 628\u001b[0m         \u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_disable_byteorder_record\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    629\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\86186\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\serialization.py:859\u001b[0m, in \u001b[0;36m_save\u001b[1;34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[0m\n\u001b[0;32m    855\u001b[0m \u001b[38;5;66;03m# given that we copy things around anyway, we might use storage.cpu()\u001b[39;00m\n\u001b[0;32m    856\u001b[0m \u001b[38;5;66;03m# this means to that to get tensors serialized, you need to implement\u001b[39;00m\n\u001b[0;32m    857\u001b[0m \u001b[38;5;66;03m# .cpu() on the underlying Storage\u001b[39;00m\n\u001b[0;32m    858\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m storage\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 859\u001b[0m     storage \u001b[38;5;241m=\u001b[39m \u001b[43mstorage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[38;5;66;03m# Now that it is on the CPU we can directly copy it into the zip file\u001b[39;00m\n\u001b[0;32m    861\u001b[0m num_bytes \u001b[38;5;241m=\u001b[39m storage\u001b[38;5;241m.\u001b[39mnbytes()\n",
      "File \u001b[1;32mc:\\Users\\86186\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\storage.py:137\u001b[0m, in \u001b[0;36m_StorageBase.cpu\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return a CPU copy of this storage if it's not already on the CPU.\"\"\"\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mUntypedStorage\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "sampler = SubgraphBatchSampler(model = model, predictor = predictor, k_min = config.scoresampler.k_min, \n",
    "                          num_hops = config.scoresampler.num_hops,save_dir = f'./data/{config.dataset}/split', alpha = config.scoresampler.alpha, \n",
    "                          beta = config.scoresampler.beta, gamma = config.scoresampler.gamma)\n",
    "sampler.process()\n",
    "end_time = time.time()\n",
    "logging.info(f'Sample time: {end_time - start_time}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0821f5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = SubgraphBatchSampler(model = model, predictor = predictor, k_min = config.scoresampler.k_min, \n",
    "                          num_hops = config.scoresampler.num_hops,save_dir = f'./data/{config.dataset}/split', alpha = config.scoresampler.alpha, \n",
    "                          beta = config.scoresampler.beta, gamma = config.scoresampler.gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9165258a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampler.cancat_pos_neg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c19215e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampler.cancat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbb03a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T21:13:25.483677Z",
     "iopub.status.busy": "2025-05-28T21:13:25.483677Z",
     "iopub.status.idle": "2025-05-28T21:15:53.508600Z",
     "shell.execute_reply": "2025-05-28T21:15:53.508600Z"
    }
   },
   "outputs": [],
   "source": [
    "# start_time = time.time()\n",
    "# sampler = SubgraphSampler(model = model, predictor = predictor, k_min = config.scoresampler.k_min, \n",
    "#                           num_hops = config.scoresampler.num_hops, alpha = config.scoresampler.alpha, \n",
    "#                           beta = config.scoresampler.beta, gamma = config.scoresampler.gamma)\n",
    "# sampler.process()\n",
    "# end_time = time.time()\n",
    "# logging.info(f'Sample time: {end_time - start_time}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
