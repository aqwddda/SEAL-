{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fba3828",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T20:36:45.506759Z",
     "iopub.status.busy": "2025-05-28T20:36:45.506759Z",
     "iopub.status.idle": "2025-05-28T20:36:52.208089Z",
     "shell.execute_reply": "2025-05-28T20:36:52.208089Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import time, os\n",
    "import logging\n",
    "from config import Config\n",
    "import numpy as np\n",
    "import torch\n",
    "import out_manager as om\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from torch.nn import BCEWithLogitsLoss, Conv1d, MaxPool1d, ModuleList\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import MLP, GCNConv, SortAggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d75be216",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T20:36:52.208089Z",
     "iopub.status.busy": "2025-05-28T20:36:52.208089Z",
     "iopub.status.idle": "2025-05-28T20:36:52.242895Z",
     "shell.execute_reply": "2025-05-28T20:36:52.242895Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration saved to: ./out\\Cora_k60_hop3_Batch\\config.json\n"
     ]
    }
   ],
   "source": [
    "config = Config()\n",
    "out_dir = om.get_existing_out_dir(config)\n",
    "om.save_config(config, out_dir)\n",
    "om.setup_logging(os.path.join(out_dir, \"ssseal_log.txt\"))\n",
    "seed = config.seed\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "device = config.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f9acf1f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T20:36:52.245409Z",
     "iopub.status.busy": "2025-05-28T20:36:52.245409Z",
     "iopub.status.idle": "2025-05-28T20:36:55.835045Z",
     "shell.execute_reply": "2025-05-28T20:36:55.835045Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data = torch.load(f'./data/{config.dataset}/split/ssseal_train_data_k{config.scoresampler.k_min}_h{config.scoresampler.num_hops}_{config.version}.pt')\n",
    "val_data = torch.load(f'./data/{config.dataset}/split/ssseal_val_data_k{config.scoresampler.k_min}_h{config.scoresampler.num_hops}_{config.version}.pt')\n",
    "test_data = torch.load(f'./data/{config.dataset}/split/ssseal_test_data_k{config.scoresampler.k_min}_h{config.scoresampler.num_hops}_{config.version}.pt')\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=32)\n",
    "test_loader = DataLoader(test_data, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88a05a3e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T20:36:55.835045Z",
     "iopub.status.busy": "2025-05-28T20:36:55.835045Z",
     "iopub.status.idle": "2025-05-28T20:36:55.845862Z",
     "shell.execute_reply": "2025-05-28T20:36:55.845862Z"
    }
   },
   "outputs": [],
   "source": [
    "class DGCNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_dim, num_layers, GNN=GCNConv, k=0.6):\n",
    "        super().__init__()\n",
    "\n",
    "        if k < 1:  # Transform percentile to number.\n",
    "            num_nodes = sorted([data.num_nodes for data in train_data])\n",
    "            k = num_nodes[int(math.ceil(k * len(num_nodes))) - 1]\n",
    "            k = int(max(10, k))\n",
    "\n",
    "        self.convs = ModuleList()\n",
    "        self.convs.append(GNN(train_data[0].x.size(1), hidden_dim))\n",
    "        for i in range(0, num_layers - 1):\n",
    "            self.convs.append(GNN(hidden_dim, hidden_dim))\n",
    "        self.convs.append(GNN(hidden_dim, 1))\n",
    "\n",
    "        conv1d_channels = [16, 32]\n",
    "        total_latent_dim = hidden_dim * num_layers + 1\n",
    "        conv1d_kws = [total_latent_dim, 5]\n",
    "        self.conv1 = Conv1d(1, conv1d_channels[0], conv1d_kws[0],\n",
    "                            conv1d_kws[0])\n",
    "        self.pool = SortAggregation(k)\n",
    "        self.maxpool1d = MaxPool1d(2, 2)\n",
    "        self.conv2 = Conv1d(conv1d_channels[0], conv1d_channels[1],\n",
    "                            conv1d_kws[1], 1)\n",
    "        dense_dim = int((k - 2) / 2 + 1)\n",
    "        dense_dim = (dense_dim - conv1d_kws[1] + 1) * conv1d_channels[1]\n",
    "        self.mlp = MLP([dense_dim, 128, 1], dropout=0.5, norm=None)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        xs = [x]\n",
    "        for conv in self.convs:\n",
    "            xs += [conv(xs[-1], edge_index).tanh()]\n",
    "        x = torch.cat(xs[1:], dim=-1)\n",
    "\n",
    "        # Global pooling.\n",
    "        x = self.pool(x, batch)\n",
    "        x = x.unsqueeze(1)  # [num_graphs, 1, k * hidden]\n",
    "        x = self.conv1(x).relu()\n",
    "        x = self.maxpool1d(x)\n",
    "        x = self.conv2(x).relu()\n",
    "        x = x.view(x.size(0), -1)  # [num_graphs, dense_dim]\n",
    "\n",
    "        return self.mlp(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfa4159b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T20:36:55.845862Z",
     "iopub.status.busy": "2025-05-28T20:36:55.845862Z",
     "iopub.status.idle": "2025-05-28T20:36:55.956725Z",
     "shell.execute_reply": "2025-05-28T20:36:55.956725Z"
    }
   },
   "outputs": [],
   "source": [
    "model = DGCNN(hidden_dim = config.ssseal.hidden_dim, num_layers = config.ssseal.num_layers, k = config.ssseal.k).to(device)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr = config.ssseal.lr)\n",
    "loss_fn = BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c860f997",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T20:36:55.956725Z",
     "iopub.status.busy": "2025-05-28T20:36:55.956725Z",
     "iopub.status.idle": "2025-05-28T20:36:55.963909Z",
     "shell.execute_reply": "2025-05-28T20:36:55.963909Z"
    }
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        data.batch = data.batch.long()\n",
    "        data.edge_index = data.edge_index.long()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index, data.batch)\n",
    "        loss = loss_fn(out.view(-1), data.y.to(torch.float))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += float(loss) * data.num_graphs\n",
    "\n",
    "    return total_loss / len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ea00f65",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T20:36:55.963909Z",
     "iopub.status.busy": "2025-05-28T20:36:55.963909Z",
     "iopub.status.idle": "2025-05-28T20:36:55.970741Z",
     "shell.execute_reply": "2025-05-28T20:36:55.970741Z"
    }
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "\n",
    "    y_pred, y_true = [], []\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        data.batch = data.batch.long()\n",
    "        data.edge_index = data.edge_index.long()\n",
    "        logits = model(data.x, data.edge_index, data.batch)\n",
    "        y_pred.append(logits.view(-1).cpu())\n",
    "        y_true.append(data.y.view(-1).cpu().to(torch.float))\n",
    "\n",
    "    return roc_auc_score(torch.cat(y_true), torch.cat(y_pred)), average_precision_score(torch.cat(y_true),torch.cat(y_pred) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e97de319",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T20:36:55.972496Z",
     "iopub.status.busy": "2025-05-28T20:36:55.972496Z",
     "iopub.status.idle": "2025-05-28T20:48:45.114133Z",
     "shell.execute_reply": "2025-05-28T20:48:45.113630Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 0.6455 Train_AUC: 0.7464, Train_AP: 0.7632 Val_AUC: 0.7251, Val_AP: 0.7461 Test_AUC: 0.7681, Test_AP: 0.7854\n",
      "Epoch: 002, Loss: 0.5748 Train_AUC: 0.7791, Train_AP: 0.7957 Val_AUC: 0.7603, Val_AP: 0.7805 Test_AUC: 0.8070, Test_AP: 0.8196\n",
      "Epoch: 003, Loss: 0.5556 Train_AUC: 0.8002, Train_AP: 0.8173 Val_AUC: 0.7849, Val_AP: 0.8085 Test_AUC: 0.8185, Test_AP: 0.8365\n",
      "Epoch: 004, Loss: 0.5417 Train_AUC: 0.8044, Train_AP: 0.8234 Val_AUC: 0.7823, Val_AP: 0.8054 Test_AUC: 0.8211, Test_AP: 0.8399\n",
      "Epoch: 005, Loss: 0.5351 Train_AUC: 0.8095, Train_AP: 0.8305 Val_AUC: 0.7906, Val_AP: 0.8141 Test_AUC: 0.8241, Test_AP: 0.8448\n",
      "Epoch: 006, Loss: 0.5289 Train_AUC: 0.8167, Train_AP: 0.8396 Val_AUC: 0.8029, Val_AP: 0.8261 Test_AUC: 0.8309, Test_AP: 0.8529\n",
      "Epoch: 007, Loss: 0.5211 Train_AUC: 0.8288, Train_AP: 0.8474 Val_AUC: 0.8184, Val_AP: 0.8343 Test_AUC: 0.8393, Test_AP: 0.8570\n",
      "Epoch: 008, Loss: 0.5148 Train_AUC: 0.8319, Train_AP: 0.8529 Val_AUC: 0.8243, Val_AP: 0.8401 Test_AUC: 0.8413, Test_AP: 0.8616\n",
      "Epoch: 009, Loss: 0.5072 Train_AUC: 0.8389, Train_AP: 0.8577 Val_AUC: 0.8366, Val_AP: 0.8485 Test_AUC: 0.8491, Test_AP: 0.8653\n",
      "Epoch: 010, Loss: 0.4994 Train_AUC: 0.8396, Train_AP: 0.8597 Val_AUC: 0.8414, Val_AP: 0.8519 Test_AUC: 0.8576, Test_AP: 0.8717\n",
      "Epoch: 011, Loss: 0.4939 Train_AUC: 0.8446, Train_AP: 0.8650 Val_AUC: 0.8437, Val_AP: 0.8568 Test_AUC: 0.8610, Test_AP: 0.8762\n",
      "Epoch: 012, Loss: 0.4878 Train_AUC: 0.8483, Train_AP: 0.8682 Val_AUC: 0.8472, Val_AP: 0.8594 Test_AUC: 0.8649, Test_AP: 0.8790\n",
      "Epoch: 013, Loss: 0.4825 Train_AUC: 0.8496, Train_AP: 0.8685 Val_AUC: 0.8457, Val_AP: 0.8581 Test_AUC: 0.8677, Test_AP: 0.8794\n",
      "Epoch: 014, Loss: 0.4769 Train_AUC: 0.8526, Train_AP: 0.8715 Val_AUC: 0.8501, Val_AP: 0.8612 Test_AUC: 0.8673, Test_AP: 0.8806\n",
      "Epoch: 015, Loss: 0.4768 Train_AUC: 0.8544, Train_AP: 0.8735 Val_AUC: 0.8525, Val_AP: 0.8638 Test_AUC: 0.8680, Test_AP: 0.8819\n",
      "Epoch: 016, Loss: 0.4725 Train_AUC: 0.8584, Train_AP: 0.8779 Val_AUC: 0.8553, Val_AP: 0.8681 Test_AUC: 0.8697, Test_AP: 0.8838\n",
      "Epoch: 017, Loss: 0.4691 Train_AUC: 0.8594, Train_AP: 0.8795 Val_AUC: 0.8583, Val_AP: 0.8703 Test_AUC: 0.8698, Test_AP: 0.8853\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m config\u001b[38;5;241m.\u001b[39mssseal\u001b[38;5;241m.\u001b[39mepochs):\n\u001b[0;32m      5\u001b[0m     start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m----> 6\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m     train_auc, train_ap \u001b[38;5;241m=\u001b[39m test(train_loader)\n\u001b[0;32m      8\u001b[0m     val_auc, val_ap \u001b[38;5;241m=\u001b[39m test(val_loader)\n",
      "Cell \u001b[1;32mIn[6], line 10\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m data\u001b[38;5;241m.\u001b[39medge_index \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39medge_index\u001b[38;5;241m.\u001b[39mlong()\n\u001b[0;32m      9\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 10\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(out\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), data\u001b[38;5;241m.\u001b[39my\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat))\n\u001b[0;32m     12\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\86186\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\86186\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[4], line 32\u001b[0m, in \u001b[0;36mDGCNN.forward\u001b[1;34m(self, x, edge_index, batch)\u001b[0m\n\u001b[0;32m     30\u001b[0m xs \u001b[38;5;241m=\u001b[39m [x]\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m conv \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvs:\n\u001b[1;32m---> 32\u001b[0m     xs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtanh()]\n\u001b[0;32m     33\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(xs[\u001b[38;5;241m1\u001b[39m:], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Global pooling.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\86186\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\86186\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\86186\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch_geometric\\nn\\conv\\gcn_conv.py:260\u001b[0m, in \u001b[0;36mGCNConv.forward\u001b[1;34m(self, x, edge_index, edge_weight)\u001b[0m\n\u001b[0;32m    257\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    258\u001b[0m             edge_index \u001b[38;5;241m=\u001b[39m cache\n\u001b[1;32m--> 260\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;66;03m# propagate_type: (x: Tensor, edge_weight: OptTensor)\u001b[39;00m\n\u001b[0;32m    263\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpropagate(edge_index, x\u001b[38;5;241m=\u001b[39mx, edge_weight\u001b[38;5;241m=\u001b[39medge_weight)\n",
      "File \u001b[1;32mc:\\Users\\86186\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\86186\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\86186\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch_geometric\\nn\\dense\\linear.py:147\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m    142\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Forward pass.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \n\u001b[0;32m    144\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;124;03m        x (torch.Tensor): The input features.\u001b[39;00m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "times = []\n",
    "best_val_auc = final_test_auc = final_test_ap = 0\n",
    "\n",
    "for epoch in range(1, 1 + config.ssseal.epochs):\n",
    "    start = time.time()\n",
    "    loss = train()\n",
    "    train_auc, train_ap = test(train_loader)\n",
    "    val_auc, val_ap = test(val_loader)\n",
    "    test_auc, test_ap = test(test_loader)\n",
    "    if val_auc > best_val_auc:\n",
    "        best_val_auc = val_auc\n",
    "        final_test_auc = test_auc\n",
    "        final_test_ap = test_ap\n",
    "\n",
    "    logging.info(f'Epoch: {epoch:03d}, Loss: {loss:.4f} '\n",
    "             f'Train_AUC: {train_auc:.4f}, Train_AP: {train_ap:.4f} '\n",
    "             f'Val_AUC: {val_auc:.4f}, Val_AP: {val_ap:.4f} '\n",
    "             f'Test_AUC: {test_auc:.4f}, Test_AP: {test_ap:.4f}')\n",
    "    times.append(time.time() - start)\n",
    "    \n",
    "logging.info(f'Median time per epoch: {torch.tensor(times).median():.4f}s'\n",
    "             f'Final Test AUC: {final_test_auc:.4f}, AP: {final_test_ap:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
