{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fba3828",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "from itertools import chain\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from scipy.sparse.csgraph import shortest_path\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from torch.nn import BCEWithLogitsLoss, Conv1d, MaxPool1d, ModuleList\n",
    "from torch_geometric.data import Data, InMemoryDataset\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import MLP, GCNConv, SortAggregation\n",
    "from torch_geometric.transforms import RandomLinkSplit\n",
    "from torch_geometric.utils import k_hop_subgraph, to_scipy_sparse_matrix\n",
    "\n",
    "seed = 2025\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9acf1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch_geometric.loader.dataloader.DataLoader object at 0x00000209C67A60C0>\n"
     ]
    }
   ],
   "source": [
    "train_data = torch.load('./data/Cora/split/ssseal_train_data.pt')\n",
    "val_data = torch.load('./data/Cora/split/ssseal_val_data.pt')\n",
    "test_data = torch.load('./data/Cora/split/ssseal_test_data.pt')\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=32)\n",
    "test_loader = DataLoader(test_data, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88a05a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DGCNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_dim, num_layers, GNN=GCNConv, k=0.6):\n",
    "        super().__init__()\n",
    "\n",
    "        if k < 1:  # Transform percentile to number.\n",
    "            num_nodes = sorted([data.num_nodes for data in train_data])\n",
    "            k = num_nodes[int(math.ceil(k * len(num_nodes))) - 1]\n",
    "            k = int(max(10, k))\n",
    "\n",
    "        self.convs = ModuleList()\n",
    "        self.convs.append(GNN(train_data[0].x.size(1), hidden_dim))\n",
    "        for i in range(0, num_layers - 1):\n",
    "            self.convs.append(GNN(hidden_dim, hidden_dim))\n",
    "        self.convs.append(GNN(hidden_dim, 1))\n",
    "\n",
    "        conv1d_channels = [16, 32]\n",
    "        total_latent_dim = hidden_dim * num_layers + 1\n",
    "        conv1d_kws = [total_latent_dim, 5]\n",
    "        self.conv1 = Conv1d(1, conv1d_channels[0], conv1d_kws[0],\n",
    "                            conv1d_kws[0])\n",
    "        self.pool = SortAggregation(k)\n",
    "        self.maxpool1d = MaxPool1d(2, 2)\n",
    "        self.conv2 = Conv1d(conv1d_channels[0], conv1d_channels[1],\n",
    "                            conv1d_kws[1], 1)\n",
    "        dense_dim = int((k - 2) / 2 + 1)\n",
    "        dense_dim = (dense_dim - conv1d_kws[1] + 1) * conv1d_channels[1]\n",
    "        self.mlp = MLP([dense_dim, 128, 1], dropout=0.5, norm=None)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        xs = [x]\n",
    "        for conv in self.convs:\n",
    "            xs += [conv(xs[-1], edge_index).tanh()]\n",
    "        x = torch.cat(xs[1:], dim=-1)\n",
    "\n",
    "        # Global pooling.\n",
    "        x = self.pool(x, batch)\n",
    "        x = x.unsqueeze(1)  # [num_graphs, 1, k * hidden]\n",
    "        x = self.conv1(x).relu()\n",
    "        x = self.maxpool1d(x)\n",
    "        x = self.conv2(x).relu()\n",
    "        x = x.view(x.size(0), -1)  # [num_graphs, dense_dim]\n",
    "\n",
    "        return self.mlp(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cfa4159b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DGCNN(hidden_dim=32, num_layers=3).to(device)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.0001)\n",
    "loss_fn = BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c860f997",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        data.batch = data.batch.long()\n",
    "        data.edge_index = data.edge_index.long()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index, data.batch)\n",
    "        loss = loss_fn(out.view(-1), data.y.to(torch.float))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += float(loss) * data.num_graphs\n",
    "\n",
    "    return total_loss / len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8ea00f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "\n",
    "    y_pred, y_true = [], []\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        data.batch = data.batch.long()\n",
    "        data.edge_index = data.edge_index.long()\n",
    "        logits = model(data.x, data.edge_index, data.batch)\n",
    "        y_pred.append(logits.view(-1).cpu())\n",
    "        y_true.append(data.y.view(-1).cpu().to(torch.float))\n",
    "\n",
    "    return roc_auc_score(torch.cat(y_true), torch.cat(y_pred)), average_precision_score(torch.cat(y_true),torch.cat(y_pred) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e97de319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Loss: 0.6821, Val_AUC: 0.6044, Val_AP: 0.6239Test_AUC: 0.6078, Test_AP: 0.6051\n",
      "Epoch: 02, Loss: 0.6651, Val_AUC: 0.6383, Val_AP: 0.6557Test_AUC: 0.6443, Test_AP: 0.6322\n",
      "Epoch: 03, Loss: 0.6413, Val_AUC: 0.6672, Val_AP: 0.6831Test_AUC: 0.6770, Test_AP: 0.6785\n",
      "Epoch: 04, Loss: 0.6166, Val_AUC: 0.6815, Val_AP: 0.6811Test_AUC: 0.6866, Test_AP: 0.6788\n",
      "Epoch: 05, Loss: 0.6011, Val_AUC: 0.6944, Val_AP: 0.7058Test_AUC: 0.6946, Test_AP: 0.7025\n",
      "Epoch: 06, Loss: 0.5832, Val_AUC: 0.7169, Val_AP: 0.7139Test_AUC: 0.7195, Test_AP: 0.7160\n",
      "Epoch: 07, Loss: 0.5656, Val_AUC: 0.7115, Val_AP: 0.7053Test_AUC: 0.7195, Test_AP: 0.7160\n",
      "Epoch: 08, Loss: 0.5368, Val_AUC: 0.7278, Val_AP: 0.7334Test_AUC: 0.7422, Test_AP: 0.7323\n",
      "Epoch: 09, Loss: 0.5190, Val_AUC: 0.7477, Val_AP: 0.7539Test_AUC: 0.7742, Test_AP: 0.7677\n",
      "Epoch: 10, Loss: 0.5037, Val_AUC: 0.7534, Val_AP: 0.7559Test_AUC: 0.7728, Test_AP: 0.7667\n",
      "Epoch: 11, Loss: 0.4869, Val_AUC: 0.7499, Val_AP: 0.7528Test_AUC: 0.7728, Test_AP: 0.7667\n",
      "Epoch: 12, Loss: 0.4801, Val_AUC: 0.7506, Val_AP: 0.7530Test_AUC: 0.7728, Test_AP: 0.7667\n",
      "Epoch: 13, Loss: 0.4689, Val_AUC: 0.7641, Val_AP: 0.7704Test_AUC: 0.7850, Test_AP: 0.7767\n",
      "Epoch: 14, Loss: 0.4615, Val_AUC: 0.7539, Val_AP: 0.7612Test_AUC: 0.7850, Test_AP: 0.7767\n",
      "Epoch: 15, Loss: 0.4534, Val_AUC: 0.7554, Val_AP: 0.7366Test_AUC: 0.7850, Test_AP: 0.7767\n",
      "Epoch: 16, Loss: 0.4483, Val_AUC: 0.7611, Val_AP: 0.7658Test_AUC: 0.7850, Test_AP: 0.7767\n",
      "Epoch: 17, Loss: 0.4456, Val_AUC: 0.7633, Val_AP: 0.7730Test_AUC: 0.7850, Test_AP: 0.7767\n",
      "Epoch: 18, Loss: 0.4385, Val_AUC: 0.7656, Val_AP: 0.7620Test_AUC: 0.7912, Test_AP: 0.7823\n",
      "Epoch: 19, Loss: 0.4342, Val_AUC: 0.7690, Val_AP: 0.7687Test_AUC: 0.7902, Test_AP: 0.7829\n",
      "Epoch: 20, Loss: 0.4306, Val_AUC: 0.7759, Val_AP: 0.7780Test_AUC: 0.7964, Test_AP: 0.7968\n",
      "Epoch: 21, Loss: 0.4281, Val_AUC: 0.7772, Val_AP: 0.7769Test_AUC: 0.7925, Test_AP: 0.7890\n",
      "Epoch: 22, Loss: 0.4212, Val_AUC: 0.7703, Val_AP: 0.7601Test_AUC: 0.7925, Test_AP: 0.7890\n",
      "Epoch: 23, Loss: 0.4182, Val_AUC: 0.7765, Val_AP: 0.7788Test_AUC: 0.7925, Test_AP: 0.7890\n",
      "Epoch: 24, Loss: 0.4143, Val_AUC: 0.7737, Val_AP: 0.7747Test_AUC: 0.7925, Test_AP: 0.7890\n",
      "Epoch: 25, Loss: 0.4115, Val_AUC: 0.7805, Val_AP: 0.7763Test_AUC: 0.7877, Test_AP: 0.7873\n",
      "Epoch: 26, Loss: 0.4105, Val_AUC: 0.7828, Val_AP: 0.7892Test_AUC: 0.7967, Test_AP: 0.8008\n",
      "Epoch: 27, Loss: 0.4080, Val_AUC: 0.7708, Val_AP: 0.7705Test_AUC: 0.7967, Test_AP: 0.8008\n",
      "Epoch: 28, Loss: 0.4042, Val_AUC: 0.7813, Val_AP: 0.7871Test_AUC: 0.7967, Test_AP: 0.8008\n",
      "Epoch: 29, Loss: 0.4007, Val_AUC: 0.7879, Val_AP: 0.7928Test_AUC: 0.7972, Test_AP: 0.7965\n",
      "Epoch: 30, Loss: 0.3985, Val_AUC: 0.7823, Val_AP: 0.7950Test_AUC: 0.7972, Test_AP: 0.7965\n",
      "Epoch: 31, Loss: 0.3983, Val_AUC: 0.7932, Val_AP: 0.8007Test_AUC: 0.7974, Test_AP: 0.7974\n",
      "Epoch: 32, Loss: 0.3992, Val_AUC: 0.7889, Val_AP: 0.7960Test_AUC: 0.7974, Test_AP: 0.7974\n",
      "Epoch: 33, Loss: 0.3980, Val_AUC: 0.7970, Val_AP: 0.8127Test_AUC: 0.8007, Test_AP: 0.7992\n",
      "Epoch: 34, Loss: 0.3916, Val_AUC: 0.7895, Val_AP: 0.8003Test_AUC: 0.8007, Test_AP: 0.7992\n",
      "Epoch: 35, Loss: 0.3934, Val_AUC: 0.7906, Val_AP: 0.8031Test_AUC: 0.8007, Test_AP: 0.7992\n",
      "Epoch: 36, Loss: 0.3938, Val_AUC: 0.7899, Val_AP: 0.8004Test_AUC: 0.8007, Test_AP: 0.7992\n",
      "Epoch: 37, Loss: 0.3898, Val_AUC: 0.7795, Val_AP: 0.7831Test_AUC: 0.8007, Test_AP: 0.7992\n",
      "Epoch: 38, Loss: 0.3877, Val_AUC: 0.7786, Val_AP: 0.7821Test_AUC: 0.8007, Test_AP: 0.7992\n",
      "Epoch: 39, Loss: 0.3914, Val_AUC: 0.7817, Val_AP: 0.7895Test_AUC: 0.8007, Test_AP: 0.7992\n",
      "Epoch: 40, Loss: 0.3860, Val_AUC: 0.7811, Val_AP: 0.7907Test_AUC: 0.8007, Test_AP: 0.7992\n",
      "Epoch: 41, Loss: 0.3830, Val_AUC: 0.7875, Val_AP: 0.7933Test_AUC: 0.8007, Test_AP: 0.7992\n",
      "Epoch: 42, Loss: 0.3817, Val_AUC: 0.7841, Val_AP: 0.7901Test_AUC: 0.8007, Test_AP: 0.7992\n",
      "Epoch: 43, Loss: 0.3783, Val_AUC: 0.7824, Val_AP: 0.7811Test_AUC: 0.8007, Test_AP: 0.7992\n",
      "Epoch: 44, Loss: 0.3808, Val_AUC: 0.7892, Val_AP: 0.7972Test_AUC: 0.8007, Test_AP: 0.7992\n",
      "Epoch: 45, Loss: 0.3784, Val_AUC: 0.7816, Val_AP: 0.7994Test_AUC: 0.8007, Test_AP: 0.7992\n",
      "Epoch: 46, Loss: 0.3758, Val_AUC: 0.7843, Val_AP: 0.7871Test_AUC: 0.8007, Test_AP: 0.7992\n",
      "Epoch: 47, Loss: 0.3719, Val_AUC: 0.7837, Val_AP: 0.7920Test_AUC: 0.8007, Test_AP: 0.7992\n",
      "Epoch: 48, Loss: 0.3755, Val_AUC: 0.7881, Val_AP: 0.7968Test_AUC: 0.8007, Test_AP: 0.7992\n",
      "Epoch: 49, Loss: 0.3708, Val_AUC: 0.7850, Val_AP: 0.7945Test_AUC: 0.8007, Test_AP: 0.7992\n",
      "Epoch: 50, Loss: 0.3674, Val_AUC: 0.7807, Val_AP: 0.7881Test_AUC: 0.8007, Test_AP: 0.7992\n",
      "Median time per epoch: 5.3850s\n"
     ]
    }
   ],
   "source": [
    "times = []\n",
    "best_val_auc = test_auc = 0\n",
    "for epoch in range(1, 51):\n",
    "    start = time.time()\n",
    "    loss = train()\n",
    "    val_auc, val_ap = test(val_loader)\n",
    "    if val_auc > best_val_auc:\n",
    "        best_val_auc = val_auc\n",
    "        test_auc, test_ap = test(test_loader)\n",
    "    print(f'Epoch: {epoch:02d}, Loss: {loss:.4f}, Val_AUC: {val_auc:.4f}, Val_AP: {val_ap:.4f}'\n",
    "          f'Test_AUC: {test_auc:.4f}, Test_AP: {test_ap:.4f}')\n",
    "    times.append(time.time() - start)\n",
    "print(f\"Median time per epoch: {torch.tensor(times).median():.4f}s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
