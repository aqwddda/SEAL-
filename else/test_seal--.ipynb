{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "eceb4103",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from itertools import chain\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data \n",
    "from torch_geometric.utils import k_hop_subgraph, negative_sampling\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import numpy as np\n",
    "import random\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "3fd08532",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeighborScorer(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mlp(x).squeeze(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "86ad0adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drnl_node_labeling(edge_index, src, dst, num_nodes=None):\n",
    "    from scipy.sparse.csgraph import shortest_path\n",
    "    from torch_geometric.utils import to_scipy_sparse_matrix\n",
    "\n",
    "    src, dst = (dst, src) if src > dst else (src, dst)\n",
    "    adj = to_scipy_sparse_matrix(edge_index, num_nodes=num_nodes).tocsr()\n",
    "\n",
    "    idx = list(range(src)) + list(range(src + 1, adj.shape[0]))\n",
    "    adj_wo_src = adj[idx, :][:, idx]\n",
    "\n",
    "    idx = list(range(dst)) + list(range(dst + 1, adj.shape[0]))\n",
    "    adj_wo_dst = adj[idx, :][:, idx]\n",
    "\n",
    "    dist2src = shortest_path(adj_wo_dst, directed=False, unweighted=True, indices=src)\n",
    "    dist2src = np.insert(dist2src, dst, 0, axis=0)\n",
    "    dist2src = torch.from_numpy(dist2src)\n",
    "\n",
    "    dist2dst = shortest_path(adj_wo_src, directed=False, unweighted=True, indices=dst - 1)\n",
    "    dist2dst = np.insert(dist2dst, src, 0, axis=0)\n",
    "    dist2dst = torch.from_numpy(dist2dst)\n",
    "\n",
    "    dist = dist2src + dist2dst\n",
    "    dist_over_2, dist_mod_2 = dist // 2, dist % 2\n",
    "\n",
    "    z = 1 + torch.min(dist2src, dist2dst)\n",
    "    z += dist_over_2 * (dist_over_2 + dist_mod_2 - 1)\n",
    "    z[src] = 1.\n",
    "    z[dst] = 1.\n",
    "    z[torch.isnan(z)] = 0.\n",
    "    maxz = 0\n",
    "    maxz = max(int(z.max()), maxz)\n",
    "    return z.to(torch.long).to(device), maxz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "0cf445a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamic_prune_subgraph(data, y, num_hops, scorer, top_n):\n",
    "    data_list = []\n",
    "    maxz_total = 0\n",
    "    edge_label_index = data.pos_edge_label_index if y == 1 else data.neg_edge_label_index\n",
    "\n",
    "    for src, dst in edge_label_index.t().tolist():\n",
    "        node_idx, edge_index, mapping, _ = k_hop_subgraph(\n",
    "            [src, dst], num_hops, data.edge_index, relabel_nodes=True)\n",
    "        src_sub, dst_sub = mapping.tolist()\n",
    "        x_sub = data.x[node_idx.to(data.x.device)]\n",
    "        all_indices = torch.arange(len(node_idx))\n",
    "        mask_src_dst = torch.ones(len(node_idx), dtype=torch.bool)\n",
    "        mask_src_dst[src_sub] = False\n",
    "        mask_src_dst[dst_sub] = False\n",
    "\n",
    "        x_sub_wo_srcdst = x_sub[mask_src_dst]\n",
    "        with torch.no_grad():\n",
    "            score = scorer(x_sub_wo_srcdst)\n",
    "        n = min(top_n, x_sub_wo_srcdst.size(0))\n",
    "        idx_top = score.topk(n).indices\n",
    "        neighbors_idx = all_indices[mask_src_dst]\n",
    "        topn_in_nodeidx = neighbors_idx[idx_top.to(neighbors_idx.device)]\n",
    "        indices_final = torch.cat([torch.tensor([src_sub, dst_sub]), topn_in_nodeidx])\n",
    "        x_sub_final = x_sub[indices_final]\n",
    "        node_idx_final = node_idx[indices_final]\n",
    "\n",
    "        final_set = set(indices_final.tolist())\n",
    "        edge_mask = [(u in final_set) and (v in final_set) for u, v in edge_index.t().tolist()]\n",
    "        edge_mask = torch.tensor(edge_mask, dtype=torch.bool)\n",
    "        edge_index_filtered = edge_index[:, edge_mask]\n",
    "        old2new = {old.item(): new for new, old in enumerate(indices_final)}\n",
    "        edge_index_final = edge_index_filtered.clone()\n",
    "        for i in range(edge_index_filtered.shape[1]):\n",
    "            edge_index_final[0, i] = old2new[edge_index_filtered[0, i].item()]\n",
    "            edge_index_final[1, i] = old2new[edge_index_filtered[1, i].item()]\n",
    "\n",
    "        src_new, dst_new = 0, 1\n",
    "        mask1 = ~((edge_index_final[0] == src_new) & (edge_index_final[1] == dst_new))\n",
    "        mask2 = ~((edge_index_final[0] == dst_new) & (edge_index_final[1] == src_new))\n",
    "        mask = mask1 & mask2\n",
    "\n",
    "        edge_index_final_no_sd = edge_index_final[:, mask]\n",
    "\n",
    "        z, maxz = drnl_node_labeling(edge_index_final_no_sd, src_new, dst_new, num_nodes=node_idx_final.size(0))\n",
    "        maxz_total = max(maxz_total, maxz)\n",
    "        data_graph = Data(x=x_sub_final, z=z, edge_index=edge_index_final_no_sd, y=y)\n",
    "        data_list.append(data_graph)\n",
    "    return data_list, maxz_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "e3c46ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def dynamic_prune_subgraph(data, y, num_hops, scorer, top_n):\n",
    "#     data_list = []\n",
    "#     if y == 1:\n",
    "#         for src, dst in data.pos_edge_label_index.t().tolist():\n",
    "#             node_idx, edge_index, mapping, _ = k_hop_subgraph(\n",
    "#                 [src, dst], num_hops, data.edge_index, relabel_nodes=True)\n",
    "#             src, dst = mapping.tolist()\n",
    "#             x_sub = data.x[node_idx]\n",
    "#             all_indices = torch.arange(len(node_idx))\n",
    "#             mask_src_dst = torch.ones(len(node_idx), dtype=torch.bool)\n",
    "#             mask_src_dst[src] = False\n",
    "#             mask_src_dst[dst] = False\n",
    "#             x_sub_wo_srcdst = x_sub[mask_src_dst]\n",
    "#             with torch.no_grad():\n",
    "#                 score = scorer(x_sub_wo_srcdst)\n",
    "\n",
    "#             n = min(top_n, x_sub_wo_srcdst.size(0))\n",
    "#             idx_top = score.topk(n).indices\n",
    "#             neighbors_idx = all_indices[mask_src_dst]\n",
    "#             topn_in_nodeidx = neighbors_idx[idx_top]\n",
    "#             indices_final = torch.cat([torch.tensor([src, dst]), topn_in_nodeidx])\n",
    "#             x_sub_final = x_sub[indices_final]\n",
    "#             node_idx_final = node_idx[indices_final]\n",
    "\n",
    "#             # 3. 保留只与这些节点相关的边\n",
    "#             # 假设你已经得到 indices_final\n",
    "#             final_set = set(indices_final.tolist())\n",
    "#             edge_mask = [(u in final_set) and (v in final_set) for u, v in edge_index.t().tolist()]\n",
    "#             edge_mask = torch.tensor(edge_mask, dtype=torch.bool)\n",
    "#             edge_index_filtered = edge_index[:, edge_mask]\n",
    "\n",
    "#             # old -> new 编号\n",
    "#             old2new = {old.item(): new for new, old in enumerate(indices_final)}\n",
    "\n",
    "#             edge_index_final = edge_index_filtered.clone()\n",
    "#             for i in range(edge_index_filtered.shape[1]):\n",
    "#                 edge_index_final[0, i] = old2new[edge_index_filtered[0, i].item()]\n",
    "#                 edge_index_final[1, i] = old2new[edge_index_filtered[1, i].item()]\n",
    "\n",
    "#             # 删除 src-dst 连接\n",
    "#             src_new, dst_new = 0, 1\n",
    "#             mask1 = ~((edge_index_final[0] == src_new) & (edge_index_final[1] == dst_new))\n",
    "#             mask2 = ~((edge_index_final[0] == dst_new) & (edge_index_final[1] == src_new))\n",
    "#             mask = mask1 & mask2\n",
    "\n",
    "#             edge_index_final_no_sd = edge_index_final[:, mask]\n",
    "\n",
    "#             z, maxz = drnl_node_labeling(edge_index_final_no_sd, src_new, dst_new, num_nodes=node_idx_final.size(0))\n",
    "#             data = Data(x = x_sub_final, z = z, edge_index = edge_index_final_no_sd, y = y)\n",
    "#             data_list.append(data)\n",
    "#     if y == 0:\n",
    "#         for src, dst in data.neg_edge_label_index.t().tolist():\n",
    "#             node_idx, edge_index, mapping, _ = k_hop_subgraph(\n",
    "#                 [src, dst], num_hops, data.edge_index, relabel_nodes=True)\n",
    "#             src, dst = mapping.tolist()\n",
    "#             x_sub = data.x[node_idx]\n",
    "#             all_indices = torch.arange(len(node_idx))\n",
    "#             mask_src_dst = torch.ones(len(node_idx), dtype=torch.bool)\n",
    "#             mask_src_dst[src] = False\n",
    "#             mask_src_dst[dst] = False\n",
    "#             x_sub_wo_srcdst = x_sub[mask_src_dst]\n",
    "#             with torch.no_grad():\n",
    "#                 score = scorer(x_sub_wo_srcdst)\n",
    "\n",
    "#             n = min(top_n, x_sub_wo_srcdst.size(0))\n",
    "#             idx_top = score.topk(n).indices\n",
    "#             neighbors_idx = all_indices[mask_src_dst]\n",
    "#             topn_in_nodeidx = neighbors_idx[idx_top]\n",
    "#             indices_final = torch.cat([torch.tensor([src, dst]), topn_in_nodeidx])\n",
    "#             x_sub_final = x_sub[indices_final]\n",
    "#             node_idx_final = node_idx[indices_final]\n",
    "\n",
    "#             # 3. 保留只与这些节点相关的边\n",
    "#             # 假设你已经得到 indices_final\n",
    "#             final_set = set(indices_final.tolist())\n",
    "#             edge_mask = [(u in final_set) and (v in final_set) for u, v in edge_index.t().tolist()]\n",
    "#             edge_mask = torch.tensor(edge_mask, dtype=torch.bool)\n",
    "#             edge_index_filtered = edge_index[:, edge_mask]\n",
    "\n",
    "#             # old -> new 编号\n",
    "#             old2new = {old.item(): new for new, old in enumerate(indices_final)}\n",
    "\n",
    "#             edge_index_final = edge_index_filtered.clone()\n",
    "#             for i in range(edge_index_filtered.shape[1]):\n",
    "#                 edge_index_final[0, i] = old2new[edge_index_filtered[0, i].item()]\n",
    "#                 edge_index_final[1, i] = old2new[edge_index_filtered[1, i].item()]\n",
    "\n",
    "#             # 删除 src-dst 连接\n",
    "#             src_new, dst_new = 0, 1\n",
    "#             mask1 = ~((edge_index_final[0] == src_new) & (edge_index_final[1] == dst_new))\n",
    "#             mask2 = ~((edge_index_final[0] == dst_new) & (edge_index_final[1] == src_new))\n",
    "#             mask = mask1 & mask2\n",
    "\n",
    "#             edge_index_final_no_sd = edge_index_final[:, mask]\n",
    "\n",
    "#             z, maxz = drnl_node_labeling(edge_index_final_no_sd, src_new, dst_new, num_nodes=node_idx_final.size(0))\n",
    "#             data = Data(x = x_sub_final, z = z, edge_index = edge_index_final_no_sd, y = y)\n",
    "#             data_list.append(data)\n",
    "#     return data, maxz\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "c1b4c497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[2708, 256], edge_index=[2, 8976], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708], pos_edge_label=[4488], pos_edge_label_index=[2, 4488], neg_edge_label=[4488], neg_edge_label_index=[2, 4488])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "dataset = Planetoid('./data/Planetoid', name='Cora')\n",
    "data = dataset[0].to(device)\n",
    "#随机初始化data.x\n",
    "num_nodes = data.num_nodes\n",
    "num_features = 256\n",
    "data.x = torch.randn((num_nodes, num_features)).to(device)\n",
    "data.x_z = None\n",
    "\n",
    "# 用PyG的RandomLinkSplit划分数据集\n",
    "from torch_geometric.transforms import RandomLinkSplit\n",
    "transform = RandomLinkSplit(num_val=0.05, num_test=0.1, is_undirected=True, split_labels=True)\n",
    "train_data, val_data, test_data = transform(data)\n",
    "train_data = train_data.to(device)\n",
    "val_data = val_data.to(device)\n",
    "test_data = test_data.to(device)\n",
    "\n",
    "print(train_data)\n",
    "\n",
    "# def make_edge_list(pos_edge_index, neg_edge_index):\n",
    "#     pos_list = [(int(src), int(dst), 1) for src, dst in pos_edge_index.t()]\n",
    "#     neg_list = [(int(src), int(dst), 0) for src, dst in neg_edge_index.t()]\n",
    "#     return pos_list + neg_list\n",
    "\n",
    "# train_edges = make_edge_list(train_data.pos_edge_label_index, train_data.neg_edge_label_index)\n",
    "# val_edges = make_edge_list(val_data.pos_edge_label_index, val_data.neg_edge_label_index)\n",
    "# test_edges = make_edge_list(test_data.pos_edge_label_index, test_data.neg_edge_label_index)\n",
    "\n",
    "# random.shuffle(train_edges)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "00277c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DGCNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, k=20):\n",
    "        super().__init__()\n",
    "        from torch_geometric.nn import GCNConv, global_sort_pool\n",
    "        self.convs = nn.ModuleList([GCNConv(input_dim, hidden_dim)])\n",
    "        for _ in range(num_layers-1):\n",
    "            self.convs.append(GCNConv(hidden_dim, hidden_dim))\n",
    "        self.pool = global_sort_pool\n",
    "        self.k = k\n",
    "        self.lin = nn.Linear(hidden_dim * self.k, 1)  # 修改这里！\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        for conv in self.convs:\n",
    "            x = conv(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "        x = self.pool(x, batch, self.k)  # [batch_size, k * hidden_dim]\n",
    "        return self.lin(x).squeeze(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "a40a7c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "import torch\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    ys, preds = [], []\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data.x_z, data.edge_index, data.batch)\n",
    "        pred = torch.sigmoid(out).cpu().numpy()\n",
    "        y = data.y.cpu().numpy()\n",
    "        preds.append(pred)\n",
    "        ys.append(y)\n",
    "    ys = np.concatenate(ys)\n",
    "    preds = np.concatenate(preds)\n",
    "    auc = roc_auc_score(ys, preds)\n",
    "    ap = average_precision_score(ys, preds)\n",
    "    return auc, ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "b2e02bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\86186\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'nn.glob.global_sort_pool' is deprecated, use 'nn.aggr.SortAggr' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Train Loss: 0.6259\n",
      "Epoch 1 | Train Loss: 0.5862\n",
      "Epoch 2 | Train Loss: 0.5765\n",
      "Epoch 3 | Train Loss: 0.5726\n",
      "Epoch 4 | Train Loss: 0.5698\n",
      "Epoch 5 | Train Loss: 0.5723\n",
      "Epoch 6 | Train Loss: 0.5673\n",
      "Epoch 7 | Train Loss: 0.5681\n",
      "Epoch 8 | Train Loss: 0.5663\n",
      "Epoch 9 | Train Loss: 0.5635\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'GlobalStorage' object has no attribute 'x_z'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[158], line 56\u001b[0m\n\u001b[0;32m     54\u001b[0m test_neg_final, _ \u001b[38;5;241m=\u001b[39m dynamic_prune_subgraph(test_data, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, num_hops\u001b[38;5;241m=\u001b[39mnum_hops, scorer\u001b[38;5;241m=\u001b[39mscorer, top_n\u001b[38;5;241m=\u001b[39mtop_n)\n\u001b[0;32m     55\u001b[0m test_loader_final \u001b[38;5;241m=\u001b[39m DataLoader(test_pos_final \u001b[38;5;241m+\u001b[39m test_neg_final, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m---> 56\u001b[0m auc_val_final, ap_val_final \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader_final\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m auc_test_final, ap_test_final \u001b[38;5;241m=\u001b[39m evaluate(model, test_loader_final, device)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m训练后 val: AUC=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mauc_val_final\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, AP=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00map_val_final\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\86186\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[157], line 10\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(model, loader, device)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[0;32m      9\u001b[0m     data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 10\u001b[0m     out \u001b[38;5;241m=\u001b[39m model(\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx_z\u001b[49m, data\u001b[38;5;241m.\u001b[39medge_index, data\u001b[38;5;241m.\u001b[39mbatch)\n\u001b[0;32m     11\u001b[0m     pred \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(out)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m     12\u001b[0m     y \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39my\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[1;32mc:\\Users\\86186\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch_geometric\\data\\data.py:561\u001b[0m, in \u001b[0;36mData.__getattr__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    555\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_store\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[0;32m    556\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    557\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object was created by an older version of PyG. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    558\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf this error occurred while loading an already existing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    559\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset, remove the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocessed/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m directory in the dataset\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    560\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroot folder and try again.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 561\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_store\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\86186\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch_geometric\\data\\storage.py:96\u001b[0m, in \u001b[0;36mBaseStorage.__getattr__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[key]\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m---> 96\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[0;32m     97\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     98\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'GlobalStorage' object has no attribute 'x_z'"
     ]
    }
   ],
   "source": [
    "# 第一步，预先统计所有子图的maxz，假设你已经有 global_maxz\n",
    "global_maxz = 30  # 举例，你要实际算一遍所有训练/验证子图的maxz\n",
    "\n",
    "scorer = NeighborScorer(input_dim=data.x.size(1), hidden_dim=64).to(device)\n",
    "model = DGCNN(global_maxz+1, hidden_dim=32, num_layers=3).to(device)\n",
    "optimizer = torch.optim.Adam(list(model.parameters()) + list(scorer.parameters()), lr=0.001)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "num_hops = 2\n",
    "top_n = 10\n",
    "num_epochs = 10\n",
    "# val_pos_init, _ = dynamic_prune_subgraph(val_data, y=1, num_hops=num_hops, scorer=scorer, top_n=top_n)\n",
    "# val_neg_init, _ = dynamic_prune_subgraph(val_data, y=0, num_hops=num_hops, scorer=scorer, top_n=top_n)\n",
    "# val_loader_init = DataLoader(val_pos_init + val_neg_init, batch_size=32, shuffle=True)\n",
    "# test_pos_init, _ = dynamic_prune_subgraph(test_data, y=1, num_hops=num_hops, scorer=scorer, top_n=top_n)\n",
    "# test_neg_init, _ = dynamic_prune_subgraph(test_data, y=0, num_hops=num_hops, scorer=scorer, top_n=top_n)\n",
    "# test_loader_init = DataLoader(test_pos_init + test_neg_init, batch_size=32, shuffle=True)\n",
    "# auc_val_init, ap_val_init = evaluate(model, val_loader_init, device)\n",
    "# auc_test_init, ap_test_init = evaluate(model, test_loader_init, device)\n",
    "# print(f'训练前 val: AUC={auc_val_init:.4f}, AP={ap_val_init:.4f}')\n",
    "# print(f'训练前 test: AUC={auc_test_init:.4f}, AP={ap_test_init:.4f}')\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    scorer.train()\n",
    "    total_loss = 0\n",
    "    train_pos_data_list, maxz1 = dynamic_prune_subgraph(train_data, y=1, num_hops=num_hops, scorer=scorer, top_n=top_n)\n",
    "    train_neg_data_list, maxz2 = dynamic_prune_subgraph(train_data, y=0, num_hops=num_hops, scorer=scorer, top_n=top_n)\n",
    "    # maxz = max(maxz1, maxz2)  # 不需要每轮用，统一用 global_maxz\n",
    "    for data in chain(train_pos_data_list, train_neg_data_list):\n",
    "        data.x_z = F.one_hot(data.z, global_maxz + 1).to(torch.float)\n",
    "    train_data_list = train_pos_data_list + train_neg_data_list\n",
    "    train_loader = DataLoader(train_data_list, batch_size=32, shuffle=True)\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x_z, data.edge_index, data.batch)\n",
    "        y = data.y.float().to(out.device)\n",
    "        if y.shape != out.shape:\n",
    "            y = y.view_as(out)\n",
    "        loss = criterion(out, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += float(loss) * data.num_graphs\n",
    "    print(f'Epoch {epoch} | Train Loss: {total_loss / len(train_data_list):.4f}')\n",
    "\n",
    "\n",
    "val_pos_final, _ = dynamic_prune_subgraph(val_data, y=1, num_hops=num_hops, scorer=scorer, top_n=top_n)\n",
    "val_neg_final, _ = dynamic_prune_subgraph(val_data, y=0, num_hops=num_hops, scorer=scorer, top_n=top_n)\n",
    "val_loader_final = DataLoader(val_pos_final + val_neg_final, batch_size=32, shuffle=False)\n",
    "\n",
    "test_pos_final, _ = dynamic_prune_subgraph(test_data, y=1, num_hops=num_hops, scorer=scorer, top_n=top_n)\n",
    "test_neg_final, _ = dynamic_prune_subgraph(test_data, y=0, num_hops=num_hops, scorer=scorer, top_n=top_n)\n",
    "test_loader_final = DataLoader(test_pos_final + test_neg_final, batch_size=32, shuffle=False)\n",
    "auc_val_final, ap_val_final = evaluate(model, val_loader_final, device)\n",
    "auc_test_final, ap_test_final = evaluate(model, test_loader_final, device)\n",
    "print(f'训练后 val: AUC={auc_val_final:.4f}, AP={ap_val_final:.4f}')\n",
    "print(f'训练后 test: AUC={auc_test_final:.4f}, AP={ap_test_final:.4f}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989ef82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scorer = NeighborScorer(input_dim=data.x.size(1), hidden_dim=64)\n",
    "# model = DGCNN(31, hidden_dim=32, num_layers=3)\n",
    "# optimizer = torch.optim.Adam(list(model.parameters()) + list(scorer.parameters()), lr=0.001)\n",
    "# criterion = nn.BCEWithLogitsLoss()\n",
    "# num_hops = 2\n",
    "# top_n = 10\n",
    "# num_epochs = 10\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     scorer.train()\n",
    "#     total_loss = 0\n",
    "#     train_pos_data_list, maxz1 = dynamic_prune_subgraph(train_data,y =1, num_hops=num_hops, scorer = scorer, top_n = 10)\n",
    "#     train_neg_data_list, maxz2 = dynamic_prune_subgraph(train_data,y =1, num_hops=num_hops, scorer = scorer, top_n = 10)\n",
    "#     maxz = max(maxz1,maxz2)\n",
    "#     print(maxz)\n",
    "#     for data in chain(train_pos_data_list, train_neg_data_list,):\n",
    "#         data.x = F.one_hot(data.z, maxz + 1).to(torch.float)\n",
    "#     train_data_list = train_pos_data_list + train_neg_data_list\n",
    "#     train_loader = DataLoader(train_data_list, batch_size=32, shuffle=True)\n",
    "#     for data in train_loader:\n",
    "#         optimizer.zero_grad()\n",
    "#         out = model(data.x, data.edge_index, data.batch)\n",
    "#         loss =loss = criterion(out.view(-1), data.y)\n",
    "#         loss.backward()\n",
    "#         optimizer()\n",
    "#         total_loss+= float(loss) * data.num_graphs\n",
    "#     print(f'Epoch {epoch} | Train Loss: {total_loss / len(train_data_list):.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
