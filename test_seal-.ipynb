{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a9377ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import time\n",
    "from itertools import chain\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from scipy.sparse.csgraph import shortest_path\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from torch.nn import BCEWithLogitsLoss, Conv1d, MaxPool1d, ModuleList\n",
    "from torch_geometric.data import Data, InMemoryDataset\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import MLP, GCNConv, SortAggregation\n",
    "from torch_geometric.transforms import RandomLinkSplit\n",
    "from torch_geometric.utils import k_hop_subgraph, to_scipy_sparse_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aac9fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SEALDataset(InMemoryDataset):\n",
    "#     def __init__(self, dataset, num_hops, top_n, scorer, split='train'):\n",
    "#         self._data = dataset[0]\n",
    "#         self.num_hops = num_hops\n",
    "#         self.top_n = top_n\n",
    "#         self.scorer = scorer\n",
    "#         super().__init__(dataset.root)\n",
    "#         index = ['train', 'val', 'test'].index(split)\n",
    "#         self.load(self.processed_paths[index])\n",
    "\n",
    "#     @property\n",
    "#     def processed_file_names(self):\n",
    "#         return ['SEAL++_train_data.pt', 'SEAL++_val_data.pt', 'SEAL++_test_data.pt']\n",
    "\n",
    "#     def process(self):\n",
    "#         transform = RandomLinkSplit(num_val=0.05, num_test=0.1,\n",
    "#                                     is_undirected=True, split_labels=True)\n",
    "#         train_data, val_data, test_data = transform(self._data)\n",
    "\n",
    "#         self._max_z = 0\n",
    "\n",
    "#         # Collect a list of subgraphs for training, validation and testing:\n",
    "#         train_pos_data_list = self.extract_enclosing_subgraphs(\n",
    "#             train_data.edge_index, train_data.pos_edge_label_index, 1)\n",
    "#         train_neg_data_list = self.extract_enclosing_subgraphs(\n",
    "#             train_data.edge_index, train_data.neg_edge_label_index, 0)\n",
    "\n",
    "#         val_pos_data_list = self.extract_enclosing_subgraphs(\n",
    "#             val_data.edge_index, val_data.pos_edge_label_index, 1)\n",
    "#         val_neg_data_list = self.extract_enclosing_subgraphs(\n",
    "#             val_data.edge_index, val_data.neg_edge_label_index, 0)\n",
    "\n",
    "#         test_pos_data_list = self.extract_enclosing_subgraphs(\n",
    "#             test_data.edge_index, test_data.pos_edge_label_index, 1)\n",
    "#         test_neg_data_list = self.extract_enclosing_subgraphs(\n",
    "#             test_data.edge_index, test_data.neg_edge_label_index, 0)\n",
    "\n",
    "#         # Convert node labeling to one-hot features.\n",
    "#         for data in chain(train_pos_data_list, train_neg_data_list,\n",
    "#                           val_pos_data_list, val_neg_data_list,\n",
    "#                           test_pos_data_list, test_neg_data_list):\n",
    "#             # We solely learn links from structure, dropping any node features:\n",
    "#             data.x = F.one_hot(data.z, self._max_z + 1).to(torch.float)\n",
    "\n",
    "#         train_data_list = train_pos_data_list + train_neg_data_list\n",
    "#         self.save(train_data_list, self.processed_paths[0])\n",
    "#         val_data_list = val_pos_data_list + val_neg_data_list\n",
    "#         self.save(val_data_list, self.processed_paths[1])\n",
    "#         test_data_list = test_pos_data_list + test_neg_data_list\n",
    "#         self.save(test_data_list, self.processed_paths[2])\n",
    "\n",
    "#     def prune_subgraph(self, src, dst, data):\n",
    "#         sub_nodes, sub_edge_index, mapping, _ = k_hop_subgraph(\n",
    "#             [src, dst], self.num_hops, data.edge_index, relabel_nodes=True)\n",
    "#         x_sub = data.x[sub_nodes]\n",
    "\n",
    "#         #打分\n",
    "#         with torch.no_grad():\n",
    "#             score = self.scorer(x_sub)\n",
    "\n",
    "#         #选top n\n",
    "#         top_n = min(self.top_n, x_sub.size(0))\n",
    "#         idx_top = score.topk(top_n).indices\n",
    "\n",
    "#         mask = torch.zeros_like(score, dtype=torch.bool)\n",
    "#         mask[idx_top] = True\n",
    "#         sub_nodes_new = sub_nodes[mask]\n",
    "\n",
    "#         keep = mask[sub_edge_index[0]] & mask[sub_edge_index[1]]\n",
    "#         sub_edge_index_new = sub_edge_index[:, keep]\n",
    "\n",
    "#         return sub_nodes_new, sub_edge_index_new, mapping\n",
    "\n",
    "#     def extract_enclosing_subgraphs(self, data, edge_label_index, y):\n",
    "#         data_list = []\n",
    "#         for src, dst in edge_label_index.t().tolist():\n",
    "\n",
    "#             sub_nodes_new, sub_edge_index_new, mapping = self.prune_subgraph(self, src, dst, data)\n",
    "#             src, dst = mapping.tolist()\n",
    "\n",
    "#             # Remove target link from the subgraph.\n",
    "#             mask1 = (sub_edge_index_new[0] != src) | (sub_edge_index_new[1] != dst)\n",
    "#             mask2 = (sub_edge_index_new[0] != dst) | (sub_edge_index_new[1] != src)\n",
    "#             sub_edge_index_new = sub_edge_index_new[:, mask1 & mask2]\n",
    "\n",
    "#             # Calculate node labeling.\n",
    "#             z = self.drnl_node_labeling(sub_edge_index_new, src, dst,\n",
    "#                                         num_nodes=sub_nodes_new.size(0))\n",
    "\n",
    "#             data = Data(x=self._data.x[sub_nodes_new], z=z,\n",
    "#                         edge_index=sub_edge_index_new, y=y)\n",
    "#             data_list.append(data)\n",
    "\n",
    "#         return data_list\n",
    "\n",
    "#     def drnl_node_labeling(self, edge_index, src, dst, num_nodes=None):\n",
    "#         # Double-radius node labeling (DRNL).\n",
    "#         src, dst = (dst, src) if src > dst else (src, dst)\n",
    "#         adj = to_scipy_sparse_matrix(edge_index, num_nodes=num_nodes).tocsr()\n",
    "\n",
    "#         idx = list(range(src)) + list(range(src + 1, adj.shape[0]))\n",
    "#         adj_wo_src = adj[idx, :][:, idx]\n",
    "\n",
    "#         idx = list(range(dst)) + list(range(dst + 1, adj.shape[0]))\n",
    "#         adj_wo_dst = adj[idx, :][:, idx]\n",
    "\n",
    "#         dist2src = shortest_path(adj_wo_dst, directed=False, unweighted=True,\n",
    "#                                  indices=src)\n",
    "#         dist2src = np.insert(dist2src, dst, 0, axis=0)\n",
    "#         dist2src = torch.from_numpy(dist2src)\n",
    "\n",
    "#         dist2dst = shortest_path(adj_wo_src, directed=False, unweighted=True,\n",
    "#                                  indices=dst - 1)\n",
    "#         dist2dst = np.insert(dist2dst, src, 0, axis=0)\n",
    "#         dist2dst = torch.from_numpy(dist2dst)\n",
    "\n",
    "#         dist = dist2src + dist2dst\n",
    "#         dist_over_2, dist_mod_2 = dist // 2, dist % 2\n",
    "\n",
    "#         z = 1 + torch.min(dist2src, dist2dst)\n",
    "#         z += dist_over_2 * (dist_over_2 + dist_mod_2 - 1)\n",
    "#         z[src] = 1.\n",
    "#         z[dst] = 1.\n",
    "#         z[torch.isnan(z)] = 0.\n",
    "\n",
    "#         self._max_z = max(int(z.max()), self._max_z)\n",
    "\n",
    "#         return z.to(torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c4c91861",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drnl_node_labeling(edge_index, src, dst, num_nodes=None):\n",
    "    # Double-radius node labeling (DRNL).\n",
    "    src, dst = (dst, src) if src > dst else (src, dst)\n",
    "    adj = to_scipy_sparse_matrix(edge_index, num_nodes=num_nodes).tocsr()\n",
    "\n",
    "    idx = list(range(src)) + list(range(src + 1, adj.shape[0]))\n",
    "    adj_wo_src = adj[idx, :][:, idx]\n",
    "\n",
    "    idx = list(range(dst)) + list(range(dst + 1, adj.shape[0]))\n",
    "    adj_wo_dst = adj[idx, :][:, idx]\n",
    "\n",
    "    dist2src = shortest_path(adj_wo_dst, directed=False, unweighted=True,\n",
    "                                indices=src)\n",
    "    dist2src = np.insert(dist2src, dst, 0, axis=0)\n",
    "    dist2src = torch.from_numpy(dist2src)\n",
    "\n",
    "    dist2dst = shortest_path(adj_wo_src, directed=False, unweighted=True,\n",
    "                                indices=dst - 1)\n",
    "    dist2dst = np.insert(dist2dst, src, 0, axis=0)\n",
    "    dist2dst = torch.from_numpy(dist2dst)\n",
    "\n",
    "    dist = dist2src + dist2dst\n",
    "    dist_over_2, dist_mod_2 = dist // 2, dist % 2\n",
    "\n",
    "    z = 1 + torch.min(dist2src, dist2dst)\n",
    "    z += dist_over_2 * (dist_over_2 + dist_mod_2 - 1)\n",
    "    z[src] = 1.\n",
    "    z[dst] = 1.\n",
    "    z[torch.isnan(z)] = 0.\n",
    "\n",
    "    _max_z = max(int(z.max()), _max_z)\n",
    "\n",
    "    return z.to(torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e78668a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_enclosing_subgraphs(batch_edges, data, scorer, num_hops, top_n):\n",
    "    \"\"\"\n",
    "    针对一批目标边动态裁剪子图并打包成DataList\n",
    "    :param batch_edges: [batch_size, 2] 目标边 (src, dst) 对\n",
    "    :param data: 全图 PyG Data\n",
    "    :param scorer: MLP/attention网络\n",
    "    :param num_hops: k-hop\n",
    "    :param top_n: Top-N邻居数量\n",
    "    :return: DataList\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    for (src, dst, label) in batch_edges:  # label: 1/0\n",
    "        # 采k-hop子图\n",
    "        node_idx, edge_index, mapping, _ = k_hop_subgraph(\n",
    "            [src, dst], num_hops, data.edge_index, relabel_nodes=True, num_nodes=data.num_nodes\n",
    "        )\n",
    "        x_sub = data.x[node_idx]  # [num_nodes_sub, feat_dim]\n",
    "        # scorer打分（训练时要参与反向传播，所以不能no_grad！）\n",
    "        score = scorer(x_sub)\n",
    "        # Top-N\n",
    "        n = min(top_n, x_sub.size(0))\n",
    "        idx_top = score.topk(n).indices\n",
    "        mask = torch.zeros_like(score, dtype=torch.bool)\n",
    "        mask[idx_top] = True\n",
    "        node_idx_new = node_idx[mask]\n",
    "        # edge筛选\n",
    "        keep = mask[edge_index[0]] & mask[edge_index[1]]\n",
    "        edge_index_new = edge_index[:, keep]\n",
    "        # relabel（节点index要重新映射成0~N-1）\n",
    "        node_map = {old.item(): i for i, old in enumerate(node_idx_new)}\n",
    "        edge_index_new = torch.tensor(\n",
    "            [[node_map[n.item()] for n in edge_index_new[0]],\n",
    "             [node_map[n.item()] for n in edge_index_new[1]]],\n",
    "            dtype=torch.long\n",
    "        )\n",
    "        # DRNL标号\n",
    "        z = drnl_node_labeling(edge_index_new, mapping[0], mapping[1], num_nodes=node_idx_new.size(0))\n",
    "        # 构造Data对象\n",
    "        data_sub = Data(\n",
    "            x=x_sub[mask],\n",
    "            z=z,\n",
    "            edge_index=edge_index_new,\n",
    "            y=torch.tensor([label], dtype=torch.float),\n",
    "        )\n",
    "        data_list.append(data_sub)\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d67d5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeighborScorer(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        score = self.mlp(x).squeeze(-1)\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f8f9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scorer = NeighborScorer(input_dim=128, hidden_dim=64)\n",
    "# dataset = Planetoid('./data/Planetoid', name='Cora')\n",
    "\n",
    "# train_dataset = SEALDataset(dataset, num_hops=2, top_n=10, scorer=scorer, split='train')\n",
    "# val_dataset = SEALDataset(dataset, num_hops=2, top_n=10, scorer=scorer, split='val')\n",
    "# test_dataset = SEALDataset(dataset, num_hops=2, top_n=10, scorer=scorer, split='test')\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fd41f7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Planetoid('./data/Planetoid', name='Cora')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "data = dataset[0].to(device)\n",
    "\n",
    "# 你可以用RandomLinkSplit, train_test_split_edges等采样正负边\n",
    "# 假设已生成 train_edges_list: [(src, dst, label), ...]，label=1/0\n",
    "# 这里只演示如何生成全部正/负例边列表\n",
    "from torch_geometric.utils import negative_sampling\n",
    "\n",
    "# 正例\n",
    "edge_index = data.edge_index\n",
    "num_pos = edge_index.size(1)\n",
    "pos_edge_list = [(edge_index[0, i].item(), edge_index[1, i].item(), 1) for i in range(num_pos)]\n",
    "\n",
    "# 负例\n",
    "neg_edge_index = negative_sampling(\n",
    "    edge_index=edge_index, num_nodes=data.num_nodes, num_neg_samples=num_pos\n",
    ")\n",
    "neg_edge_list = [(neg_edge_index[0, i].item(), neg_edge_index[1, i].item(), 0) for i in range(num_pos)]\n",
    "\n",
    "# 合并打乱\n",
    "all_edges = pos_edge_list + neg_edge_list\n",
    "random.shuffle(all_edges)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dcd15aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DGCNN(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_layers, dataset, GNN=GCNConv, k=0.6):\n",
    "        super().__init__()\n",
    "\n",
    "        if k < 1:  # Transform percentile to number.\n",
    "            num_nodes = sorted([data.num_nodes for data in dataset])\n",
    "            k = num_nodes[int(math.ceil(k * len(num_nodes))) - 1]\n",
    "            k = int(max(10, k))\n",
    "\n",
    "        self.convs = ModuleList()\n",
    "        self.convs.append(GNN(dataset.num_features, hidden_dim))\n",
    "        for i in range(0, num_layers - 1):\n",
    "            self.convs.append(GNN(hidden_dim, hidden_dim))\n",
    "        self.convs.append(GNN(hidden_dim, 1))\n",
    "\n",
    "        conv1d_channels = [16, 32]\n",
    "        total_latent_dim = hidden_dim * num_layers + 1\n",
    "        conv1d_kws = [total_latent_dim, 5]\n",
    "        self.conv1 = Conv1d(1, conv1d_channels[0], conv1d_kws[0],\n",
    "                            conv1d_kws[0])\n",
    "        self.pool = SortAggregation(k)\n",
    "        self.maxpool1d = MaxPool1d(2, 2)\n",
    "        self.conv2 = Conv1d(conv1d_channels[0], conv1d_channels[1],\n",
    "                            conv1d_kws[1], 1)\n",
    "        dense_dim = int((k - 2) / 2 + 1)\n",
    "        dense_dim = (dense_dim - conv1d_kws[1] + 1) * conv1d_channels[1]\n",
    "        self.mlp = MLP([dense_dim, 128, 1], dropout=0.5, norm=None)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        xs = [x]\n",
    "        for conv in self.convs:\n",
    "            xs += [conv(xs[-1], edge_index).tanh()]\n",
    "        x = torch.cat(xs[1:], dim=-1)\n",
    "\n",
    "        # Global pooling.\n",
    "        x = self.pool(x, batch)\n",
    "        x = x.unsqueeze(1)  # [num_graphs, 1, k * hidden]\n",
    "        x = self.conv1(x).relu()\n",
    "        x = self.maxpool1d(x)\n",
    "        x = self.conv2(x).relu()\n",
    "        x = x.view(x.size(0), -1)  # [num_graphs, dense_dim]\n",
    "\n",
    "        return self.mlp(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb1cf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DGCNN(nn.Module):\n",
    "#     def __init__(self, hidden_dim, num_layers, GNN=GCNConv, k=0.6):\n",
    "#         super().__init__()\n",
    "\n",
    "#         if k < 1:  # Transform percentile to number.\n",
    "#             num_nodes = sorted([data.num_nodes for data in train_dataset])\n",
    "#             k = num_nodes[int(math.ceil(k * len(num_nodes))) - 1]\n",
    "#             k = int(max(10, k))\n",
    "\n",
    "#         self.convs = ModuleList()\n",
    "#         self.convs.append(GNN(train_dataset.num_features, hidden_dim))\n",
    "#         for i in range(0, num_layers - 1):\n",
    "#             self.convs.append(GNN(hidden_dim, hidden_dim))\n",
    "#         self.convs.append(GNN(hidden_dim, 1))\n",
    "\n",
    "#         conv1d_channels = [16, 32]\n",
    "#         total_latent_dim = hidden_dim * num_layers + 1\n",
    "#         conv1d_kws = [total_latent_dim, 5]\n",
    "#         self.conv1 = Conv1d(1, conv1d_channels[0], conv1d_kws[0],\n",
    "#                             conv1d_kws[0])\n",
    "#         self.pool = SortAggregation(k)\n",
    "#         self.maxpool1d = MaxPool1d(2, 2)\n",
    "#         self.conv2 = Conv1d(conv1d_channels[0], conv1d_channels[1],\n",
    "#                             conv1d_kws[1], 1)\n",
    "#         dense_dim = int((k - 2) / 2 + 1)\n",
    "#         dense_dim = (dense_dim - conv1d_kws[1] + 1) * conv1d_channels[1]\n",
    "#         self.mlp = MLP([dense_dim, 128, 1], dropout=0.5, norm=None)\n",
    "\n",
    "#     def forward(self, x, edge_index, batch):\n",
    "#         xs = [x]\n",
    "#         for conv in self.convs:\n",
    "#             xs += [conv(xs[-1], edge_index).tanh()]\n",
    "#         x = torch.cat(xs[1:], dim=-1)\n",
    "\n",
    "#         # Global pooling.\n",
    "#         x = self.pool(x, batch)\n",
    "#         x = x.unsqueeze(1)  # [num_graphs, 1, k * hidden]\n",
    "#         x = self.conv1(x).relu()\n",
    "#         x = self.maxpool1d(x)\n",
    "#         x = self.conv2(x).relu()\n",
    "#         x = x.view(x.size(0), -1)  # [num_graphs, dense_dim]\n",
    "\n",
    "#         return self.mlp(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c28595c7",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'num_nodes'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m scorer \u001b[38;5;241m=\u001b[39m NeighborScorer(input_dim\u001b[38;5;241m=\u001b[39mdata\u001b[38;5;241m.\u001b[39mx\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m), hidden_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mDGCNN\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_edges\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      3\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(\u001b[38;5;28mlist\u001b[39m(model\u001b[38;5;241m.\u001b[39mparameters()) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(scorer\u001b[38;5;241m.\u001b[39mparameters()), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[0;32m      4\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mBCEWithLogitsLoss()\n",
      "Cell \u001b[1;32mIn[20], line 6\u001b[0m, in \u001b[0;36mDGCNN.__init__\u001b[1;34m(self, hidden_dim, num_layers, dataset, GNN, k)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:  \u001b[38;5;66;03m# Transform percentile to number.\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m     num_nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m([\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_nodes\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m dataset])\n\u001b[0;32m      7\u001b[0m     k \u001b[38;5;241m=\u001b[39m num_nodes[\u001b[38;5;28mint\u001b[39m(math\u001b[38;5;241m.\u001b[39mceil(k \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(num_nodes))) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m      8\u001b[0m     k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m10\u001b[39m, k))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'num_nodes'"
     ]
    }
   ],
   "source": [
    "scorer = NeighborScorer(input_dim=data.x.size(1), hidden_dim=64).to(device)\n",
    "model = DGCNN(hidden_dim=32, num_layers=3, dataset=all_edges).to(device)\n",
    "optimizer = torch.optim.Adam(list(model.parameters()) + list(scorer.parameters()), lr=0.001)\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450cceb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "num_epochs = 10\n",
    "num_hops = 2\n",
    "top_n = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    scorer.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for i in range(0, len(all_edges), batch_size):\n",
    "        batch_edges = all_edges[i:i+batch_size]\n",
    "        batch_data_list = []\n",
    "        for src, dst, label in batch_edges:\n",
    "            x_sub, edge_index_sub, z_sub = dynamic_prune_subgraph(\n",
    "                src, dst, data, num_hops, scorer, top_n\n",
    "            )\n",
    "            batch_data_list.append(\n",
    "                Data(x=x_sub, z=z_sub, edge_index=edge_index_sub,\n",
    "                     y=torch.tensor([label], dtype=torch.float))\n",
    "            )\n",
    "        batch = DataLoader(batch_data_list, batch_size=len(batch_data_list), shuffle=False)\n",
    "        for data_sub in batch:\n",
    "            data_sub = data_sub.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(data_sub.x, data_sub.edge_index, data_sub.batch)\n",
    "            loss = criterion(out.view(-1), data_sub.y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += float(loss) * data_sub.num_graphs\n",
    "\n",
    "    print(f'Epoch {epoch}, Loss {total_loss / len(all_edges):.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9fd137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# model = DGCNN(hidden_dim=32, num_layers=3).to(device)\n",
    "# optimizer = torch.optim.Adam(list(model.parameters()) + list(scorer.parameters()), lr=0.0001)\n",
    "# criterion = BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39253cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train():\n",
    "#     model.train()\n",
    "#     scorer.train()\n",
    "#     total_loss = 0\n",
    "    \n",
    "#     for data in train_loader:\n",
    "#         data = data.to(device)\n",
    "#         optimizer.zero_grad()\n",
    "#         out = model(data.x, data.edge_index, data.batch)\n",
    "#         loss = criterion(out.view(-1), data.y.to(torch.float))\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         total_loss += float(loss) * data.num_graphs\n",
    "\n",
    "#     return total_loss / len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3b4b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "\n",
    "    y_pred, y_true = [], []\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        logits = model(data.x, data.edge_index, data.batch)\n",
    "        y_pred.append(logits.view(-1).cpu())\n",
    "        y_true.append(data.y.view(-1).cpu().to(torch.float))\n",
    "\n",
    "    return roc_auc_score(torch.cat(y_true), torch.cat(y_pred)), average_precision_score(torch.cat(y_true),torch.cat(y_pred) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1bcd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# times = []\n",
    "# best_val_auc = test_auc = 0\n",
    "# for epoch in range(1, 51):\n",
    "#     start = time.time()\n",
    "#     loss = train()\n",
    "#     val_auc, val_ap = test(val_loader)\n",
    "#     if val_auc > best_val_auc:\n",
    "#         best_val_auc = val_auc\n",
    "#         test_auc, test_ap = test(test_loader)\n",
    "#     print(f'Epoch: {epoch:02d}, Loss: {loss:.4f}, Val_AUC: {val_auc:.4f}, Val_AP: {val_ap:.4f}'\n",
    "#           f'Test_AUC: {test_auc:.4f}, Test_AP: {test_ap:.4f}')\n",
    "#     times.append(time.time() - start)\n",
    "# print(f\"Median time per epoch: {torch.tensor(times).median():.4f}s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
